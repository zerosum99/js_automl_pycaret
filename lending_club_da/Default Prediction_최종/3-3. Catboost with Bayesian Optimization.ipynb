{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288c226-efaa-48d8-ba6e-d5b7eaaf85cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14e08c6-8884-4d12-be9f-46b904531ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# pd.options.display.max_rows = 10000\n",
    "# pd.options.display.max_columns = 1000\n",
    "# pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d38b7a-bc60-475d-8b4b-929edfefbcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244741c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:48:14.024560Z",
     "start_time": "2021-06-15T01:48:14.020751Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.570573,
     "end_time": "2021-06-10T05:21:01.264405",
     "exception": false,
     "start_time": "2021-06-10T05:20:58.693832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bfa42-76fe-4fc3-8c65-d5ad9caff36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5947c0",
   "metadata": {
    "papermill": {
     "duration": 0.02783,
     "end_time": "2021-06-10T05:21:01.312660",
     "exception": false,
     "start_time": "2021-06-10T05:21:01.284830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Deterministic\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1563e-ffef-4532-b481-0a9a3da66ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a02d4f-0201-4c0f-8fe2-e898a6803885",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_read =\"/Users/a06411/Documents/data_hub/lending_club/lgb_selected.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb336a1a",
   "metadata": {
    "papermill": {
     "duration": 12.321696,
     "end_time": "2021-06-10T05:21:13.654303",
     "exception": false,
     "start_time": "2021-06-10T05:21:01.332607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load Data\n",
    "df = pd.read_pickle(path_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14626247-dfe3-4d95-a6d7-cf061957580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1382351, 63)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92c875-007a-4beb-9bf6-c49a60737505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6349fd4",
   "metadata": {
    "papermill": {
     "duration": 2.457296,
     "end_time": "2021-06-10T05:21:16.131860",
     "exception": false,
     "start_time": "2021-06-10T05:21:13.674564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "df['issue_year'] = df['issue_d'].dt.year\n",
    "df.loc[df['issue_year'] == 2007 , 'issue_year'] = 201200\n",
    "df.loc[df['issue_year'] == 2008 , 'issue_year'] = 201200\n",
    "df.loc[df['issue_year'] == 2009 , 'issue_year'] = 201200\n",
    "df.loc[df['issue_year'] == 2010 , 'issue_year'] = 201200\n",
    "df.loc[df['issue_year'] == 2011 , 'issue_year'] = 201200\n",
    "df.loc[df['issue_year'] == 2012 , 'issue_year'] = 201200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277092d7",
   "metadata": {
    "papermill": {
     "duration": 0.027484,
     "end_time": "2021-06-10T05:21:16.179330",
     "exception": false,
     "start_time": "2021-06-10T05:21:16.151846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88cdd97a",
   "metadata": {
    "papermill": {
     "duration": 0.655643,
     "end_time": "2021-06-10T05:21:16.854561",
     "exception": false,
     "start_time": "2021-06-10T05:21:16.198918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = df[df['issue_year'] != 2018]\n",
    "test = df[df['issue_year'] == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d8133c",
   "metadata": {
    "papermill": {
     "duration": 0.028826,
     "end_time": "2021-06-10T05:21:16.902935",
     "exception": false,
     "start_time": "2021-06-10T05:21:16.874109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = test[['id','loan_status']]\n",
    "# test.drop('loan_status',1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed97d41",
   "metadata": {
    "papermill": {
     "duration": 0.608789,
     "end_time": "2021-06-10T05:21:17.531273",
     "exception": false,
     "start_time": "2021-06-10T05:21:16.922484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttrain = train[train['issue_year'] != 2017]\n",
    "ttest = train[train['issue_year'] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67d622ff",
   "metadata": {
    "papermill": {
     "duration": 1.000635,
     "end_time": "2021-06-10T05:21:18.551374",
     "exception": false,
     "start_time": "2021-06-10T05:21:17.550739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = ttrain.copy()\n",
    "y_train = ttrain['loan_status']\n",
    "X_train.drop(['loan_status'], axis = 1, inplace = True)\n",
    "\n",
    "X_test = ttest.copy()\n",
    "y_test = ttest['loan_status']\n",
    "X_test.drop(['loan_status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2af3240",
   "metadata": {
    "papermill": {
     "duration": 0.031199,
     "end_time": "2021-06-10T05:21:18.604605",
     "exception": false,
     "start_time": "2021-06-10T05:21:18.573406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_features = ['earliest_cr_line', 'issue_d', 'last_credit_pull_d', 'last_pymnt_d', 'next_pymnt_d',\n",
    "                  'initial_list_status','out_prncp','out_prncp_inv','total_pymnt','total_pymnt_inv','total_rec_prncp','total_rec_int','total_rec_late_fee',\n",
    "                   'recoveries','collection_recovery_fee','last_pymnt_amnt','last_fico_range_high','last_fico_range_low' , 'id', 'loan_status' , 'issue_year']\n",
    "features  = [col for col in list(train) if col not in remove_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d85bcc-9c80-438c-86fe-e698b4dbc166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2147590",
   "metadata": {
    "papermill": {
     "duration": 0.045504,
     "end_time": "2021-06-10T05:21:18.669606",
     "exception": false,
     "start_time": "2021-06-10T05:21:18.624102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1132562 entries, 0 to 2260697\n",
      "Data columns (total 63 columns):\n",
      " #   Column                       Non-Null Count    Dtype         \n",
      "---  ------                       --------------    -----         \n",
      " 0   int_rate                     1132562 non-null  float64       \n",
      " 1   dti                          1132562 non-null  float64       \n",
      " 2   annual_inc                   1132562 non-null  float64       \n",
      " 3   mo_sin_old_rev_tl_op         1132562 non-null  float64       \n",
      " 4   acc_open_past_24mths         1132562 non-null  float64       \n",
      " 5   loan_amnt                    1132562 non-null  float64       \n",
      " 6   emp_length                   1132562 non-null  int64         \n",
      " 7   addr_state                   1132562 non-null  int64         \n",
      " 8   revol_bal                    1132562 non-null  float64       \n",
      " 9   term                         1132562 non-null  int64         \n",
      " 10  sub_grade                    1132562 non-null  int64         \n",
      " 11  funded_amnt_inv              1132562 non-null  float64       \n",
      " 12  installment                  1132562 non-null  float64       \n",
      " 13  purpose                      1132562 non-null  int64         \n",
      " 14  total_rev_hi_lim             1132562 non-null  float64       \n",
      " 15  fico_range_low               1132562 non-null  float64       \n",
      " 16  debt_settlement_flag         1132562 non-null  int64         \n",
      " 17  mort_acc                     1132562 non-null  float64       \n",
      " 18  total_bc_limit               1132562 non-null  float64       \n",
      " 19  home_ownership               1132562 non-null  int64         \n",
      " 20  avg_cur_bal                  1132562 non-null  float64       \n",
      " 21  all_util                     1132562 non-null  float64       \n",
      " 22  mths_since_recent_bc         1132562 non-null  float64       \n",
      " 23  total_acc                    1132562 non-null  float64       \n",
      " 24  open_acc_6m                  1132562 non-null  float64       \n",
      " 25  bc_util                      1132562 non-null  float64       \n",
      " 26  num_actv_rev_tl              1132562 non-null  float64       \n",
      " 27  funded_amnt                  1132562 non-null  float64       \n",
      " 28  hardship_flag                1132562 non-null  int64         \n",
      " 29  num_rev_tl_bal_gt_0          1132562 non-null  float64       \n",
      " 30  mths_since_recent_inq        1132562 non-null  float64       \n",
      " 31  inq_last_6mths               1132562 non-null  float64       \n",
      " 32  num_il_tl                    1132562 non-null  float64       \n",
      " 33  mo_sin_old_il_acct           1132562 non-null  float64       \n",
      " 34  num_rev_accts                1132562 non-null  float64       \n",
      " 35  num_tl_120dpd_2m             1132562 non-null  float64       \n",
      " 36  total_il_high_credit_limit   1132562 non-null  float64       \n",
      " 37  application_type             1132562 non-null  int64         \n",
      " 38  revol_util                   1132562 non-null  float64       \n",
      " 39  tot_hi_cred_lim              1132562 non-null  float64       \n",
      " 40  delinq_2yrs                  1132562 non-null  float64       \n",
      " 41  mo_sin_rcnt_tl               1132562 non-null  float64       \n",
      " 42  num_actv_bc_tl               1132562 non-null  float64       \n",
      " 43  mths_since_last_record       1132562 non-null  float64       \n",
      " 44  percent_bc_gt_75             1132562 non-null  float64       \n",
      " 45  bc_open_to_buy               1132562 non-null  float64       \n",
      " 46  max_bal_bc                   1132562 non-null  float64       \n",
      " 47  grade                        1132562 non-null  int64         \n",
      " 48  open_rv_24m                  1132562 non-null  float64       \n",
      " 49  mo_sin_rcnt_rev_tl_op        1132562 non-null  float64       \n",
      " 50  pct_tl_nvr_dlq               1132562 non-null  float64       \n",
      " 51  verification_status          1132562 non-null  int64         \n",
      " 52  tot_cur_bal                  1132562 non-null  float64       \n",
      " 53  total_bal_ex_mort            1132562 non-null  float64       \n",
      " 54  mths_since_last_major_derog  1132562 non-null  float64       \n",
      " 55  inq_fi                       1132562 non-null  float64       \n",
      " 56  mths_since_rcnt_il           1132562 non-null  float64       \n",
      " 57  inq_last_12m                 1132562 non-null  float64       \n",
      " 58  mths_since_last_delinq       1132562 non-null  float64       \n",
      " 59  num_bc_tl                    1132562 non-null  float64       \n",
      " 60  issue_d                      1132562 non-null  datetime64[ns]\n",
      " 61  id                           1132562 non-null  object        \n",
      " 62  issue_year                   1132562 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(49), int64(12), object(1)\n",
      "memory usage: 553.0+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53210463",
   "metadata": {
    "papermill": {
     "duration": 0.028688,
     "end_time": "2021-06-10T05:21:18.718399",
     "exception": false,
     "start_time": "2021-06-10T05:21:18.689711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_feat = [\n",
    "    'debt_settlement_flag',\n",
    "    'hardship_flag',\n",
    "    'term',\n",
    "    'application_type',\n",
    "    ## 'pymnt_plan',\n",
    "        'verification_status',\n",
    "        'home_ownership',\n",
    "        'grade',\n",
    "        'purpose'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de59d141",
   "metadata": {
    "papermill": {
     "duration": 0.325123,
     "end_time": "2021-06-10T05:21:19.063432",
     "exception": false,
     "start_time": "2021-06-10T05:21:18.738309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfa7c9-0382-4ff5-80e2-257e7b6830e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a321f-fe46-4998-bdcd-01ed197719b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e6a0e4f",
   "metadata": {
    "papermill": {
     "duration": 0.030612,
     "end_time": "2021-06-10T05:21:19.115032",
     "exception": false,
     "start_time": "2021-06-10T05:21:19.084420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import catboost as cgb\n",
    "\n",
    "def cat_hyp(depth, bagging_temperature,  learning_rate):\n",
    "    params = {\"iterations\": 100,\n",
    "            'eval_metric': 'AUC',\n",
    "              'loss_function': 'Logloss',\n",
    "            \"verbose\": False} \n",
    "    params[\"depth\"] = int(round(depth)) \n",
    "    params[\"bagging_temperature\"] = bagging_temperature\n",
    "    params[\"learning_rate\"] = learning_rate\n",
    "#     params[\"l2_leaf_reg\"] = l2_leaf_reg\n",
    "  \n",
    "    cat_feat = [] # Categorical features list, we have nothing in this dataset\n",
    "    cv_dataset = cgb.Pool(data=X_train,\n",
    "                        label=y_train,\n",
    "                        cat_features=cat_feat)\n",
    "\n",
    "    scores = cgb.cv(cv_dataset,\n",
    "              params,\n",
    "              fold_count=3)\n",
    "    print (scores)\n",
    "    return np.max(scores['test-AUC-mean']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde13c2f",
   "metadata": {
    "papermill": {
     "duration": 0.027069,
     "end_time": "2021-06-10T05:21:19.162002",
     "exception": false,
     "start_time": "2021-06-10T05:21:19.134933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search space\n",
    "pds = {'depth': (4, 10),\n",
    "       'bagging_temperature': (0.1,10),\n",
    "#        'l2_leaf_reg': (0.1, 10),\n",
    "       'learning_rate': (0.1, 0.2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d29259",
   "metadata": {
    "papermill": {
     "duration": 2859.067635,
     "end_time": "2021-06-10T06:08:58.249452",
     "exception": false,
     "start_time": "2021-06-10T05:21:19.181817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... |   depth   | learni... |\n",
      "-------------------------------------------------------------\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7657601867\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7675683765\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7686908555\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.589878   \n",
      "1            1       0.744900      0.000732           0.528068   \n",
      "2            2       0.749478      0.001388           0.488845   \n",
      "3            3       0.751297      0.001205           0.464209   \n",
      "4            4       0.752115      0.001170           0.448630   \n",
      "5            5       0.753297      0.001469           0.438575   \n",
      "6            6       0.753806      0.001184           0.431359   \n",
      "7            7       0.754895      0.001311           0.426261   \n",
      "8            8       0.756008      0.001143           0.422582   \n",
      "9            9       0.756586      0.001211           0.420199   \n",
      "10          10       0.756927      0.001134           0.418467   \n",
      "11          11       0.757598      0.001205           0.417100   \n",
      "12          12       0.758095      0.001189           0.416067   \n",
      "13          13       0.758630      0.001250           0.415183   \n",
      "14          14       0.759102      0.001215           0.414514   \n",
      "15          15       0.759437      0.001236           0.414064   \n",
      "16          16       0.759814      0.001256           0.413669   \n",
      "17          17       0.760142      0.001324           0.413321   \n",
      "18          18       0.760364      0.001324           0.413042   \n",
      "19          19       0.760672      0.001356           0.412770   \n",
      "20          20       0.760943      0.001305           0.412557   \n",
      "21          21       0.761170      0.001320           0.412357   \n",
      "22          22       0.761427      0.001352           0.412180   \n",
      "23          23       0.761681      0.001319           0.412015   \n",
      "24          24       0.761886      0.001326           0.411880   \n",
      "25          25       0.762085      0.001299           0.411735   \n",
      "26          26       0.762230      0.001351           0.411617   \n",
      "27          27       0.762382      0.001360           0.411490   \n",
      "28          28       0.762585      0.001409           0.411369   \n",
      "29          29       0.762734      0.001435           0.411277   \n",
      "30          30       0.762918      0.001448           0.411172   \n",
      "31          31       0.763071      0.001439           0.411075   \n",
      "32          32       0.763199      0.001419           0.410982   \n",
      "33          33       0.763367      0.001450           0.410892   \n",
      "34          34       0.763449      0.001432           0.410825   \n",
      "35          35       0.763563      0.001447           0.410763   \n",
      "36          36       0.763698      0.001426           0.410684   \n",
      "37          37       0.763784      0.001432           0.410633   \n",
      "38          38       0.763925      0.001456           0.410555   \n",
      "39          39       0.764038      0.001459           0.410489   \n",
      "40          40       0.764138      0.001403           0.410430   \n",
      "41          41       0.764233      0.001422           0.410381   \n",
      "42          42       0.764341      0.001458           0.410321   \n",
      "43          43       0.764437      0.001452           0.410263   \n",
      "44          44       0.764515      0.001442           0.410216   \n",
      "45          45       0.764603      0.001484           0.410169   \n",
      "46          46       0.764670      0.001501           0.410121   \n",
      "47          47       0.764755      0.001517           0.410071   \n",
      "48          48       0.764841      0.001547           0.410017   \n",
      "49          49       0.764903      0.001538           0.409987   \n",
      "50          50       0.764987      0.001560           0.409939   \n",
      "51          51       0.765061      0.001544           0.409893   \n",
      "52          52       0.765122      0.001505           0.409857   \n",
      "53          53       0.765231      0.001468           0.409803   \n",
      "54          54       0.765269      0.001453           0.409774   \n",
      "55          55       0.765299      0.001465           0.409753   \n",
      "56          56       0.765356      0.001470           0.409720   \n",
      "57          57       0.765399      0.001478           0.409650   \n",
      "58          58       0.765471      0.001514           0.409606   \n",
      "59          59       0.765570      0.001500           0.409556   \n",
      "60          60       0.765639      0.001501           0.409520   \n",
      "61          61       0.765705      0.001459           0.409487   \n",
      "62          62       0.765747      0.001495           0.409462   \n",
      "63          63       0.765797      0.001524           0.409432   \n",
      "64          64       0.765850      0.001525           0.409403   \n",
      "65          65       0.765978      0.001512           0.409328   \n",
      "66          66       0.766021      0.001530           0.409307   \n",
      "67          67       0.766076      0.001546           0.409276   \n",
      "68          68       0.766111      0.001532           0.409257   \n",
      "69          69       0.766168      0.001549           0.409227   \n",
      "70          70       0.766211      0.001549           0.409205   \n",
      "71          71       0.766242      0.001556           0.409186   \n",
      "72          72       0.766279      0.001543           0.409166   \n",
      "73          73       0.766331      0.001525           0.409077   \n",
      "74          74       0.766369      0.001522           0.409055   \n",
      "75          75       0.766419      0.001529           0.409027   \n",
      "76          76       0.766445      0.001518           0.409014   \n",
      "77          77       0.766462      0.001534           0.409001   \n",
      "78          78       0.766514      0.001544           0.408977   \n",
      "79          79       0.766563      0.001527           0.408950   \n",
      "80          80       0.766633      0.001509           0.408910   \n",
      "81          81       0.766670      0.001500           0.408892   \n",
      "82          82       0.766700      0.001516           0.408875   \n",
      "83          83       0.766746      0.001516           0.408848   \n",
      "84          84       0.766775      0.001508           0.408835   \n",
      "85          85       0.766835      0.001481           0.408803   \n",
      "86          86       0.766876      0.001476           0.408784   \n",
      "87          87       0.766919      0.001478           0.408763   \n",
      "88          88       0.766962      0.001485           0.408741   \n",
      "89          89       0.766981      0.001477           0.408737   \n",
      "90          90       0.767019      0.001436           0.408716   \n",
      "91          91       0.767052      0.001436           0.408697   \n",
      "92          92       0.767073      0.001468           0.408683   \n",
      "93          93       0.767099      0.001464           0.408672   \n",
      "94          94       0.767142      0.001452           0.408648   \n",
      "95          95       0.767190      0.001467           0.408624   \n",
      "96          96       0.767252      0.001496           0.408593   \n",
      "97          97       0.767292      0.001498           0.408576   \n",
      "98          98       0.767312      0.001510           0.408543   \n",
      "99          99       0.767340      0.001479           0.408525   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000353            0.589768           0.000422  \n",
      "1           0.000356            0.527882           0.000602  \n",
      "2           0.000729            0.488574           0.001110  \n",
      "3           0.000140            0.463859           0.000443  \n",
      "4           0.000587            0.448235           0.000206  \n",
      "5           0.000446            0.438104           0.000195  \n",
      "6           0.000423            0.430774           0.000304  \n",
      "7           0.000270            0.425505           0.000561  \n",
      "8           0.000373            0.421606           0.000426  \n",
      "9           0.000486            0.419075           0.000311  \n",
      "10          0.000552            0.417257           0.000295  \n",
      "11          0.000634            0.415746           0.000247  \n",
      "12          0.000601            0.414540           0.000285  \n",
      "13          0.000611            0.413577           0.000291  \n",
      "14          0.000597            0.412725           0.000324  \n",
      "15          0.000587            0.412102           0.000315  \n",
      "16          0.000600            0.411555           0.000332  \n",
      "17          0.000598            0.411047           0.000327  \n",
      "18          0.000639            0.410672           0.000302  \n",
      "19          0.000641            0.410250           0.000300  \n",
      "20          0.000651            0.409799           0.000270  \n",
      "21          0.000654            0.409415           0.000265  \n",
      "22          0.000669            0.409065           0.000263  \n",
      "23          0.000650            0.408762           0.000342  \n",
      "24          0.000655            0.408427           0.000368  \n",
      "25          0.000649            0.408093           0.000368  \n",
      "26          0.000671            0.407835           0.000328  \n",
      "27          0.000694            0.407606           0.000338  \n",
      "28          0.000723            0.407256           0.000317  \n",
      "29          0.000726            0.407034           0.000319  \n",
      "30          0.000729            0.406766           0.000287  \n",
      "31          0.000737            0.406494           0.000288  \n",
      "32          0.000740            0.406247           0.000315  \n",
      "33          0.000762            0.405986           0.000310  \n",
      "34          0.000744            0.405804           0.000262  \n",
      "35          0.000760            0.405515           0.000259  \n",
      "36          0.000743            0.405309           0.000284  \n",
      "37          0.000744            0.405125           0.000302  \n",
      "38          0.000746            0.404889           0.000307  \n",
      "39          0.000753            0.404688           0.000323  \n",
      "40          0.000732            0.404492           0.000340  \n",
      "41          0.000740            0.404257           0.000306  \n",
      "42          0.000746            0.404030           0.000258  \n",
      "43          0.000747            0.403861           0.000278  \n",
      "44          0.000749            0.403684           0.000319  \n",
      "45          0.000772            0.403525           0.000328  \n",
      "46          0.000787            0.403361           0.000314  \n",
      "47          0.000789            0.403134           0.000279  \n",
      "48          0.000810            0.402935           0.000249  \n",
      "49          0.000806            0.402708           0.000295  \n",
      "50          0.000817            0.402524           0.000316  \n",
      "51          0.000811            0.402317           0.000303  \n",
      "52          0.000786            0.402102           0.000320  \n",
      "53          0.000770            0.401879           0.000295  \n",
      "54          0.000758            0.401680           0.000254  \n",
      "55          0.000765            0.401479           0.000255  \n",
      "56          0.000772            0.401315           0.000253  \n",
      "57          0.000847            0.401141           0.000187  \n",
      "58          0.000865            0.401009           0.000119  \n",
      "59          0.000852            0.400819           0.000151  \n",
      "60          0.000849            0.400613           0.000160  \n",
      "61          0.000829            0.400427           0.000184  \n",
      "62          0.000846            0.400220           0.000189  \n",
      "63          0.000861            0.400056           0.000171  \n",
      "64          0.000864            0.399842           0.000196  \n",
      "65          0.000863            0.399697           0.000196  \n",
      "66          0.000869            0.399545           0.000203  \n",
      "67          0.000871            0.399363           0.000244  \n",
      "68          0.000870            0.399189           0.000248  \n",
      "69          0.000876            0.398978           0.000262  \n",
      "70          0.000878            0.398797           0.000295  \n",
      "71          0.000887            0.398664           0.000282  \n",
      "72          0.000878            0.398512           0.000288  \n",
      "73          0.000826            0.398304           0.000280  \n",
      "74          0.000826            0.398107           0.000286  \n",
      "75          0.000834            0.397906           0.000293  \n",
      "76          0.000832            0.397732           0.000277  \n",
      "77          0.000836            0.397564           0.000283  \n",
      "78          0.000836            0.397391           0.000254  \n",
      "79          0.000828            0.397251           0.000278  \n",
      "80          0.000818            0.397078           0.000261  \n",
      "81          0.000815            0.396903           0.000288  \n",
      "82          0.000828            0.396711           0.000289  \n",
      "83          0.000829            0.396525           0.000251  \n",
      "84          0.000827            0.396359           0.000265  \n",
      "85          0.000820            0.396221           0.000244  \n",
      "86          0.000819            0.396058           0.000259  \n",
      "87          0.000826            0.395863           0.000278  \n",
      "88          0.000831            0.395665           0.000283  \n",
      "89          0.000829            0.395483           0.000268  \n",
      "90          0.000808            0.395316           0.000253  \n",
      "91          0.000810            0.395146           0.000299  \n",
      "92          0.000824            0.394962           0.000315  \n",
      "93          0.000816            0.394802           0.000304  \n",
      "94          0.000811            0.394636           0.000279  \n",
      "95          0.000825            0.394440           0.000245  \n",
      "96          0.000842            0.394302           0.000262  \n",
      "97          0.000847            0.394130           0.000225  \n",
      "98          0.000885            0.393946           0.000196  \n",
      "99          0.000865            0.393795           0.000209  \n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7673   \u001b[0m | \u001b[0m3.808    \u001b[0m | \u001b[0m9.704    \u001b[0m | \u001b[0m0.1732   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7615905264\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7634426717\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7641380457\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.724845      0.005877           0.622339   \n",
      "1            1       0.734269      0.001179           0.571389   \n",
      "2            2       0.739746      0.000811           0.534205   \n",
      "3            3       0.741241      0.001198           0.507002   \n",
      "4            4       0.742023      0.002038           0.486927   \n",
      "5            5       0.743680      0.000875           0.471383   \n",
      "6            6       0.745170      0.001236           0.459771   \n",
      "7            7       0.746597      0.001274           0.450635   \n",
      "8            8       0.747786      0.001542           0.444123   \n",
      "9            9       0.748667      0.001564           0.438545   \n",
      "10          10       0.749447      0.001689           0.434169   \n",
      "11          11       0.750198      0.001420           0.430672   \n",
      "12          12       0.750844      0.001428           0.428062   \n",
      "13          13       0.751579      0.001300           0.425784   \n",
      "14          14       0.752044      0.001370           0.424069   \n",
      "15          15       0.752739      0.001214           0.422780   \n",
      "16          16       0.753200      0.001136           0.421534   \n",
      "17          17       0.753578      0.001081           0.420564   \n",
      "18          18       0.754051      0.001112           0.419708   \n",
      "19          19       0.754386      0.001117           0.419026   \n",
      "20          20       0.754696      0.001111           0.418393   \n",
      "21          21       0.754961      0.001115           0.417854   \n",
      "22          22       0.755177      0.001210           0.417458   \n",
      "23          23       0.755383      0.001145           0.417081   \n",
      "24          24       0.755563      0.001130           0.416744   \n",
      "25          25       0.755858      0.001082           0.416444   \n",
      "26          26       0.756111      0.001109           0.416167   \n",
      "27          27       0.756331      0.001084           0.415948   \n",
      "28          28       0.756588      0.001095           0.415754   \n",
      "29          29       0.756766      0.001099           0.415556   \n",
      "30          30       0.756969      0.001185           0.415357   \n",
      "31          31       0.757145      0.001158           0.415206   \n",
      "32          32       0.757303      0.001120           0.415057   \n",
      "33          33       0.757480      0.001131           0.414906   \n",
      "34          34       0.757577      0.001129           0.414780   \n",
      "35          35       0.757740      0.001119           0.414658   \n",
      "36          36       0.757909      0.001144           0.414546   \n",
      "37          37       0.758097      0.001179           0.414418   \n",
      "38          38       0.758253      0.001147           0.414309   \n",
      "39          39       0.758369      0.001178           0.414206   \n",
      "40          40       0.758490      0.001204           0.414112   \n",
      "41          41       0.758670      0.001225           0.413999   \n",
      "42          42       0.758829      0.001202           0.413902   \n",
      "43          43       0.758904      0.001210           0.413830   \n",
      "44          44       0.759014      0.001227           0.413738   \n",
      "45          45       0.759181      0.001227           0.413640   \n",
      "46          46       0.759319      0.001222           0.413554   \n",
      "47          47       0.759453      0.001240           0.413464   \n",
      "48          48       0.759542      0.001258           0.413388   \n",
      "49          49       0.759660      0.001235           0.413308   \n",
      "50          50       0.759762      0.001237           0.413228   \n",
      "51          51       0.759853      0.001226           0.413163   \n",
      "52          52       0.759975      0.001208           0.413092   \n",
      "53          53       0.760072      0.001168           0.413031   \n",
      "54          54       0.760154      0.001164           0.412970   \n",
      "55          55       0.760243      0.001180           0.412909   \n",
      "56          56       0.760317      0.001208           0.412856   \n",
      "57          57       0.760437      0.001227           0.412787   \n",
      "58          58       0.760528      0.001218           0.412729   \n",
      "59          59       0.760600      0.001222           0.412676   \n",
      "60          60       0.760679      0.001202           0.412626   \n",
      "61          61       0.760767      0.001220           0.412581   \n",
      "62          62       0.760837      0.001224           0.412537   \n",
      "63          63       0.760896      0.001200           0.412493   \n",
      "64          64       0.760967      0.001220           0.412446   \n",
      "65          65       0.761045      0.001222           0.412398   \n",
      "66          66       0.761132      0.001231           0.412343   \n",
      "67          67       0.761205      0.001258           0.412298   \n",
      "68          68       0.761290      0.001279           0.412243   \n",
      "69          69       0.761368      0.001281           0.412198   \n",
      "70          70       0.761428      0.001275           0.412156   \n",
      "71          71       0.761495      0.001279           0.412116   \n",
      "72          72       0.761551      0.001285           0.412015   \n",
      "73          73       0.761615      0.001288           0.411975   \n",
      "74          74       0.761691      0.001313           0.411928   \n",
      "75          75       0.761766      0.001322           0.411888   \n",
      "76          76       0.761809      0.001332           0.411861   \n",
      "77          77       0.761871      0.001345           0.411830   \n",
      "78          78       0.761928      0.001349           0.411784   \n",
      "79          79       0.761995      0.001326           0.411744   \n",
      "80          80       0.762055      0.001313           0.411709   \n",
      "81          81       0.762106      0.001316           0.411680   \n",
      "82          82       0.762168      0.001305           0.411640   \n",
      "83          83       0.762219      0.001296           0.411545   \n",
      "84          84       0.762287      0.001315           0.411511   \n",
      "85          85       0.762350      0.001330           0.411478   \n",
      "86          86       0.762396      0.001316           0.411447   \n",
      "87          87       0.762461      0.001308           0.411415   \n",
      "88          88       0.762503      0.001318           0.411388   \n",
      "89          89       0.762560      0.001312           0.411352   \n",
      "90          90       0.762605      0.001325           0.411322   \n",
      "91          91       0.762663      0.001331           0.411291   \n",
      "92          92       0.762705      0.001335           0.411265   \n",
      "93          93       0.762762      0.001324           0.411231   \n",
      "94          94       0.762808      0.001338           0.411202   \n",
      "95          95       0.762855      0.001319           0.411168   \n",
      "96          96       0.762899      0.001325           0.411137   \n",
      "97          97       0.762950      0.001321           0.411104   \n",
      "98          98       0.763007      0.001295           0.411073   \n",
      "99          99       0.763057      0.001317           0.411044   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001122            0.622262           0.000994  \n",
      "1           0.000423            0.571308           0.000217  \n",
      "2           0.000128            0.534130           0.000271  \n",
      "3           0.000533            0.506938           0.000717  \n",
      "4           0.000456            0.486865           0.000442  \n",
      "5           0.000136            0.471309           0.000500  \n",
      "6           0.000381            0.459700           0.000372  \n",
      "7           0.000366            0.450548           0.000266  \n",
      "8           0.000795            0.444016           0.000211  \n",
      "9           0.000631            0.438414           0.000017  \n",
      "10          0.000398            0.434034           0.000277  \n",
      "11          0.000520            0.430534           0.000268  \n",
      "12          0.000518            0.427922           0.000223  \n",
      "13          0.000573            0.425632           0.000284  \n",
      "14          0.000510            0.423911           0.000301  \n",
      "15          0.000442            0.422611           0.000356  \n",
      "16          0.000414            0.421362           0.000376  \n",
      "17          0.000356            0.420393           0.000455  \n",
      "18          0.000453            0.419532           0.000375  \n",
      "19          0.000482            0.418839           0.000384  \n",
      "20          0.000528            0.418208           0.000395  \n",
      "21          0.000515            0.417667           0.000389  \n",
      "22          0.000500            0.417265           0.000412  \n",
      "23          0.000453            0.416883           0.000478  \n",
      "24          0.000435            0.416541           0.000488  \n",
      "25          0.000471            0.416230           0.000452  \n",
      "26          0.000482            0.415957           0.000467  \n",
      "27          0.000465            0.415733           0.000493  \n",
      "28          0.000464            0.415525           0.000510  \n",
      "29          0.000502            0.415311           0.000490  \n",
      "30          0.000501            0.415105           0.000495  \n",
      "31          0.000509            0.414941           0.000493  \n",
      "32          0.000519            0.414780           0.000491  \n",
      "33          0.000520            0.414623           0.000502  \n",
      "34          0.000508            0.414488           0.000520  \n",
      "35          0.000494            0.414362           0.000535  \n",
      "36          0.000497            0.414240           0.000535  \n",
      "37          0.000516            0.414109           0.000533  \n",
      "38          0.000528            0.413991           0.000525  \n",
      "39          0.000547            0.413878           0.000511  \n",
      "40          0.000560            0.413772           0.000502  \n",
      "41          0.000570            0.413649           0.000491  \n",
      "42          0.000575            0.413545           0.000501  \n",
      "43          0.000565            0.413455           0.000516  \n",
      "44          0.000569            0.413357           0.000526  \n",
      "45          0.000556            0.413259           0.000531  \n",
      "46          0.000563            0.413167           0.000532  \n",
      "47          0.000590            0.413073           0.000515  \n",
      "48          0.000582            0.412987           0.000530  \n",
      "49          0.000585            0.412901           0.000528  \n",
      "50          0.000578            0.412817           0.000534  \n",
      "51          0.000586            0.412743           0.000529  \n",
      "52          0.000585            0.412663           0.000530  \n",
      "53          0.000566            0.412590           0.000541  \n",
      "54          0.000574            0.412524           0.000531  \n",
      "55          0.000579            0.412447           0.000522  \n",
      "56          0.000577            0.412388           0.000519  \n",
      "57          0.000592            0.412318           0.000506  \n",
      "58          0.000585            0.412253           0.000513  \n",
      "59          0.000592            0.412185           0.000513  \n",
      "60          0.000582            0.412130           0.000520  \n",
      "61          0.000586            0.412076           0.000521  \n",
      "62          0.000587            0.412019           0.000526  \n",
      "63          0.000588            0.411963           0.000526  \n",
      "64          0.000597            0.411907           0.000519  \n",
      "65          0.000612            0.411854           0.000504  \n",
      "66          0.000613            0.411794           0.000510  \n",
      "67          0.000622            0.411742           0.000501  \n",
      "68          0.000631            0.411684           0.000493  \n",
      "69          0.000625            0.411629           0.000502  \n",
      "70          0.000614            0.411581           0.000517  \n",
      "71          0.000610            0.411532           0.000522  \n",
      "72          0.000611            0.411425           0.000536  \n",
      "73          0.000606            0.411380           0.000544  \n",
      "74          0.000618            0.411331           0.000543  \n",
      "75          0.000614            0.411284           0.000549  \n",
      "76          0.000618            0.411250           0.000544  \n",
      "77          0.000623            0.411209           0.000549  \n",
      "78          0.000617            0.411156           0.000559  \n",
      "79          0.000612            0.411109           0.000570  \n",
      "80          0.000606            0.411067           0.000574  \n",
      "81          0.000611            0.411029           0.000570  \n",
      "82          0.000610            0.410985           0.000569  \n",
      "83          0.000698            0.410886           0.000465  \n",
      "84          0.000705            0.410842           0.000462  \n",
      "85          0.000713            0.410805           0.000454  \n",
      "86          0.000709            0.410769           0.000459  \n",
      "87          0.000703            0.410727           0.000464  \n",
      "88          0.000706            0.410693           0.000462  \n",
      "89          0.000703            0.410652           0.000467  \n",
      "90          0.000712            0.410616           0.000468  \n",
      "91          0.000713            0.410576           0.000468  \n",
      "92          0.000716            0.410538           0.000466  \n",
      "93          0.000712            0.410499           0.000465  \n",
      "94          0.000720            0.410462           0.000461  \n",
      "95          0.000718            0.410423           0.000460  \n",
      "96          0.000724            0.410388           0.000457  \n",
      "97          0.000718            0.410345           0.000462  \n",
      "98          0.000714            0.410308           0.000466  \n",
      "99          0.000721            0.410272           0.000465  \n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7631   \u001b[0m | \u001b[0m6.027    \u001b[0m | \u001b[0m4.936    \u001b[0m | \u001b[0m0.1156   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655754508\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7674163385\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7687817532\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740342      0.000950           0.596478   \n",
      "1            1       0.746162      0.000415           0.536117   \n",
      "2            2       0.748916      0.001529           0.496718   \n",
      "3            3       0.749516      0.001403           0.471984   \n",
      "4            4       0.749814      0.001567           0.455547   \n",
      "5            5       0.751817      0.001756           0.443802   \n",
      "6            6       0.752606      0.001662           0.435896   \n",
      "7            7       0.753842      0.001266           0.430027   \n",
      "8            8       0.754452      0.001408           0.425843   \n",
      "9            9       0.755531      0.001255           0.422639   \n",
      "10          10       0.756138      0.001362           0.420416   \n",
      "11          11       0.756677      0.001358           0.418762   \n",
      "12          12       0.757075      0.001292           0.417430   \n",
      "13          13       0.757593      0.001281           0.416400   \n",
      "14          14       0.757995      0.001311           0.415691   \n",
      "15          15       0.758303      0.001351           0.415020   \n",
      "16          16       0.758647      0.001365           0.414525   \n",
      "17          17       0.759003      0.001396           0.414082   \n",
      "18          18       0.759348      0.001398           0.413735   \n",
      "19          19       0.759649      0.001428           0.413433   \n",
      "20          20       0.759919      0.001451           0.413183   \n",
      "21          21       0.760231      0.001498           0.412948   \n",
      "22          22       0.760482      0.001509           0.412741   \n",
      "23          23       0.760707      0.001519           0.412529   \n",
      "24          24       0.760939      0.001535           0.412368   \n",
      "25          25       0.761140      0.001570           0.412231   \n",
      "26          26       0.761371      0.001556           0.412068   \n",
      "27          27       0.761570      0.001561           0.411940   \n",
      "28          28       0.761750      0.001573           0.411802   \n",
      "29          29       0.761958      0.001556           0.411680   \n",
      "30          30       0.762131      0.001525           0.411569   \n",
      "31          31       0.762349      0.001528           0.411433   \n",
      "32          32       0.762507      0.001523           0.411344   \n",
      "33          33       0.762656      0.001555           0.411245   \n",
      "34          34       0.762809      0.001567           0.411150   \n",
      "35          35       0.762936      0.001571           0.411067   \n",
      "36          36       0.763051      0.001548           0.410995   \n",
      "37          37       0.763170      0.001554           0.410919   \n",
      "38          38       0.763270      0.001539           0.410862   \n",
      "39          39       0.763426      0.001564           0.410776   \n",
      "40          40       0.763549      0.001548           0.410714   \n",
      "41          41       0.763665      0.001562           0.410641   \n",
      "42          42       0.763787      0.001559           0.410554   \n",
      "43          43       0.763898      0.001571           0.410488   \n",
      "44          44       0.764002      0.001576           0.410418   \n",
      "45          45       0.764090      0.001599           0.410369   \n",
      "46          46       0.764179      0.001623           0.410313   \n",
      "47          47       0.764275      0.001613           0.410246   \n",
      "48          48       0.764368      0.001624           0.410194   \n",
      "49          49       0.764492      0.001615           0.410127   \n",
      "50          50       0.764558      0.001619           0.410083   \n",
      "51          51       0.764620      0.001629           0.410046   \n",
      "52          52       0.764686      0.001652           0.410012   \n",
      "53          53       0.764780      0.001652           0.409958   \n",
      "54          54       0.764864      0.001643           0.409902   \n",
      "55          55       0.764955      0.001655           0.409852   \n",
      "56          56       0.765035      0.001645           0.409802   \n",
      "57          57       0.765121      0.001637           0.409752   \n",
      "58          58       0.765234      0.001617           0.409695   \n",
      "59          59       0.765302      0.001585           0.409654   \n",
      "60          60       0.765363      0.001616           0.409622   \n",
      "61          61       0.765436      0.001587           0.409582   \n",
      "62          62       0.765521      0.001556           0.409536   \n",
      "63          63       0.765584      0.001561           0.409497   \n",
      "64          64       0.765637      0.001574           0.409466   \n",
      "65          65       0.765711      0.001560           0.409427   \n",
      "66          66       0.765793      0.001522           0.409380   \n",
      "67          67       0.765870      0.001538           0.409340   \n",
      "68          68       0.765935      0.001552           0.409306   \n",
      "69          69       0.765997      0.001574           0.409268   \n",
      "70          70       0.766053      0.001570           0.409233   \n",
      "71          71       0.766084      0.001575           0.409214   \n",
      "72          72       0.766142      0.001587           0.409183   \n",
      "73          73       0.766192      0.001574           0.409155   \n",
      "74          74       0.766237      0.001575           0.409131   \n",
      "75          75       0.766271      0.001577           0.409107   \n",
      "76          76       0.766346      0.001558           0.409068   \n",
      "77          77       0.766375      0.001563           0.409044   \n",
      "78          78       0.766410      0.001563           0.409020   \n",
      "79          79       0.766467      0.001589           0.408983   \n",
      "80          80       0.766540      0.001588           0.408944   \n",
      "81          81       0.766573      0.001606           0.408922   \n",
      "82          82       0.766609      0.001609           0.408899   \n",
      "83          83       0.766657      0.001604           0.408877   \n",
      "84          84       0.766691      0.001613           0.408855   \n",
      "85          85       0.766742      0.001624           0.408829   \n",
      "86          86       0.766783      0.001637           0.408807   \n",
      "87          87       0.766823      0.001650           0.408786   \n",
      "88          88       0.766858      0.001635           0.408767   \n",
      "89          89       0.766893      0.001645           0.408751   \n",
      "90          90       0.766910      0.001640           0.408738   \n",
      "91          91       0.766966      0.001618           0.408712   \n",
      "92          92       0.767008      0.001644           0.408687   \n",
      "93          93       0.767043      0.001622           0.408668   \n",
      "94          94       0.767078      0.001629           0.408649   \n",
      "95          95       0.767119      0.001631           0.408623   \n",
      "96          96       0.767174      0.001628           0.408592   \n",
      "97          97       0.767210      0.001616           0.408535   \n",
      "98          98       0.767245      0.001616           0.408516   \n",
      "99          99       0.767258      0.001609           0.408505   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000390            0.596391           0.000431  \n",
      "1           0.000734            0.536050           0.000893  \n",
      "2           0.000566            0.496633           0.000967  \n",
      "3           0.000297            0.471859           0.000754  \n",
      "4           0.000553            0.455391           0.000643  \n",
      "5           0.000412            0.443584           0.000235  \n",
      "6           0.000393            0.435622           0.000324  \n",
      "7           0.000336            0.429682           0.000456  \n",
      "8           0.000396            0.425401           0.000501  \n",
      "9           0.000374            0.422095           0.000580  \n",
      "10          0.000389            0.419775           0.000578  \n",
      "11          0.000496            0.418059           0.000479  \n",
      "12          0.000581            0.416673           0.000485  \n",
      "13          0.000569            0.415574           0.000493  \n",
      "14          0.000628            0.414788           0.000465  \n",
      "15          0.000702            0.414005           0.000385  \n",
      "16          0.000681            0.413429           0.000427  \n",
      "17          0.000676            0.412930           0.000456  \n",
      "18          0.000693            0.412481           0.000438  \n",
      "19          0.000744            0.412084           0.000419  \n",
      "20          0.000766            0.411740           0.000424  \n",
      "21          0.000798            0.411411           0.000402  \n",
      "22          0.000819            0.411095           0.000431  \n",
      "23          0.000836            0.410798           0.000439  \n",
      "24          0.000853            0.410560           0.000412  \n",
      "25          0.000880            0.410312           0.000383  \n",
      "26          0.000903            0.410050           0.000388  \n",
      "27          0.000909            0.409826           0.000368  \n",
      "28          0.000904            0.409634           0.000386  \n",
      "29          0.000898            0.409443           0.000398  \n",
      "30          0.000894            0.409268           0.000400  \n",
      "31          0.000895            0.409038           0.000425  \n",
      "32          0.000896            0.408842           0.000428  \n",
      "33          0.000907            0.408639           0.000428  \n",
      "34          0.000910            0.408454           0.000415  \n",
      "35          0.000895            0.408268           0.000397  \n",
      "36          0.000887            0.408102           0.000403  \n",
      "37          0.000891            0.407939           0.000407  \n",
      "38          0.000879            0.407802           0.000423  \n",
      "39          0.000889            0.407638           0.000439  \n",
      "40          0.000882            0.407474           0.000443  \n",
      "41          0.000882            0.407316           0.000439  \n",
      "42          0.000884            0.407163           0.000446  \n",
      "43          0.000895            0.406997           0.000453  \n",
      "44          0.000900            0.406841           0.000454  \n",
      "45          0.000912            0.406706           0.000429  \n",
      "46          0.000934            0.406578           0.000411  \n",
      "47          0.000932            0.406478           0.000411  \n",
      "48          0.000937            0.406311           0.000429  \n",
      "49          0.000931            0.406172           0.000424  \n",
      "50          0.000923            0.406060           0.000421  \n",
      "51          0.000927            0.405950           0.000413  \n",
      "52          0.000938            0.405819           0.000424  \n",
      "53          0.000942            0.405685           0.000443  \n",
      "54          0.000948            0.405564           0.000446  \n",
      "55          0.000950            0.405424           0.000460  \n",
      "56          0.000945            0.405312           0.000437  \n",
      "57          0.000943            0.405201           0.000447  \n",
      "58          0.000935            0.405075           0.000455  \n",
      "59          0.000922            0.404946           0.000473  \n",
      "60          0.000944            0.404820           0.000458  \n",
      "61          0.000924            0.404724           0.000470  \n",
      "62          0.000910            0.404612           0.000488  \n",
      "63          0.000912            0.404511           0.000482  \n",
      "64          0.000920            0.404378           0.000484  \n",
      "65          0.000912            0.404249           0.000512  \n",
      "66          0.000893            0.404127           0.000527  \n",
      "67          0.000906            0.403991           0.000527  \n",
      "68          0.000909            0.403847           0.000525  \n",
      "69          0.000919            0.403741           0.000521  \n",
      "70          0.000919            0.403609           0.000511  \n",
      "71          0.000918            0.403511           0.000513  \n",
      "72          0.000920            0.403378           0.000512  \n",
      "73          0.000914            0.403270           0.000530  \n",
      "74          0.000907            0.403157           0.000511  \n",
      "75          0.000909            0.403039           0.000539  \n",
      "76          0.000903            0.402914           0.000545  \n",
      "77          0.000910            0.402825           0.000552  \n",
      "78          0.000915            0.402696           0.000570  \n",
      "79          0.000930            0.402558           0.000549  \n",
      "80          0.000934            0.402467           0.000550  \n",
      "81          0.000943            0.402359           0.000524  \n",
      "82          0.000945            0.402230           0.000518  \n",
      "83          0.000945            0.402122           0.000521  \n",
      "84          0.000950            0.402007           0.000545  \n",
      "85          0.000959            0.401897           0.000568  \n",
      "86          0.000965            0.401816           0.000561  \n",
      "87          0.000972            0.401691           0.000540  \n",
      "88          0.000965            0.401571           0.000534  \n",
      "89          0.000968            0.401478           0.000536  \n",
      "90          0.000968            0.401362           0.000517  \n",
      "91          0.000956            0.401227           0.000532  \n",
      "92          0.000973            0.401113           0.000539  \n",
      "93          0.000960            0.401003           0.000556  \n",
      "94          0.000965            0.400888           0.000557  \n",
      "95          0.000961            0.400785           0.000567  \n",
      "96          0.000955            0.400650           0.000573  \n",
      "97          0.000897            0.400518           0.000598  \n",
      "98          0.000899            0.400422           0.000601  \n",
      "99          0.000898            0.400338           0.000603  \n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7673   \u001b[0m | \u001b[0m0.675    \u001b[0m | \u001b[0m9.197    \u001b[0m | \u001b[0m0.1601   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7630135427\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7646844279\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7656728419\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.715524      0.007015           0.582424   \n",
      "1            1       0.727153      0.001703           0.519547   \n",
      "2            2       0.734646      0.002829           0.483371   \n",
      "3            3       0.739279      0.003039           0.461150   \n",
      "4            4       0.742760      0.001543           0.446914   \n",
      "5            5       0.745420      0.001590           0.437696   \n",
      "6            6       0.746949      0.001622           0.431627   \n",
      "7            7       0.748491      0.001540           0.427738   \n",
      "8            8       0.749482      0.001685           0.424719   \n",
      "9            9       0.750291      0.001549           0.422767   \n",
      "10          10       0.751072      0.001646           0.421365   \n",
      "11          11       0.752067      0.001471           0.420193   \n",
      "12          12       0.752526      0.001312           0.419307   \n",
      "13          13       0.753068      0.001137           0.418668   \n",
      "14          14       0.753688      0.001331           0.418127   \n",
      "15          15       0.754258      0.001375           0.417536   \n",
      "16          16       0.754666      0.001311           0.417106   \n",
      "17          17       0.754962      0.001422           0.416754   \n",
      "18          18       0.755213      0.001417           0.416251   \n",
      "19          19       0.755499      0.001404           0.415972   \n",
      "20          20       0.755778      0.001379           0.415739   \n",
      "21          21       0.756039      0.001379           0.415494   \n",
      "22          22       0.756373      0.001387           0.415279   \n",
      "23          23       0.756560      0.001369           0.415136   \n",
      "24          24       0.756845      0.001371           0.414939   \n",
      "25          25       0.757015      0.001398           0.414793   \n",
      "26          26       0.757241      0.001408           0.414648   \n",
      "27          27       0.757428      0.001401           0.414506   \n",
      "28          28       0.757639      0.001441           0.414374   \n",
      "29          29       0.757854      0.001481           0.414238   \n",
      "30          30       0.758050      0.001471           0.414119   \n",
      "31          31       0.758232      0.001478           0.414002   \n",
      "32          32       0.758431      0.001504           0.413890   \n",
      "33          33       0.758563      0.001501           0.413781   \n",
      "34          34       0.758730      0.001415           0.413678   \n",
      "35          35       0.758839      0.001428           0.413606   \n",
      "36          36       0.758930      0.001427           0.413531   \n",
      "37          37       0.759074      0.001380           0.413449   \n",
      "38          38       0.759202      0.001428           0.413378   \n",
      "39          39       0.759307      0.001416           0.413313   \n",
      "40          40       0.759488      0.001378           0.413218   \n",
      "41          41       0.759592      0.001368           0.413154   \n",
      "42          42       0.759712      0.001355           0.413073   \n",
      "43          43       0.759802      0.001361           0.413021   \n",
      "44          44       0.759925      0.001376           0.412949   \n",
      "45          45       0.760077      0.001316           0.412868   \n",
      "46          46       0.760226      0.001315           0.412786   \n",
      "47          47       0.760336      0.001298           0.412722   \n",
      "48          48       0.760431      0.001316           0.412666   \n",
      "49          49       0.760556      0.001313           0.412603   \n",
      "50          50       0.760668      0.001342           0.412539   \n",
      "51          51       0.760790      0.001364           0.412461   \n",
      "52          52       0.760863      0.001367           0.412415   \n",
      "53          53       0.760976      0.001322           0.412353   \n",
      "54          54       0.761061      0.001321           0.412302   \n",
      "55          55       0.761174      0.001317           0.412241   \n",
      "56          56       0.761282      0.001334           0.412183   \n",
      "57          57       0.761364      0.001349           0.412135   \n",
      "58          58       0.761445      0.001375           0.412088   \n",
      "59          59       0.761540      0.001369           0.412033   \n",
      "60          60       0.761604      0.001364           0.411986   \n",
      "61          61       0.761701      0.001332           0.411929   \n",
      "62          62       0.761787      0.001290           0.411803   \n",
      "63          63       0.761860      0.001298           0.411753   \n",
      "64          64       0.761954      0.001320           0.411545   \n",
      "65          65       0.762036      0.001351           0.411497   \n",
      "66          66       0.762112      0.001332           0.411448   \n",
      "67          67       0.762212      0.001324           0.411399   \n",
      "68          68       0.762288      0.001311           0.411352   \n",
      "69          69       0.762362      0.001327           0.411313   \n",
      "70          70       0.762425      0.001339           0.411274   \n",
      "71          71       0.762512      0.001329           0.411232   \n",
      "72          72       0.762595      0.001348           0.411184   \n",
      "73          73       0.762681      0.001372           0.411131   \n",
      "74          74       0.762789      0.001372           0.411065   \n",
      "75          75       0.762881      0.001347           0.411021   \n",
      "76          76       0.762939      0.001350           0.410979   \n",
      "77          77       0.762999      0.001314           0.410944   \n",
      "78          78       0.763061      0.001330           0.410908   \n",
      "79          79       0.763115      0.001324           0.410869   \n",
      "80          80       0.763190      0.001317           0.410829   \n",
      "81          81       0.763264      0.001332           0.410791   \n",
      "82          82       0.763332      0.001326           0.410752   \n",
      "83          83       0.763410      0.001308           0.410708   \n",
      "84          84       0.763482      0.001339           0.410662   \n",
      "85          85       0.763553      0.001344           0.410625   \n",
      "86          86       0.763631      0.001349           0.410581   \n",
      "87          87       0.763693      0.001330           0.410543   \n",
      "88          88       0.763743      0.001323           0.410512   \n",
      "89          89       0.763799      0.001336           0.410477   \n",
      "90          90       0.763859      0.001297           0.410444   \n",
      "91          91       0.763936      0.001312           0.410402   \n",
      "92          92       0.764029      0.001301           0.410350   \n",
      "93          93       0.764099      0.001328           0.410308   \n",
      "94          94       0.764152      0.001329           0.410275   \n",
      "95          95       0.764227      0.001350           0.410241   \n",
      "96          96       0.764294      0.001339           0.410205   \n",
      "97          97       0.764346      0.001310           0.410178   \n",
      "98          98       0.764413      0.001322           0.410139   \n",
      "99          99       0.764457      0.001344           0.410115   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001551            0.582359           0.001317  \n",
      "1           0.001714            0.519514           0.001301  \n",
      "2           0.001577            0.483319           0.001319  \n",
      "3           0.000645            0.461087           0.000158  \n",
      "4           0.000804            0.446849           0.000723  \n",
      "5           0.000702            0.437605           0.000106  \n",
      "6           0.000582            0.431517           0.000401  \n",
      "7           0.000344            0.427621           0.000467  \n",
      "8           0.000377            0.424559           0.000427  \n",
      "9           0.000388            0.422598           0.000448  \n",
      "10          0.000478            0.421174           0.000399  \n",
      "11          0.000374            0.419996           0.000462  \n",
      "12          0.000359            0.419092           0.000536  \n",
      "13          0.000377            0.418459           0.000510  \n",
      "14          0.000389            0.417913           0.000518  \n",
      "15          0.000320            0.417309           0.000619  \n",
      "16          0.000346            0.416896           0.000601  \n",
      "17          0.000396            0.416530           0.000572  \n",
      "18          0.000725            0.416022           0.000324  \n",
      "19          0.000713            0.415734           0.000305  \n",
      "20          0.000695            0.415490           0.000313  \n",
      "21          0.000711            0.415226           0.000326  \n",
      "22          0.000727            0.414995           0.000338  \n",
      "23          0.000752            0.414836           0.000339  \n",
      "24          0.000736            0.414634           0.000337  \n",
      "25          0.000743            0.414474           0.000343  \n",
      "26          0.000749            0.414316           0.000337  \n",
      "27          0.000760            0.414156           0.000313  \n",
      "28          0.000782            0.414005           0.000293  \n",
      "29          0.000792            0.413861           0.000279  \n",
      "30          0.000797            0.413745           0.000279  \n",
      "31          0.000816            0.413625           0.000267  \n",
      "32          0.000829            0.413506           0.000261  \n",
      "33          0.000813            0.413397           0.000282  \n",
      "34          0.000787            0.413289           0.000304  \n",
      "35          0.000787            0.413207           0.000298  \n",
      "36          0.000772            0.413125           0.000309  \n",
      "37          0.000750            0.413039           0.000329  \n",
      "38          0.000770            0.412958           0.000317  \n",
      "39          0.000770            0.412879           0.000327  \n",
      "40          0.000769            0.412774           0.000324  \n",
      "41          0.000767            0.412695           0.000329  \n",
      "42          0.000762            0.412605           0.000342  \n",
      "43          0.000758            0.412538           0.000347  \n",
      "44          0.000761            0.412461           0.000340  \n",
      "45          0.000737            0.412373           0.000360  \n",
      "46          0.000734            0.412286           0.000373  \n",
      "47          0.000733            0.412209           0.000376  \n",
      "48          0.000746            0.412146           0.000372  \n",
      "49          0.000738            0.412073           0.000374  \n",
      "50          0.000755            0.411992           0.000371  \n",
      "51          0.000783            0.411917           0.000354  \n",
      "52          0.000785            0.411864           0.000355  \n",
      "53          0.000770            0.411799           0.000358  \n",
      "54          0.000772            0.411742           0.000356  \n",
      "55          0.000771            0.411677           0.000359  \n",
      "56          0.000762            0.411611           0.000372  \n",
      "57          0.000766            0.411556           0.000376  \n",
      "58          0.000770            0.411504           0.000380  \n",
      "59          0.000778            0.411443           0.000380  \n",
      "60          0.000784            0.411387           0.000377  \n",
      "61          0.000768            0.411325           0.000391  \n",
      "62          0.000751            0.411190           0.000428  \n",
      "63          0.000750            0.411136           0.000432  \n",
      "64          0.000748            0.410921           0.000418  \n",
      "65          0.000767            0.410861           0.000409  \n",
      "66          0.000762            0.410807           0.000416  \n",
      "67          0.000759            0.410754           0.000422  \n",
      "68          0.000753            0.410697           0.000426  \n",
      "69          0.000762            0.410651           0.000422  \n",
      "70          0.000770            0.410605           0.000418  \n",
      "71          0.000764            0.410552           0.000424  \n",
      "72          0.000764            0.410497           0.000425  \n",
      "73          0.000766            0.410440           0.000428  \n",
      "74          0.000778            0.410379           0.000409  \n",
      "75          0.000767            0.410326           0.000414  \n",
      "76          0.000768            0.410274           0.000414  \n",
      "77          0.000760            0.410227           0.000419  \n",
      "78          0.000765            0.410183           0.000419  \n",
      "79          0.000763            0.410133           0.000421  \n",
      "80          0.000761            0.410081           0.000427  \n",
      "81          0.000764            0.410037           0.000425  \n",
      "82          0.000763            0.409987           0.000430  \n",
      "83          0.000760            0.409937           0.000429  \n",
      "84          0.000770            0.409886           0.000419  \n",
      "85          0.000769            0.409842           0.000420  \n",
      "86          0.000769            0.409792           0.000425  \n",
      "87          0.000755            0.409747           0.000435  \n",
      "88          0.000752            0.409702           0.000440  \n",
      "89          0.000759            0.409655           0.000443  \n",
      "90          0.000743            0.409614           0.000454  \n",
      "91          0.000756            0.409565           0.000444  \n",
      "92          0.000753            0.409514           0.000439  \n",
      "93          0.000768            0.409466           0.000423  \n",
      "94          0.000767            0.409421           0.000427  \n",
      "95          0.000773            0.409376           0.000425  \n",
      "96          0.000772            0.409329           0.000425  \n",
      "97          0.000759            0.409292           0.000431  \n",
      "98          0.000766            0.409247           0.000424  \n",
      "99          0.000771            0.409215           0.000422  \n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.7645   \u001b[0m | \u001b[0m7.11     \u001b[0m | \u001b[0m4.124    \u001b[0m | \u001b[0m0.197    \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7618307811\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7636132248\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7644847522\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.724845      0.005877           0.620927   \n",
      "1            1       0.734270      0.001179           0.569356   \n",
      "2            2       0.739758      0.000820           0.531976   \n",
      "3            3       0.741246      0.001194           0.504805   \n",
      "4            4       0.742422      0.001415           0.484778   \n",
      "5            5       0.744637      0.001094           0.469390   \n",
      "6            6       0.746197      0.001092           0.457863   \n",
      "7            7       0.747415      0.001219           0.449173   \n",
      "8            8       0.748556      0.000915           0.442393   \n",
      "9            9       0.749308      0.001238           0.437193   \n",
      "10          10       0.750115      0.001259           0.432953   \n",
      "11          11       0.750905      0.001444           0.429771   \n",
      "12          12       0.751473      0.001259           0.427272   \n",
      "13          13       0.752054      0.001282           0.425175   \n",
      "14          14       0.752631      0.001110           0.423531   \n",
      "15          15       0.753102      0.001189           0.422253   \n",
      "16          16       0.753539      0.001209           0.421235   \n",
      "17          17       0.753781      0.001179           0.420297   \n",
      "18          18       0.754119      0.001230           0.419506   \n",
      "19          19       0.754400      0.001169           0.418831   \n",
      "20          20       0.754736      0.001079           0.418280   \n",
      "21          21       0.755077      0.001192           0.417844   \n",
      "22          22       0.755296      0.001250           0.417440   \n",
      "23          23       0.755540      0.001199           0.417088   \n",
      "24          24       0.755731      0.001162           0.416773   \n",
      "25          25       0.755995      0.001123           0.416468   \n",
      "26          26       0.756217      0.001111           0.416174   \n",
      "27          27       0.756479      0.001174           0.415929   \n",
      "28          28       0.756683      0.001229           0.415732   \n",
      "29          29       0.756863      0.001237           0.415534   \n",
      "30          30       0.757072      0.001263           0.415379   \n",
      "31          31       0.757288      0.001267           0.415189   \n",
      "32          32       0.757469      0.001241           0.415033   \n",
      "33          33       0.757657      0.001240           0.414874   \n",
      "34          34       0.757814      0.001230           0.414739   \n",
      "35          35       0.757985      0.001226           0.414618   \n",
      "36          36       0.758107      0.001184           0.414483   \n",
      "37          37       0.758246      0.001238           0.414368   \n",
      "38          38       0.758401      0.001218           0.414268   \n",
      "39          39       0.758550      0.001238           0.414138   \n",
      "40          40       0.758720      0.001216           0.414033   \n",
      "41          41       0.758866      0.001251           0.413927   \n",
      "42          42       0.759020      0.001238           0.413826   \n",
      "43          43       0.759141      0.001238           0.413746   \n",
      "44          44       0.759265      0.001257           0.413658   \n",
      "45          45       0.759372      0.001240           0.413485   \n",
      "46          46       0.759495      0.001244           0.413406   \n",
      "47          47       0.759598      0.001246           0.413338   \n",
      "48          48       0.759693      0.001258           0.413256   \n",
      "49          49       0.759814      0.001241           0.413180   \n",
      "50          50       0.759940      0.001235           0.413105   \n",
      "51          51       0.760029      0.001238           0.413042   \n",
      "52          52       0.760129      0.001236           0.412974   \n",
      "53          53       0.760231      0.001270           0.412906   \n",
      "54          54       0.760319      0.001253           0.412846   \n",
      "55          55       0.760391      0.001271           0.412787   \n",
      "56          56       0.760486      0.001261           0.412726   \n",
      "57          57       0.760575      0.001268           0.412672   \n",
      "58          58       0.760677      0.001255           0.412610   \n",
      "59          59       0.760767      0.001279           0.412553   \n",
      "60          60       0.760844      0.001284           0.412501   \n",
      "61          61       0.760914      0.001299           0.412453   \n",
      "62          62       0.760986      0.001307           0.412409   \n",
      "63          63       0.761060      0.001295           0.412304   \n",
      "64          64       0.761122      0.001284           0.412209   \n",
      "65          65       0.761201      0.001274           0.412156   \n",
      "66          66       0.761274      0.001273           0.412116   \n",
      "67          67       0.761354      0.001273           0.412071   \n",
      "68          68       0.761407      0.001280           0.412040   \n",
      "69          69       0.761468      0.001303           0.412004   \n",
      "70          70       0.761523      0.001308           0.411898   \n",
      "71          71       0.761591      0.001316           0.411861   \n",
      "72          72       0.761684      0.001345           0.411808   \n",
      "73          73       0.761755      0.001345           0.411766   \n",
      "74          74       0.761840      0.001328           0.411721   \n",
      "75          75       0.761906      0.001317           0.411682   \n",
      "76          76       0.761971      0.001332           0.411643   \n",
      "77          77       0.762018      0.001333           0.411614   \n",
      "78          78       0.762091      0.001338           0.411576   \n",
      "79          79       0.762157      0.001331           0.411540   \n",
      "80          80       0.762236      0.001305           0.411492   \n",
      "81          81       0.762309      0.001312           0.411452   \n",
      "82          82       0.762356      0.001293           0.411413   \n",
      "83          83       0.762411      0.001295           0.411380   \n",
      "84          84       0.762474      0.001311           0.411342   \n",
      "85          85       0.762524      0.001298           0.411312   \n",
      "86          86       0.762603      0.001326           0.411276   \n",
      "87          87       0.762664      0.001325           0.411243   \n",
      "88          88       0.762737      0.001342           0.411197   \n",
      "89          89       0.762790      0.001342           0.411165   \n",
      "90          90       0.762833      0.001340           0.411136   \n",
      "91          91       0.762885      0.001341           0.411107   \n",
      "92          92       0.762947      0.001334           0.411073   \n",
      "93          93       0.763016      0.001359           0.411037   \n",
      "94          94       0.763060      0.001353           0.411008   \n",
      "95          95       0.763123      0.001370           0.410975   \n",
      "96          96       0.763160      0.001375           0.410954   \n",
      "97          97       0.763205      0.001353           0.410925   \n",
      "98          98       0.763261      0.001343           0.410893   \n",
      "99          99       0.763310      0.001353           0.410863   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001141            0.620849           0.001012  \n",
      "1           0.000426            0.569275           0.000217  \n",
      "2           0.000126            0.531901           0.000278  \n",
      "3           0.000535            0.504741           0.000720  \n",
      "4           0.000292            0.484717           0.000425  \n",
      "5           0.000031            0.469313           0.000542  \n",
      "6           0.000427            0.457779           0.000199  \n",
      "7           0.000469            0.449087           0.000151  \n",
      "8           0.000618            0.442297           0.000067  \n",
      "9           0.000362            0.437080           0.000290  \n",
      "10          0.000330            0.432842           0.000384  \n",
      "11          0.000423            0.429655           0.000298  \n",
      "12          0.000398            0.427154           0.000352  \n",
      "13          0.000335            0.425058           0.000405  \n",
      "14          0.000265            0.423391           0.000503  \n",
      "15          0.000435            0.422107           0.000349  \n",
      "16          0.000378            0.421068           0.000403  \n",
      "17          0.000434            0.420126           0.000381  \n",
      "18          0.000441            0.419318           0.000402  \n",
      "19          0.000425            0.418642           0.000434  \n",
      "20          0.000454            0.418081           0.000429  \n",
      "21          0.000453            0.417645           0.000447  \n",
      "22          0.000445            0.417229           0.000481  \n",
      "23          0.000410            0.416874           0.000530  \n",
      "24          0.000404            0.416554           0.000546  \n",
      "25          0.000410            0.416244           0.000555  \n",
      "26          0.000418            0.415945           0.000552  \n",
      "27          0.000480            0.415690           0.000497  \n",
      "28          0.000491            0.415485           0.000498  \n",
      "29          0.000496            0.415287           0.000504  \n",
      "30          0.000511            0.415130           0.000501  \n",
      "31          0.000499            0.414935           0.000520  \n",
      "32          0.000470            0.414766           0.000547  \n",
      "33          0.000480            0.414601           0.000548  \n",
      "34          0.000496            0.414468           0.000543  \n",
      "35          0.000488            0.414336           0.000557  \n",
      "36          0.000505            0.414191           0.000547  \n",
      "37          0.000518            0.414072           0.000543  \n",
      "38          0.000515            0.413962           0.000553  \n",
      "39          0.000518            0.413831           0.000551  \n",
      "40          0.000524            0.413721           0.000546  \n",
      "41          0.000552            0.413616           0.000526  \n",
      "42          0.000554            0.413513           0.000526  \n",
      "43          0.000545            0.413419           0.000542  \n",
      "44          0.000532            0.413324           0.000559  \n",
      "45          0.000687            0.413146           0.000404  \n",
      "46          0.000693            0.413061           0.000399  \n",
      "47          0.000693            0.412986           0.000402  \n",
      "48          0.000707            0.412892           0.000398  \n",
      "49          0.000700            0.412808           0.000407  \n",
      "50          0.000696            0.412725           0.000416  \n",
      "51          0.000693            0.412651           0.000421  \n",
      "52          0.000678            0.412572           0.000438  \n",
      "53          0.000690            0.412498           0.000428  \n",
      "54          0.000687            0.412427           0.000440  \n",
      "55          0.000700            0.412361           0.000428  \n",
      "56          0.000697            0.412290           0.000437  \n",
      "57          0.000701            0.412226           0.000437  \n",
      "58          0.000698            0.412157           0.000441  \n",
      "59          0.000704            0.412093           0.000441  \n",
      "60          0.000719            0.412025           0.000427  \n",
      "61          0.000727            0.411974           0.000422  \n",
      "62          0.000734            0.411922           0.000419  \n",
      "63          0.000729            0.411815           0.000431  \n",
      "64          0.000796            0.411711           0.000361  \n",
      "65          0.000802            0.411654           0.000360  \n",
      "66          0.000803            0.411607           0.000365  \n",
      "67          0.000798            0.411555           0.000375  \n",
      "68          0.000799            0.411512           0.000373  \n",
      "69          0.000809            0.411467           0.000367  \n",
      "70          0.000709            0.411356           0.000461  \n",
      "71          0.000714            0.411308           0.000458  \n",
      "72          0.000731            0.411247           0.000450  \n",
      "73          0.000734            0.411195           0.000447  \n",
      "74          0.000719            0.411142           0.000464  \n",
      "75          0.000718            0.411097           0.000465  \n",
      "76          0.000728            0.411046           0.000461  \n",
      "77          0.000731            0.411005           0.000460  \n",
      "78          0.000737            0.410959           0.000451  \n",
      "79          0.000732            0.410915           0.000457  \n",
      "80          0.000726            0.410857           0.000464  \n",
      "81          0.000731            0.410812           0.000463  \n",
      "82          0.000720            0.410767           0.000474  \n",
      "83          0.000727            0.410723           0.000469  \n",
      "84          0.000740            0.410673           0.000461  \n",
      "85          0.000730            0.410631           0.000469  \n",
      "86          0.000741            0.410586           0.000461  \n",
      "87          0.000742            0.410543           0.000462  \n",
      "88          0.000752            0.410489           0.000452  \n",
      "89          0.000756            0.410450           0.000442  \n",
      "90          0.000751            0.410412           0.000448  \n",
      "91          0.000750            0.410374           0.000453  \n",
      "92          0.000749            0.410334           0.000450  \n",
      "93          0.000757            0.410295           0.000449  \n",
      "94          0.000758            0.410257           0.000450  \n",
      "95          0.000766            0.410219           0.000445  \n",
      "96          0.000764            0.410190           0.000444  \n",
      "97          0.000753            0.410155           0.000449  \n",
      "98          0.000751            0.410117           0.000455  \n",
      "99          0.000755            0.410084           0.000451  \n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.7633   \u001b[0m | \u001b[0m8.341    \u001b[0m | \u001b[0m5.274    \u001b[0m | \u001b[0m0.1182   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7638330225\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7657977778\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.766927346\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.728347      0.005430           0.602208   \n",
      "1            1       0.738288      0.002175           0.543811   \n",
      "2            2       0.742196      0.002051           0.505117   \n",
      "3            3       0.745425      0.000732           0.478387   \n",
      "4            4       0.746593      0.000766           0.460944   \n",
      "5            5       0.748031      0.001302           0.449354   \n",
      "6            6       0.749639      0.001193           0.440572   \n",
      "7            7       0.750462      0.000838           0.434315   \n",
      "8            8       0.751584      0.001107           0.429631   \n",
      "9            9       0.752607      0.001162           0.426057   \n",
      "10          10       0.753143      0.001264           0.423712   \n",
      "11          11       0.753694      0.001216           0.421679   \n",
      "12          12       0.754127      0.001197           0.420293   \n",
      "13          13       0.754715      0.001298           0.419038   \n",
      "14          14       0.755099      0.001385           0.418165   \n",
      "15          15       0.755565      0.001468           0.417437   \n",
      "16          16       0.756012      0.001460           0.416816   \n",
      "17          17       0.756428      0.001424           0.416343   \n",
      "18          18       0.756715      0.001338           0.415930   \n",
      "19          19       0.757017      0.001374           0.415559   \n",
      "20          20       0.757230      0.001344           0.415239   \n",
      "21          21       0.757563      0.001385           0.414951   \n",
      "22          22       0.757836      0.001460           0.414712   \n",
      "23          23       0.758023      0.001411           0.414473   \n",
      "24          24       0.758293      0.001466           0.414285   \n",
      "25          25       0.758498      0.001484           0.414113   \n",
      "26          26       0.758740      0.001516           0.413939   \n",
      "27          27       0.758964      0.001490           0.413780   \n",
      "28          28       0.759186      0.001517           0.413619   \n",
      "29          29       0.759392      0.001560           0.413462   \n",
      "30          30       0.759585      0.001563           0.413331   \n",
      "31          31       0.759777      0.001572           0.413202   \n",
      "32          32       0.759952      0.001571           0.413087   \n",
      "33          33       0.760060      0.001580           0.413002   \n",
      "34          34       0.760168      0.001565           0.412904   \n",
      "35          35       0.760330      0.001604           0.412798   \n",
      "36          36       0.760492      0.001616           0.412691   \n",
      "37          37       0.760665      0.001590           0.412589   \n",
      "38          38       0.760768      0.001560           0.412495   \n",
      "39          39       0.760899      0.001550           0.412426   \n",
      "40          40       0.760993      0.001564           0.412358   \n",
      "41          41       0.761105      0.001561           0.412227   \n",
      "42          42       0.761197      0.001599           0.412161   \n",
      "43          43       0.761339      0.001603           0.412080   \n",
      "44          44       0.761435      0.001597           0.412015   \n",
      "45          45       0.761582      0.001571           0.411931   \n",
      "46          46       0.761671      0.001546           0.411866   \n",
      "47          47       0.761753      0.001509           0.411808   \n",
      "48          48       0.761854      0.001535           0.411748   \n",
      "49          49       0.761969      0.001529           0.411679   \n",
      "50          50       0.762107      0.001554           0.411603   \n",
      "51          51       0.762214      0.001563           0.411554   \n",
      "52          52       0.762297      0.001552           0.411496   \n",
      "53          53       0.762428      0.001526           0.411421   \n",
      "54          54       0.762542      0.001519           0.411358   \n",
      "55          55       0.762641      0.001484           0.411303   \n",
      "56          56       0.762734      0.001502           0.411246   \n",
      "57          57       0.762812      0.001515           0.411197   \n",
      "58          58       0.762905      0.001514           0.411142   \n",
      "59          59       0.762994      0.001535           0.411090   \n",
      "60          60       0.763102      0.001537           0.411033   \n",
      "61          61       0.763163      0.001519           0.410990   \n",
      "62          62       0.763232      0.001509           0.410943   \n",
      "63          63       0.763294      0.001514           0.410904   \n",
      "64          64       0.763356      0.001469           0.410867   \n",
      "65          65       0.763421      0.001452           0.410824   \n",
      "66          66       0.763476      0.001456           0.410795   \n",
      "67          67       0.763574      0.001449           0.410745   \n",
      "68          68       0.763658      0.001447           0.410654   \n",
      "69          69       0.763728      0.001459           0.410610   \n",
      "70          70       0.763823      0.001428           0.410556   \n",
      "71          71       0.763886      0.001426           0.410472   \n",
      "72          72       0.763934      0.001432           0.410440   \n",
      "73          73       0.764015      0.001441           0.410394   \n",
      "74          74       0.764098      0.001419           0.410350   \n",
      "75          75       0.764148      0.001426           0.410320   \n",
      "76          76       0.764209      0.001438           0.410286   \n",
      "77          77       0.764258      0.001443           0.410255   \n",
      "78          78       0.764313      0.001469           0.410220   \n",
      "79          79       0.764385      0.001492           0.410179   \n",
      "80          80       0.764452      0.001492           0.410135   \n",
      "81          81       0.764510      0.001526           0.410100   \n",
      "82          82       0.764572      0.001533           0.410064   \n",
      "83          83       0.764633      0.001536           0.410033   \n",
      "84          84       0.764676      0.001535           0.410005   \n",
      "85          85       0.764739      0.001535           0.409966   \n",
      "86          86       0.764784      0.001537           0.409935   \n",
      "87          87       0.764830      0.001558           0.409906   \n",
      "88          88       0.764884      0.001556           0.409872   \n",
      "89          89       0.764972      0.001539           0.409829   \n",
      "90          90       0.765043      0.001557           0.409785   \n",
      "91          91       0.765119      0.001541           0.409740   \n",
      "92          92       0.765167      0.001526           0.409709   \n",
      "93          93       0.765219      0.001531           0.409677   \n",
      "94          94       0.765274      0.001533           0.409641   \n",
      "95          95       0.765308      0.001529           0.409619   \n",
      "96          96       0.765345      0.001540           0.409597   \n",
      "97          97       0.765403      0.001545           0.409568   \n",
      "98          98       0.765468      0.001552           0.409535   \n",
      "99          99       0.765519      0.001566           0.409504   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001318            0.602122           0.001224  \n",
      "1           0.001221            0.543741           0.001087  \n",
      "2           0.001059            0.505063           0.000734  \n",
      "3           0.000702            0.478322           0.000342  \n",
      "4           0.000523            0.460840           0.000050  \n",
      "5           0.000670            0.449221           0.000119  \n",
      "6           0.000783            0.440417           0.000404  \n",
      "7           0.000518            0.434125           0.000343  \n",
      "8           0.000694            0.429446           0.000231  \n",
      "9           0.000603            0.425865           0.000319  \n",
      "10          0.000380            0.423491           0.000552  \n",
      "11          0.000393            0.421451           0.000489  \n",
      "12          0.000573            0.420047           0.000338  \n",
      "13          0.000583            0.418786           0.000359  \n",
      "14          0.000593            0.417882           0.000360  \n",
      "15          0.000644            0.417136           0.000333  \n",
      "16          0.000615            0.416497           0.000386  \n",
      "17          0.000604            0.416015           0.000397  \n",
      "18          0.000645            0.415580           0.000368  \n",
      "19          0.000632            0.415192           0.000398  \n",
      "20          0.000638            0.414859           0.000409  \n",
      "21          0.000660            0.414557           0.000410  \n",
      "22          0.000700            0.414292           0.000405  \n",
      "23          0.000701            0.414023           0.000417  \n",
      "24          0.000716            0.413816           0.000411  \n",
      "25          0.000717            0.413628           0.000428  \n",
      "26          0.000748            0.413435           0.000420  \n",
      "27          0.000731            0.413255           0.000434  \n",
      "28          0.000756            0.413086           0.000415  \n",
      "29          0.000757            0.412918           0.000404  \n",
      "30          0.000762            0.412767           0.000410  \n",
      "31          0.000769            0.412626           0.000414  \n",
      "32          0.000767            0.412490           0.000421  \n",
      "33          0.000775            0.412383           0.000412  \n",
      "34          0.000793            0.412274           0.000399  \n",
      "35          0.000801            0.412146           0.000392  \n",
      "36          0.000817            0.412025           0.000377  \n",
      "37          0.000813            0.411910           0.000386  \n",
      "38          0.000813            0.411802           0.000394  \n",
      "39          0.000804            0.411712           0.000401  \n",
      "40          0.000805            0.411622           0.000401  \n",
      "41          0.000897            0.411479           0.000319  \n",
      "42          0.000908            0.411403           0.000313  \n",
      "43          0.000911            0.411302           0.000316  \n",
      "44          0.000911            0.411219           0.000316  \n",
      "45          0.000898            0.411117           0.000328  \n",
      "46          0.000912            0.411041           0.000328  \n",
      "47          0.000896            0.410960           0.000338  \n",
      "48          0.000912            0.410881           0.000328  \n",
      "49          0.000909            0.410794           0.000331  \n",
      "50          0.000920            0.410710           0.000325  \n",
      "51          0.000924            0.410639           0.000324  \n",
      "52          0.000927            0.410564           0.000327  \n",
      "53          0.000914            0.410476           0.000341  \n",
      "54          0.000913            0.410403           0.000346  \n",
      "55          0.000905            0.410334           0.000352  \n",
      "56          0.000924            0.410265           0.000333  \n",
      "57          0.000925            0.410202           0.000330  \n",
      "58          0.000929            0.410132           0.000332  \n",
      "59          0.000928            0.410068           0.000324  \n",
      "60          0.000931            0.409999           0.000317  \n",
      "61          0.000927            0.409940           0.000324  \n",
      "62          0.000923            0.409874           0.000332  \n",
      "63          0.000917            0.409823           0.000338  \n",
      "64          0.000901            0.409775           0.000342  \n",
      "65          0.000897            0.409713           0.000340  \n",
      "66          0.000901            0.409663           0.000336  \n",
      "67          0.000905            0.409598           0.000338  \n",
      "68          0.000895            0.409492           0.000358  \n",
      "69          0.000906            0.409436           0.000353  \n",
      "70          0.000888            0.409365           0.000367  \n",
      "71          0.000810            0.409267           0.000445  \n",
      "72          0.000806            0.409219           0.000446  \n",
      "73          0.000813            0.409160           0.000443  \n",
      "74          0.000807            0.409108           0.000447  \n",
      "75          0.000812            0.409059           0.000449  \n",
      "76          0.000818            0.409011           0.000441  \n",
      "77          0.000818            0.408967           0.000437  \n",
      "78          0.000837            0.408918           0.000433  \n",
      "79          0.000850            0.408867           0.000423  \n",
      "80          0.000841            0.408811           0.000430  \n",
      "81          0.000859            0.408761           0.000423  \n",
      "82          0.000860            0.408712           0.000424  \n",
      "83          0.000859            0.408661           0.000423  \n",
      "84          0.000861            0.408615           0.000424  \n",
      "85          0.000863            0.408558           0.000429  \n",
      "86          0.000860            0.408508           0.000432  \n",
      "87          0.000871            0.408461           0.000425  \n",
      "88          0.000869            0.408412           0.000426  \n",
      "89          0.000867            0.408353           0.000421  \n",
      "90          0.000882            0.408295           0.000408  \n",
      "91          0.000874            0.408234           0.000407  \n",
      "92          0.000871            0.408182           0.000405  \n",
      "93          0.000867            0.408132           0.000414  \n",
      "94          0.000871            0.408079           0.000416  \n",
      "95          0.000873            0.408030           0.000423  \n",
      "96          0.000878            0.407986           0.000423  \n",
      "97          0.000878            0.407939           0.000419  \n",
      "98          0.000881            0.407887           0.000422  \n",
      "99          0.000887            0.407835           0.000415  \n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.7655   \u001b[0m | \u001b[0m1.916    \u001b[0m | \u001b[0m5.825    \u001b[0m | \u001b[0m0.1525   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.764290347\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7662232001\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7673530671\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.728347      0.005430           0.597782   \n",
      "1            1       0.739904      0.000713           0.537493   \n",
      "2            2       0.742979      0.000650           0.499548   \n",
      "3            3       0.745958      0.000924           0.473832   \n",
      "4            4       0.747908      0.000914           0.456647   \n",
      "5            5       0.749266      0.001170           0.445519   \n",
      "6            6       0.750626      0.001884           0.437423   \n",
      "7            7       0.751894      0.001655           0.431468   \n",
      "8            8       0.752741      0.001592           0.427364   \n",
      "9            9       0.753411      0.001735           0.424248   \n",
      "10          10       0.754093      0.001658           0.422000   \n",
      "11          11       0.754645      0.001569           0.420336   \n",
      "12          12       0.754991      0.001649           0.419185   \n",
      "13          13       0.755496      0.001654           0.418160   \n",
      "14          14       0.755858      0.001656           0.417359   \n",
      "15          15       0.756173      0.001585           0.416728   \n",
      "16          16       0.756561      0.001541           0.416179   \n",
      "17          17       0.756835      0.001528           0.415753   \n",
      "18          18       0.757222      0.001490           0.415374   \n",
      "19          19       0.757520      0.001459           0.415051   \n",
      "20          20       0.757830      0.001503           0.414784   \n",
      "21          21       0.758051      0.001482           0.414536   \n",
      "22          22       0.758295      0.001391           0.414329   \n",
      "23          23       0.758551      0.001373           0.414123   \n",
      "24          24       0.758801      0.001366           0.413899   \n",
      "25          25       0.759018      0.001370           0.413746   \n",
      "26          26       0.759197      0.001434           0.413611   \n",
      "27          27       0.759407      0.001444           0.413464   \n",
      "28          28       0.759585      0.001416           0.413334   \n",
      "29          29       0.759767      0.001421           0.413209   \n",
      "30          30       0.759965      0.001396           0.413082   \n",
      "31          31       0.760093      0.001398           0.412974   \n",
      "32          32       0.760274      0.001392           0.412850   \n",
      "33          33       0.760469      0.001407           0.412733   \n",
      "34          34       0.760596      0.001430           0.412643   \n",
      "35          35       0.760765      0.001433           0.412539   \n",
      "36          36       0.760911      0.001448           0.412447   \n",
      "37          37       0.761044      0.001457           0.412359   \n",
      "38          38       0.761166      0.001447           0.412276   \n",
      "39          39       0.761266      0.001422           0.412211   \n",
      "40          40       0.761418      0.001424           0.412110   \n",
      "41          41       0.761537      0.001403           0.412039   \n",
      "42          42       0.761639      0.001387           0.411976   \n",
      "43          43       0.761751      0.001389           0.411913   \n",
      "44          44       0.761837      0.001389           0.411782   \n",
      "45          45       0.761964      0.001404           0.411702   \n",
      "46          46       0.762031      0.001383           0.411646   \n",
      "47          47       0.762180      0.001397           0.411560   \n",
      "48          48       0.762277      0.001400           0.411500   \n",
      "49          49       0.762385      0.001405           0.411442   \n",
      "50          50       0.762487      0.001422           0.411383   \n",
      "51          51       0.762599      0.001403           0.411321   \n",
      "52          52       0.762660      0.001406           0.411278   \n",
      "53          53       0.762747      0.001420           0.411223   \n",
      "54          54       0.762832      0.001398           0.411169   \n",
      "55          55       0.762923      0.001393           0.411116   \n",
      "56          56       0.763002      0.001422           0.411073   \n",
      "57          57       0.763101      0.001451           0.411016   \n",
      "58          58       0.763186      0.001448           0.410967   \n",
      "59          59       0.763276      0.001441           0.410913   \n",
      "60          60       0.763345      0.001466           0.410875   \n",
      "61          61       0.763451      0.001490           0.410813   \n",
      "62          62       0.763526      0.001521           0.410768   \n",
      "63          63       0.763597      0.001517           0.410728   \n",
      "64          64       0.763657      0.001488           0.410689   \n",
      "65          65       0.763716      0.001487           0.410647   \n",
      "66          66       0.763790      0.001489           0.410602   \n",
      "67          67       0.763884      0.001504           0.410550   \n",
      "68          68       0.763953      0.001529           0.410509   \n",
      "69          69       0.764049      0.001510           0.410460   \n",
      "70          70       0.764137      0.001505           0.410409   \n",
      "71          71       0.764200      0.001476           0.410374   \n",
      "72          72       0.764248      0.001495           0.410336   \n",
      "73          73       0.764327      0.001468           0.410292   \n",
      "74          74       0.764407      0.001433           0.410249   \n",
      "75          75       0.764491      0.001438           0.410198   \n",
      "76          76       0.764568      0.001424           0.410152   \n",
      "77          77       0.764630      0.001411           0.410118   \n",
      "78          78       0.764668      0.001413           0.410092   \n",
      "79          79       0.764740      0.001448           0.410059   \n",
      "80          80       0.764801      0.001451           0.410025   \n",
      "81          81       0.764871      0.001464           0.409990   \n",
      "82          82       0.764942      0.001478           0.409945   \n",
      "83          83       0.765002      0.001506           0.409913   \n",
      "84          84       0.765064      0.001512           0.409884   \n",
      "85          85       0.765127      0.001486           0.409847   \n",
      "86          86       0.765201      0.001478           0.409803   \n",
      "87          87       0.765251      0.001465           0.409774   \n",
      "88          88       0.765312      0.001469           0.409741   \n",
      "89          89       0.765390      0.001440           0.409699   \n",
      "90          90       0.765445      0.001478           0.409627   \n",
      "91          91       0.765501      0.001479           0.409552   \n",
      "92          92       0.765550      0.001486           0.409520   \n",
      "93          93       0.765597      0.001484           0.409492   \n",
      "94          94       0.765660      0.001486           0.409453   \n",
      "95          95       0.765709      0.001489           0.409422   \n",
      "96          96       0.765784      0.001514           0.409383   \n",
      "97          97       0.765839      0.001538           0.409345   \n",
      "98          98       0.765896      0.001525           0.409317   \n",
      "99          99       0.765956      0.001549           0.409280   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001372            0.597697           0.001274  \n",
      "1           0.000233            0.537431           0.000077  \n",
      "2           0.000750            0.499492           0.000909  \n",
      "3           0.000593            0.473782           0.000871  \n",
      "4           0.000481            0.456589           0.000030  \n",
      "5           0.000391            0.445427           0.000196  \n",
      "6           0.000565            0.437306           0.000199  \n",
      "7           0.000643            0.431338           0.000118  \n",
      "8           0.000553            0.427203           0.000303  \n",
      "9           0.000705            0.424069           0.000241  \n",
      "10          0.000675            0.421806           0.000179  \n",
      "11          0.000712            0.420137           0.000271  \n",
      "12          0.000779            0.418953           0.000275  \n",
      "13          0.000853            0.417915           0.000334  \n",
      "14          0.000855            0.417096           0.000301  \n",
      "15          0.000771            0.416439           0.000358  \n",
      "16          0.000757            0.415879           0.000349  \n",
      "17          0.000806            0.415441           0.000358  \n",
      "18          0.000788            0.415049           0.000337  \n",
      "19          0.000793            0.414704           0.000328  \n",
      "20          0.000798            0.414421           0.000320  \n",
      "21          0.000803            0.414163           0.000342  \n",
      "22          0.000803            0.413934           0.000365  \n",
      "23          0.000780            0.413713           0.000395  \n",
      "24          0.000759            0.413475           0.000421  \n",
      "25          0.000771            0.413303           0.000408  \n",
      "26          0.000783            0.413148           0.000394  \n",
      "27          0.000764            0.412980           0.000423  \n",
      "28          0.000759            0.412830           0.000441  \n",
      "29          0.000769            0.412692           0.000442  \n",
      "30          0.000757            0.412557           0.000427  \n",
      "31          0.000768            0.412424           0.000433  \n",
      "32          0.000767            0.412290           0.000429  \n",
      "33          0.000799            0.412151           0.000429  \n",
      "34          0.000804            0.412038           0.000420  \n",
      "35          0.000811            0.411917           0.000413  \n",
      "36          0.000816            0.411809           0.000399  \n",
      "37          0.000810            0.411704           0.000388  \n",
      "38          0.000806            0.411606           0.000387  \n",
      "39          0.000795            0.411521           0.000393  \n",
      "40          0.000814            0.411407           0.000378  \n",
      "41          0.000811            0.411318           0.000392  \n",
      "42          0.000812            0.411234           0.000409  \n",
      "43          0.000808            0.411145           0.000413  \n",
      "44          0.000775            0.410995           0.000412  \n",
      "45          0.000767            0.410897           0.000425  \n",
      "46          0.000751            0.410819           0.000443  \n",
      "47          0.000762            0.410719           0.000435  \n",
      "48          0.000761            0.410640           0.000427  \n",
      "49          0.000765            0.410566           0.000429  \n",
      "50          0.000774            0.410492           0.000428  \n",
      "51          0.000765            0.410415           0.000444  \n",
      "52          0.000773            0.410353           0.000439  \n",
      "53          0.000787            0.410279           0.000427  \n",
      "54          0.000785            0.410207           0.000429  \n",
      "55          0.000774            0.410134           0.000437  \n",
      "56          0.000780            0.410067           0.000438  \n",
      "57          0.000801            0.409996           0.000421  \n",
      "58          0.000800            0.409930           0.000419  \n",
      "59          0.000789            0.409858           0.000438  \n",
      "60          0.000801            0.409805           0.000433  \n",
      "61          0.000814            0.409727           0.000428  \n",
      "62          0.000833            0.409670           0.000419  \n",
      "63          0.000834            0.409611           0.000418  \n",
      "64          0.000824            0.409557           0.000429  \n",
      "65          0.000818            0.409498           0.000435  \n",
      "66          0.000827            0.409433           0.000434  \n",
      "67          0.000836            0.409366           0.000435  \n",
      "68          0.000845            0.409302           0.000432  \n",
      "69          0.000833            0.409244           0.000437  \n",
      "70          0.000834            0.409181           0.000434  \n",
      "71          0.000825            0.409133           0.000436  \n",
      "72          0.000827            0.409079           0.000431  \n",
      "73          0.000815            0.409018           0.000438  \n",
      "74          0.000797            0.408959           0.000448  \n",
      "75          0.000804            0.408894           0.000435  \n",
      "76          0.000800            0.408832           0.000442  \n",
      "77          0.000793            0.408774           0.000451  \n",
      "78          0.000794            0.408728           0.000442  \n",
      "79          0.000805            0.408680           0.000435  \n",
      "80          0.000806            0.408632           0.000444  \n",
      "81          0.000815            0.408577           0.000434  \n",
      "82          0.000824            0.408519           0.000422  \n",
      "83          0.000838            0.408468           0.000415  \n",
      "84          0.000836            0.408422           0.000415  \n",
      "85          0.000832            0.408371           0.000415  \n",
      "86          0.000827            0.408312           0.000423  \n",
      "87          0.000821            0.408256           0.000428  \n",
      "88          0.000813            0.408202           0.000432  \n",
      "89          0.000803            0.408145           0.000432  \n",
      "90          0.000825            0.408049           0.000435  \n",
      "91          0.000768            0.407951           0.000479  \n",
      "92          0.000776            0.407897           0.000476  \n",
      "93          0.000772            0.407846           0.000480  \n",
      "94          0.000772            0.407793           0.000478  \n",
      "95          0.000775            0.407735           0.000481  \n",
      "96          0.000788            0.407680           0.000476  \n",
      "97          0.000798            0.407626           0.000469  \n",
      "98          0.000794            0.407572           0.000475  \n",
      "99          0.000807            0.407514           0.000476  \n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.766    \u001b[0m | \u001b[0m4.376    \u001b[0m | \u001b[0m5.747    \u001b[0m | \u001b[0m0.1612   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7636373181\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7653496199\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7663212327\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.728347      0.005430           0.610459   \n",
      "1            1       0.738284      0.002183           0.554796   \n",
      "2            2       0.742248      0.001916           0.516294   \n",
      "3            3       0.745465      0.000744           0.488708   \n",
      "4            4       0.746498      0.000904           0.470069   \n",
      "5            5       0.748217      0.001468           0.456431   \n",
      "6            6       0.749704      0.001585           0.446539   \n",
      "7            7       0.750889      0.001457           0.439068   \n",
      "8            8       0.751430      0.001442           0.433860   \n",
      "9            9       0.752189      0.001491           0.429764   \n",
      "10          10       0.752630      0.001324           0.426670   \n",
      "11          11       0.753030      0.001150           0.424308   \n",
      "12          12       0.753628      0.001080           0.422468   \n",
      "13          13       0.754249      0.001189           0.420933   \n",
      "14          14       0.754747      0.001221           0.419731   \n",
      "15          15       0.755160      0.001287           0.418859   \n",
      "16          16       0.755617      0.001293           0.418056   \n",
      "17          17       0.755943      0.001329           0.417398   \n",
      "18          18       0.756194      0.001426           0.416848   \n",
      "19          19       0.756523      0.001430           0.416403   \n",
      "20          20       0.756794      0.001438           0.416006   \n",
      "21          21       0.757120      0.001441           0.415628   \n",
      "22          22       0.757397      0.001340           0.415359   \n",
      "23          23       0.757694      0.001325           0.415111   \n",
      "24          24       0.757914      0.001340           0.414851   \n",
      "25          25       0.758165      0.001335           0.414638   \n",
      "26          26       0.758379      0.001378           0.414442   \n",
      "27          27       0.758616      0.001397           0.414259   \n",
      "28          28       0.758895      0.001408           0.414071   \n",
      "29          29       0.759098      0.001437           0.413912   \n",
      "30          30       0.759301      0.001431           0.413761   \n",
      "31          31       0.759493      0.001446           0.413621   \n",
      "32          32       0.759673      0.001426           0.413491   \n",
      "33          33       0.759798      0.001431           0.413391   \n",
      "34          34       0.759940      0.001397           0.413279   \n",
      "35          35       0.760101      0.001378           0.413081   \n",
      "36          36       0.760252      0.001422           0.412969   \n",
      "37          37       0.760415      0.001402           0.412858   \n",
      "38          38       0.760562      0.001441           0.412768   \n",
      "39          39       0.760719      0.001441           0.412664   \n",
      "40          40       0.760826      0.001442           0.412585   \n",
      "41          41       0.760933      0.001441           0.412423   \n",
      "42          42       0.761037      0.001459           0.412356   \n",
      "43          43       0.761154      0.001450           0.412286   \n",
      "44          44       0.761252      0.001490           0.412206   \n",
      "45          45       0.761391      0.001464           0.412122   \n",
      "46          46       0.761456      0.001490           0.412070   \n",
      "47          47       0.761554      0.001484           0.412008   \n",
      "48          48       0.761637      0.001481           0.411949   \n",
      "49          49       0.761735      0.001438           0.411892   \n",
      "50          50       0.761834      0.001456           0.411829   \n",
      "51          51       0.761925      0.001446           0.411771   \n",
      "52          52       0.762015      0.001444           0.411710   \n",
      "53          53       0.762079      0.001482           0.411663   \n",
      "54          54       0.762169      0.001486           0.411604   \n",
      "55          55       0.762244      0.001488           0.411553   \n",
      "56          56       0.762313      0.001502           0.411507   \n",
      "57          57       0.762412      0.001516           0.411447   \n",
      "58          58       0.762502      0.001522           0.411398   \n",
      "59          59       0.762564      0.001520           0.411356   \n",
      "60          60       0.762657      0.001501           0.411302   \n",
      "61          61       0.762727      0.001504           0.411257   \n",
      "62          62       0.762797      0.001507           0.411216   \n",
      "63          63       0.762884      0.001513           0.411162   \n",
      "64          64       0.762958      0.001501           0.411126   \n",
      "65          65       0.763047      0.001528           0.411074   \n",
      "66          66       0.763118      0.001522           0.411031   \n",
      "67          67       0.763177      0.001528           0.410998   \n",
      "68          68       0.763256      0.001504           0.410950   \n",
      "69          69       0.763333      0.001496           0.410906   \n",
      "70          70       0.763382      0.001477           0.410829   \n",
      "71          71       0.763434      0.001484           0.410793   \n",
      "72          72       0.763507      0.001484           0.410754   \n",
      "73          73       0.763559      0.001483           0.410718   \n",
      "74          74       0.763624      0.001489           0.410680   \n",
      "75          75       0.763702      0.001494           0.410637   \n",
      "76          76       0.763759      0.001478           0.410605   \n",
      "77          77       0.763809      0.001461           0.410576   \n",
      "78          78       0.763867      0.001447           0.410540   \n",
      "79          79       0.763920      0.001442           0.410506   \n",
      "80          80       0.763999      0.001393           0.410463   \n",
      "81          81       0.764052      0.001407           0.410429   \n",
      "82          82       0.764128      0.001406           0.410385   \n",
      "83          83       0.764174      0.001423           0.410358   \n",
      "84          84       0.764238      0.001412           0.410325   \n",
      "85          85       0.764290      0.001421           0.410296   \n",
      "86          86       0.764333      0.001417           0.410270   \n",
      "87          87       0.764413      0.001395           0.410227   \n",
      "88          88       0.764475      0.001395           0.410191   \n",
      "89          89       0.764546      0.001367           0.410153   \n",
      "90          90       0.764589      0.001381           0.410128   \n",
      "91          91       0.764637      0.001369           0.410100   \n",
      "92          92       0.764674      0.001369           0.410033   \n",
      "93          93       0.764738      0.001363           0.409997   \n",
      "94          94       0.764805      0.001372           0.409955   \n",
      "95          95       0.764846      0.001364           0.409929   \n",
      "96          96       0.764922      0.001377           0.409887   \n",
      "97          97       0.765001      0.001384           0.409849   \n",
      "98          98       0.765054      0.001371           0.409819   \n",
      "99          99       0.765103      0.001359           0.409791   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001215            0.610373           0.001126  \n",
      "1           0.001181            0.554717           0.001048  \n",
      "2           0.001014            0.516241           0.000709  \n",
      "3           0.000697            0.488651           0.000335  \n",
      "4           0.000648            0.470000           0.000292  \n",
      "5           0.000210            0.456354           0.000318  \n",
      "6           0.000494            0.446448           0.000299  \n",
      "7           0.000522            0.438973           0.000174  \n",
      "8           0.000347            0.433734           0.000492  \n",
      "9           0.000363            0.429630           0.000473  \n",
      "10          0.000485            0.426512           0.000482  \n",
      "11          0.000468            0.424126           0.000545  \n",
      "12          0.000446            0.422277           0.000457  \n",
      "13          0.000481            0.420717           0.000302  \n",
      "14          0.000508            0.419502           0.000316  \n",
      "15          0.000539            0.418630           0.000333  \n",
      "16          0.000624            0.417802           0.000283  \n",
      "17          0.000679            0.417131           0.000310  \n",
      "18          0.000721            0.416560           0.000289  \n",
      "19          0.000667            0.416106           0.000386  \n",
      "20          0.000646            0.415701           0.000370  \n",
      "21          0.000698            0.415305           0.000336  \n",
      "22          0.000676            0.415015           0.000354  \n",
      "23          0.000678            0.414753           0.000354  \n",
      "24          0.000691            0.414484           0.000341  \n",
      "25          0.000684            0.414265           0.000365  \n",
      "26          0.000731            0.414057           0.000326  \n",
      "27          0.000731            0.413859           0.000341  \n",
      "28          0.000723            0.413644           0.000360  \n",
      "29          0.000695            0.413472           0.000388  \n",
      "30          0.000705            0.413303           0.000399  \n",
      "31          0.000709            0.413145           0.000404  \n",
      "32          0.000722            0.413001           0.000395  \n",
      "33          0.000731            0.412887           0.000398  \n",
      "34          0.000734            0.412761           0.000402  \n",
      "35          0.000853            0.412547           0.000279  \n",
      "36          0.000871            0.412420           0.000273  \n",
      "37          0.000867            0.412302           0.000270  \n",
      "38          0.000874            0.412198           0.000272  \n",
      "39          0.000889            0.412078           0.000263  \n",
      "40          0.000884            0.411986           0.000267  \n",
      "41          0.000788            0.411816           0.000362  \n",
      "42          0.000791            0.411732           0.000361  \n",
      "43          0.000794            0.411644           0.000355  \n",
      "44          0.000799            0.411548           0.000346  \n",
      "45          0.000792            0.411453           0.000353  \n",
      "46          0.000801            0.411384           0.000350  \n",
      "47          0.000801            0.411308           0.000348  \n",
      "48          0.000806            0.411236           0.000348  \n",
      "49          0.000786            0.411158           0.000364  \n",
      "50          0.000800            0.411083           0.000352  \n",
      "51          0.000799            0.411010           0.000349  \n",
      "52          0.000807            0.410933           0.000341  \n",
      "53          0.000823            0.410868           0.000324  \n",
      "54          0.000829            0.410796           0.000323  \n",
      "55          0.000834            0.410729           0.000325  \n",
      "56          0.000838            0.410673           0.000324  \n",
      "57          0.000837            0.410593           0.000330  \n",
      "58          0.000834            0.410533           0.000340  \n",
      "59          0.000834            0.410472           0.000347  \n",
      "60          0.000827            0.410402           0.000352  \n",
      "61          0.000821            0.410340           0.000357  \n",
      "62          0.000821            0.410281           0.000361  \n",
      "63          0.000825            0.410214           0.000354  \n",
      "64          0.000818            0.410156           0.000353  \n",
      "65          0.000831            0.410094           0.000345  \n",
      "66          0.000830            0.410034           0.000348  \n",
      "67          0.000834            0.409983           0.000350  \n",
      "68          0.000817            0.409923           0.000368  \n",
      "69          0.000812            0.409864           0.000375  \n",
      "70          0.000874            0.409770           0.000316  \n",
      "71          0.000881            0.409719           0.000312  \n",
      "72          0.000883            0.409666           0.000312  \n",
      "73          0.000879            0.409615           0.000313  \n",
      "74          0.000882            0.409561           0.000309  \n",
      "75          0.000884            0.409504           0.000308  \n",
      "76          0.000879            0.409456           0.000319  \n",
      "77          0.000871            0.409411           0.000323  \n",
      "78          0.000873            0.409361           0.000326  \n",
      "79          0.000869            0.409316           0.000332  \n",
      "80          0.000858            0.409260           0.000344  \n",
      "81          0.000865            0.409210           0.000336  \n",
      "82          0.000868            0.409160           0.000329  \n",
      "83          0.000874            0.409118           0.000326  \n",
      "84          0.000870            0.409073           0.000327  \n",
      "85          0.000875            0.409027           0.000330  \n",
      "86          0.000869            0.408986           0.000331  \n",
      "87          0.000863            0.408937           0.000339  \n",
      "88          0.000869            0.408889           0.000332  \n",
      "89          0.000853            0.408834           0.000353  \n",
      "90          0.000865            0.408793           0.000346  \n",
      "91          0.000858            0.408749           0.000345  \n",
      "92          0.000799            0.408666           0.000408  \n",
      "93          0.000797            0.408619           0.000409  \n",
      "94          0.000806            0.408571           0.000414  \n",
      "95          0.000797            0.408532           0.000417  \n",
      "96          0.000797            0.408478           0.000419  \n",
      "97          0.000799            0.408421           0.000422  \n",
      "98          0.000790            0.408375           0.000430  \n",
      "99          0.000785            0.408336           0.000433  \n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7651   \u001b[0m | \u001b[0m1.481    \u001b[0m | \u001b[0m5.753    \u001b[0m | \u001b[0m0.1366   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7647588693\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7668368269\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7679073822\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740342      0.000950           0.618079   \n",
      "1            1       0.745979      0.000878           0.565171   \n",
      "2            2       0.748568      0.000881           0.526569   \n",
      "3            3       0.748954      0.001345           0.499000   \n",
      "4            4       0.749274      0.001627           0.479795   \n",
      "5            5       0.750568      0.001658           0.465046   \n",
      "6            6       0.751471      0.001513           0.453810   \n",
      "7            7       0.752413      0.001438           0.444881   \n",
      "8            8       0.753010      0.001546           0.438565   \n",
      "9            9       0.753973      0.001542           0.433391   \n",
      "10          10       0.754488      0.001309           0.429486   \n",
      "11          11       0.755169      0.001235           0.426364   \n",
      "12          12       0.755493      0.001393           0.423937   \n",
      "13          13       0.756064      0.001460           0.421893   \n",
      "14          14       0.756505      0.001471           0.420337   \n",
      "15          15       0.756871      0.001396           0.419056   \n",
      "16          16       0.757257      0.001468           0.417941   \n",
      "17          17       0.757581      0.001500           0.417084   \n",
      "18          18       0.757925      0.001482           0.416396   \n",
      "19          19       0.758151      0.001470           0.415808   \n",
      "20          20       0.758509      0.001508           0.415261   \n",
      "21          21       0.758896      0.001519           0.414802   \n",
      "22          22       0.759197      0.001543           0.414415   \n",
      "23          23       0.759390      0.001573           0.414088   \n",
      "24          24       0.759587      0.001565           0.413823   \n",
      "25          25       0.759796      0.001578           0.413575   \n",
      "26          26       0.760143      0.001579           0.413315   \n",
      "27          27       0.760379      0.001606           0.413125   \n",
      "28          28       0.760575      0.001539           0.412924   \n",
      "29          29       0.760806      0.001537           0.412754   \n",
      "30          30       0.760929      0.001540           0.412610   \n",
      "31          31       0.761132      0.001498           0.412458   \n",
      "32          32       0.761301      0.001518           0.412316   \n",
      "33          33       0.761449      0.001525           0.412186   \n",
      "34          34       0.761583      0.001534           0.412085   \n",
      "35          35       0.761729      0.001508           0.411977   \n",
      "36          36       0.761869      0.001521           0.411882   \n",
      "37          37       0.762011      0.001537           0.411776   \n",
      "38          38       0.762142      0.001542           0.411682   \n",
      "39          39       0.762286      0.001552           0.411581   \n",
      "40          40       0.762402      0.001561           0.411502   \n",
      "41          41       0.762569      0.001554           0.411401   \n",
      "42          42       0.762670      0.001573           0.411320   \n",
      "43          43       0.762799      0.001573           0.411239   \n",
      "44          44       0.762907      0.001587           0.411160   \n",
      "45          45       0.763036      0.001564           0.411084   \n",
      "46          46       0.763151      0.001552           0.411009   \n",
      "47          47       0.763247      0.001547           0.410955   \n",
      "48          48       0.763325      0.001536           0.410893   \n",
      "49          49       0.763431      0.001557           0.410828   \n",
      "50          50       0.763547      0.001535           0.410759   \n",
      "51          51       0.763607      0.001525           0.410721   \n",
      "52          52       0.763689      0.001558           0.410675   \n",
      "53          53       0.763781      0.001556           0.410621   \n",
      "54          54       0.763863      0.001555           0.410556   \n",
      "55          55       0.763938      0.001563           0.410502   \n",
      "56          56       0.764030      0.001561           0.410449   \n",
      "57          57       0.764105      0.001564           0.410404   \n",
      "58          58       0.764208      0.001583           0.410345   \n",
      "59          59       0.764280      0.001584           0.410303   \n",
      "60          60       0.764343      0.001588           0.410265   \n",
      "61          61       0.764412      0.001592           0.410222   \n",
      "62          62       0.764471      0.001576           0.410187   \n",
      "63          63       0.764522      0.001574           0.410153   \n",
      "64          64       0.764597      0.001547           0.410111   \n",
      "65          65       0.764671      0.001537           0.410065   \n",
      "66          66       0.764741      0.001559           0.410023   \n",
      "67          67       0.764835      0.001574           0.409974   \n",
      "68          68       0.764913      0.001568           0.409934   \n",
      "69          69       0.764985      0.001568           0.409891   \n",
      "70          70       0.765057      0.001586           0.409851   \n",
      "71          71       0.765113      0.001588           0.409817   \n",
      "72          72       0.765164      0.001594           0.409785   \n",
      "73          73       0.765227      0.001600           0.409747   \n",
      "74          74       0.765284      0.001617           0.409717   \n",
      "75          75       0.765343      0.001617           0.409681   \n",
      "76          76       0.765419      0.001629           0.409635   \n",
      "77          77       0.765454      0.001620           0.409610   \n",
      "78          78       0.765518      0.001619           0.409573   \n",
      "79          79       0.765581      0.001607           0.409536   \n",
      "80          80       0.765655      0.001610           0.409500   \n",
      "81          81       0.765702      0.001620           0.409472   \n",
      "82          82       0.765758      0.001577           0.409435   \n",
      "83          83       0.765791      0.001575           0.409412   \n",
      "84          84       0.765834      0.001597           0.409390   \n",
      "85          85       0.765900      0.001598           0.409352   \n",
      "86          86       0.765939      0.001596           0.409327   \n",
      "87          87       0.765988      0.001589           0.409297   \n",
      "88          88       0.766040      0.001561           0.409267   \n",
      "89          89       0.766103      0.001586           0.409233   \n",
      "90          90       0.766133      0.001587           0.409212   \n",
      "91          91       0.766186      0.001585           0.409181   \n",
      "92          92       0.766217      0.001580           0.409160   \n",
      "93          93       0.766266      0.001581           0.409127   \n",
      "94          94       0.766310      0.001572           0.409034   \n",
      "95          95       0.766345      0.001569           0.409012   \n",
      "96          96       0.766399      0.001583           0.408986   \n",
      "97          97       0.766421      0.001587           0.408973   \n",
      "98          98       0.766467      0.001603           0.408945   \n",
      "99          99       0.766501      0.001601           0.408921   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000308            0.617997           0.000330  \n",
      "1           0.000245            0.565062           0.000331  \n",
      "2           0.000187            0.526443           0.000370  \n",
      "3           0.000229            0.498854           0.000274  \n",
      "4           0.001028            0.479643           0.000629  \n",
      "5           0.001202            0.464844           0.000818  \n",
      "6           0.000947            0.453572           0.000491  \n",
      "7           0.000735            0.444597           0.000348  \n",
      "8           0.000693            0.438226           0.000314  \n",
      "9           0.000454            0.432997           0.000362  \n",
      "10          0.000589            0.429055           0.000331  \n",
      "11          0.000602            0.425881           0.000319  \n",
      "12          0.000560            0.423429           0.000341  \n",
      "13          0.000597            0.421293           0.000304  \n",
      "14          0.000653            0.419692           0.000277  \n",
      "15          0.000584            0.418358           0.000361  \n",
      "16          0.000595            0.417185           0.000388  \n",
      "17          0.000620            0.416305           0.000360  \n",
      "18          0.000606            0.415537           0.000391  \n",
      "19          0.000646            0.414868           0.000379  \n",
      "20          0.000668            0.414270           0.000358  \n",
      "21          0.000669            0.413744           0.000379  \n",
      "22          0.000746            0.413290           0.000329  \n",
      "23          0.000758            0.412913           0.000326  \n",
      "24          0.000757            0.412595           0.000329  \n",
      "25          0.000736            0.412273           0.000363  \n",
      "26          0.000755            0.411932           0.000371  \n",
      "27          0.000765            0.411668           0.000371  \n",
      "28          0.000772            0.411414           0.000387  \n",
      "29          0.000772            0.411180           0.000388  \n",
      "30          0.000777            0.410995           0.000398  \n",
      "31          0.000787            0.410786           0.000390  \n",
      "32          0.000791            0.410565           0.000404  \n",
      "33          0.000799            0.410345           0.000409  \n",
      "34          0.000802            0.410176           0.000401  \n",
      "35          0.000793            0.409995           0.000402  \n",
      "36          0.000800            0.409830           0.000411  \n",
      "37          0.000807            0.409672           0.000410  \n",
      "38          0.000799            0.409514           0.000403  \n",
      "39          0.000823            0.409352           0.000398  \n",
      "40          0.000829            0.409195           0.000400  \n",
      "41          0.000836            0.409034           0.000381  \n",
      "42          0.000847            0.408891           0.000363  \n",
      "43          0.000849            0.408766           0.000362  \n",
      "44          0.000840            0.408631           0.000361  \n",
      "45          0.000839            0.408500           0.000376  \n",
      "46          0.000845            0.408357           0.000386  \n",
      "47          0.000842            0.408237           0.000393  \n",
      "48          0.000841            0.408115           0.000386  \n",
      "49          0.000852            0.407982           0.000388  \n",
      "50          0.000842            0.407877           0.000383  \n",
      "51          0.000841            0.407774           0.000403  \n",
      "52          0.000860            0.407653           0.000400  \n",
      "53          0.000861            0.407526           0.000394  \n",
      "54          0.000847            0.407396           0.000410  \n",
      "55          0.000858            0.407282           0.000406  \n",
      "56          0.000856            0.407171           0.000412  \n",
      "57          0.000864            0.407058           0.000391  \n",
      "58          0.000879            0.406953           0.000387  \n",
      "59          0.000879            0.406848           0.000387  \n",
      "60          0.000882            0.406754           0.000392  \n",
      "61          0.000879            0.406668           0.000398  \n",
      "62          0.000874            0.406556           0.000405  \n",
      "63          0.000872            0.406459           0.000392  \n",
      "64          0.000858            0.406353           0.000400  \n",
      "65          0.000857            0.406227           0.000383  \n",
      "66          0.000872            0.406107           0.000381  \n",
      "67          0.000879            0.405999           0.000393  \n",
      "68          0.000880            0.405886           0.000394  \n",
      "69          0.000875            0.405786           0.000409  \n",
      "70          0.000882            0.405679           0.000414  \n",
      "71          0.000883            0.405593           0.000415  \n",
      "72          0.000885            0.405481           0.000438  \n",
      "73          0.000893            0.405393           0.000428  \n",
      "74          0.000894            0.405321           0.000423  \n",
      "75          0.000894            0.405218           0.000429  \n",
      "76          0.000906            0.405103           0.000450  \n",
      "77          0.000904            0.405034           0.000461  \n",
      "78          0.000905            0.404929           0.000478  \n",
      "79          0.000904            0.404813           0.000491  \n",
      "80          0.000905            0.404744           0.000482  \n",
      "81          0.000911            0.404668           0.000486  \n",
      "82          0.000890            0.404563           0.000497  \n",
      "83          0.000890            0.404466           0.000488  \n",
      "84          0.000903            0.404374           0.000513  \n",
      "85          0.000905            0.404259           0.000505  \n",
      "86          0.000904            0.404159           0.000508  \n",
      "87          0.000901            0.404064           0.000517  \n",
      "88          0.000891            0.403980           0.000507  \n",
      "89          0.000896            0.403891           0.000502  \n",
      "90          0.000899            0.403802           0.000508  \n",
      "91          0.000900            0.403722           0.000509  \n",
      "92          0.000898            0.403635           0.000531  \n",
      "93          0.000899            0.403544           0.000530  \n",
      "94          0.000890            0.403429           0.000532  \n",
      "95          0.000887            0.403348           0.000551  \n",
      "96          0.000896            0.403260           0.000538  \n",
      "97          0.000896            0.403184           0.000538  \n",
      "98          0.000903            0.403101           0.000527  \n",
      "99          0.000899            0.403041           0.000538  \n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7665   \u001b[0m | \u001b[0m4.615    \u001b[0m | \u001b[0m8.711    \u001b[0m | \u001b[0m0.12     \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7637590014\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7659251214\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7666735324\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.738094      0.000608           0.626708   \n",
      "1            1       0.745436      0.000569           0.578347   \n",
      "2            2       0.747056      0.001048           0.542077   \n",
      "3            3       0.747960      0.001727           0.514123   \n",
      "4            4       0.748104      0.001287           0.493112   \n",
      "5            5       0.749476      0.001008           0.476788   \n",
      "6            6       0.750099      0.001345           0.464302   \n",
      "7            7       0.750381      0.001414           0.454789   \n",
      "8            8       0.750914      0.001210           0.447125   \n",
      "9            9       0.751511      0.001390           0.441188   \n",
      "10          10       0.752657      0.001465           0.436133   \n",
      "11          11       0.753184      0.001512           0.432191   \n",
      "12          12       0.753836      0.001434           0.429078   \n",
      "13          13       0.754296      0.001422           0.426487   \n",
      "14          14       0.754807      0.001475           0.424499   \n",
      "15          15       0.755370      0.001426           0.422701   \n",
      "16          16       0.755830      0.001557           0.421357   \n",
      "17          17       0.756176      0.001548           0.420177   \n",
      "18          18       0.756494      0.001493           0.419144   \n",
      "19          19       0.756861      0.001519           0.418232   \n",
      "20          20       0.757305      0.001455           0.417510   \n",
      "21          21       0.757535      0.001436           0.416856   \n",
      "22          22       0.757749      0.001445           0.416309   \n",
      "23          23       0.758030      0.001416           0.415840   \n",
      "24          24       0.758231      0.001420           0.415445   \n",
      "25          25       0.758483      0.001467           0.415124   \n",
      "26          26       0.758776      0.001427           0.414808   \n",
      "27          27       0.758949      0.001460           0.414498   \n",
      "28          28       0.759103      0.001443           0.414259   \n",
      "29          29       0.759268      0.001458           0.414010   \n",
      "30          30       0.759491      0.001484           0.413808   \n",
      "31          31       0.759673      0.001521           0.413625   \n",
      "32          32       0.759825      0.001511           0.413461   \n",
      "33          33       0.760010      0.001486           0.413304   \n",
      "34          34       0.760187      0.001438           0.413159   \n",
      "35          35       0.760355      0.001458           0.412993   \n",
      "36          36       0.760488      0.001439           0.412873   \n",
      "37          37       0.760613      0.001444           0.412762   \n",
      "38          38       0.760738      0.001471           0.412664   \n",
      "39          39       0.760882      0.001457           0.412529   \n",
      "40          40       0.761001      0.001490           0.412443   \n",
      "41          41       0.761125      0.001462           0.412346   \n",
      "42          42       0.761227      0.001461           0.412265   \n",
      "43          43       0.761348      0.001476           0.412172   \n",
      "44          44       0.761472      0.001448           0.412082   \n",
      "45          45       0.761591      0.001446           0.412004   \n",
      "46          46       0.761713      0.001438           0.411929   \n",
      "47          47       0.761856      0.001455           0.411847   \n",
      "48          48       0.761973      0.001427           0.411776   \n",
      "49          49       0.762059      0.001441           0.411700   \n",
      "50          50       0.762174      0.001435           0.411629   \n",
      "51          51       0.762286      0.001415           0.411550   \n",
      "52          52       0.762379      0.001413           0.411487   \n",
      "53          53       0.762479      0.001428           0.411426   \n",
      "54          54       0.762556      0.001448           0.411370   \n",
      "55          55       0.762660      0.001426           0.411304   \n",
      "56          56       0.762751      0.001420           0.411243   \n",
      "57          57       0.762831      0.001430           0.411195   \n",
      "58          58       0.762912      0.001431           0.411139   \n",
      "59          59       0.762988      0.001442           0.411092   \n",
      "60          60       0.763105      0.001481           0.411032   \n",
      "61          61       0.763211      0.001465           0.410969   \n",
      "62          62       0.763278      0.001481           0.410927   \n",
      "63          63       0.763355      0.001483           0.410876   \n",
      "64          64       0.763449      0.001500           0.410825   \n",
      "65          65       0.763511      0.001497           0.410790   \n",
      "66          66       0.763581      0.001490           0.410747   \n",
      "67          67       0.763655      0.001498           0.410704   \n",
      "68          68       0.763737      0.001520           0.410655   \n",
      "69          69       0.763795      0.001514           0.410614   \n",
      "70          70       0.763894      0.001510           0.410561   \n",
      "71          71       0.763962      0.001513           0.410510   \n",
      "72          72       0.764026      0.001505           0.410467   \n",
      "73          73       0.764099      0.001509           0.410431   \n",
      "74          74       0.764159      0.001507           0.410394   \n",
      "75          75       0.764195      0.001515           0.410371   \n",
      "76          76       0.764252      0.001534           0.410336   \n",
      "77          77       0.764307      0.001530           0.410300   \n",
      "78          78       0.764375      0.001529           0.410264   \n",
      "79          79       0.764428      0.001539           0.410231   \n",
      "80          80       0.764483      0.001543           0.410201   \n",
      "81          81       0.764547      0.001542           0.410164   \n",
      "82          82       0.764606      0.001535           0.410130   \n",
      "83          83       0.764666      0.001525           0.410098   \n",
      "84          84       0.764706      0.001515           0.410072   \n",
      "85          85       0.764771      0.001527           0.410034   \n",
      "86          86       0.764838      0.001518           0.409996   \n",
      "87          87       0.764901      0.001511           0.409962   \n",
      "88          88       0.764956      0.001505           0.409928   \n",
      "89          89       0.765006      0.001488           0.409892   \n",
      "90          90       0.765040      0.001475           0.409871   \n",
      "91          91       0.765079      0.001484           0.409848   \n",
      "92          92       0.765122      0.001490           0.409822   \n",
      "93          93       0.765163      0.001501           0.409765   \n",
      "94          94       0.765194      0.001500           0.409742   \n",
      "95          95       0.765232      0.001497           0.409721   \n",
      "96          96       0.765303      0.001488           0.409686   \n",
      "97          97       0.765346      0.001499           0.409659   \n",
      "98          98       0.765390      0.001503           0.409634   \n",
      "99          99       0.765453      0.001514           0.409595   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000279            0.626630           0.000337  \n",
      "1           0.000114            0.578249           0.000175  \n",
      "2           0.000492            0.541996           0.000670  \n",
      "3           0.000350            0.514051           0.000125  \n",
      "4           0.000544            0.493037           0.000466  \n",
      "5           0.000442            0.476714           0.000351  \n",
      "6           0.000099            0.464209           0.000498  \n",
      "7           0.000045            0.454675           0.000508  \n",
      "8           0.000099            0.446997           0.000492  \n",
      "9           0.000173            0.441023           0.000506  \n",
      "10          0.000192            0.435917           0.000490  \n",
      "11          0.000050            0.431931           0.000635  \n",
      "12          0.000181            0.428782           0.000549  \n",
      "13          0.000356            0.426167           0.000448  \n",
      "14          0.000514            0.424164           0.000461  \n",
      "15          0.000478            0.422316           0.000421  \n",
      "16          0.000558            0.420945           0.000329  \n",
      "17          0.000596            0.419737           0.000324  \n",
      "18          0.000562            0.418667           0.000380  \n",
      "19          0.000599            0.417729           0.000326  \n",
      "20          0.000619            0.416963           0.000298  \n",
      "21          0.000571            0.416274           0.000352  \n",
      "22          0.000562            0.415695           0.000364  \n",
      "23          0.000632            0.415185           0.000318  \n",
      "24          0.000648            0.414750           0.000309  \n",
      "25          0.000634            0.414403           0.000347  \n",
      "26          0.000637            0.414049           0.000358  \n",
      "27          0.000644            0.413729           0.000375  \n",
      "28          0.000656            0.413454           0.000374  \n",
      "29          0.000660            0.413175           0.000387  \n",
      "30          0.000705            0.412939           0.000365  \n",
      "31          0.000734            0.412711           0.000362  \n",
      "32          0.000750            0.412508           0.000353  \n",
      "33          0.000743            0.412330           0.000363  \n",
      "34          0.000743            0.412148           0.000361  \n",
      "35          0.000751            0.411954           0.000347  \n",
      "36          0.000740            0.411796           0.000365  \n",
      "37          0.000748            0.411645           0.000370  \n",
      "38          0.000763            0.411506           0.000359  \n",
      "39          0.000768            0.411337           0.000357  \n",
      "40          0.000765            0.411217           0.000368  \n",
      "41          0.000748            0.411090           0.000377  \n",
      "42          0.000763            0.410966           0.000369  \n",
      "43          0.000770            0.410842           0.000369  \n",
      "44          0.000758            0.410725           0.000385  \n",
      "45          0.000759            0.410610           0.000390  \n",
      "46          0.000758            0.410498           0.000391  \n",
      "47          0.000767            0.410384           0.000376  \n",
      "48          0.000758            0.410276           0.000378  \n",
      "49          0.000771            0.410181           0.000368  \n",
      "50          0.000770            0.410062           0.000368  \n",
      "51          0.000769            0.409949           0.000371  \n",
      "52          0.000775            0.409844           0.000377  \n",
      "53          0.000786            0.409748           0.000375  \n",
      "54          0.000793            0.409657           0.000367  \n",
      "55          0.000787            0.409575           0.000371  \n",
      "56          0.000788            0.409485           0.000374  \n",
      "57          0.000791            0.409414           0.000378  \n",
      "58          0.000786            0.409331           0.000375  \n",
      "59          0.000793            0.409254           0.000374  \n",
      "60          0.000805            0.409163           0.000375  \n",
      "61          0.000788            0.409069           0.000378  \n",
      "62          0.000793            0.409006           0.000376  \n",
      "63          0.000800            0.408931           0.000372  \n",
      "64          0.000815            0.408854           0.000352  \n",
      "65          0.000810            0.408785           0.000354  \n",
      "66          0.000808            0.408707           0.000351  \n",
      "67          0.000809            0.408630           0.000361  \n",
      "68          0.000821            0.408543           0.000353  \n",
      "69          0.000822            0.408462           0.000349  \n",
      "70          0.000820            0.408365           0.000347  \n",
      "71          0.000823            0.408282           0.000344  \n",
      "72          0.000821            0.408208           0.000346  \n",
      "73          0.000828            0.408140           0.000338  \n",
      "74          0.000823            0.408062           0.000356  \n",
      "75          0.000828            0.407999           0.000353  \n",
      "76          0.000836            0.407928           0.000346  \n",
      "77          0.000833            0.407876           0.000342  \n",
      "78          0.000830            0.407813           0.000335  \n",
      "79          0.000839            0.407750           0.000335  \n",
      "80          0.000842            0.407695           0.000346  \n",
      "81          0.000841            0.407625           0.000360  \n",
      "82          0.000838            0.407568           0.000366  \n",
      "83          0.000833            0.407512           0.000369  \n",
      "84          0.000836            0.407444           0.000371  \n",
      "85          0.000837            0.407369           0.000368  \n",
      "86          0.000830            0.407305           0.000365  \n",
      "87          0.000830            0.407245           0.000358  \n",
      "88          0.000825            0.407175           0.000360  \n",
      "89          0.000816            0.407098           0.000361  \n",
      "90          0.000810            0.407049           0.000375  \n",
      "91          0.000814            0.407000           0.000373  \n",
      "92          0.000817            0.406947           0.000362  \n",
      "93          0.000786            0.406864           0.000381  \n",
      "94          0.000783            0.406823           0.000385  \n",
      "95          0.000782            0.406777           0.000392  \n",
      "96          0.000782            0.406712           0.000395  \n",
      "97          0.000787            0.406655           0.000390  \n",
      "98          0.000784            0.406600           0.000393  \n",
      "99          0.000793            0.406525           0.000382  \n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7655   \u001b[0m | \u001b[0m5.191    \u001b[0m | \u001b[0m7.554    \u001b[0m | \u001b[0m0.1046   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7641465872\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7661725075\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7672187996\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740342      0.000950           0.629466   \n",
      "1            1       0.745957      0.000900           0.581898   \n",
      "2            2       0.748516      0.000896           0.545215   \n",
      "3            3       0.748830      0.001320           0.517556   \n",
      "4            4       0.749213      0.001823           0.497202   \n",
      "5            5       0.750303      0.001447           0.481134   \n",
      "6            6       0.751086      0.001421           0.468206   \n",
      "7            7       0.751603      0.001385           0.457789   \n",
      "8            8       0.752252      0.001590           0.449648   \n",
      "9            9       0.753248      0.001456           0.442911   \n",
      "10          10       0.753765      0.001669           0.437856   \n",
      "11          11       0.754225      0.001420           0.433962   \n",
      "12          12       0.754470      0.001282           0.430512   \n",
      "13          13       0.755073      0.001260           0.427687   \n",
      "14          14       0.755506      0.001313           0.425451   \n",
      "15          15       0.756011      0.001319           0.423464   \n",
      "16          16       0.756466      0.001388           0.421737   \n",
      "17          17       0.756907      0.001468           0.420394   \n",
      "18          18       0.757207      0.001401           0.419293   \n",
      "19          19       0.757396      0.001451           0.418344   \n",
      "20          20       0.757688      0.001399           0.417538   \n",
      "21          21       0.757990      0.001260           0.416853   \n",
      "22          22       0.758300      0.001340           0.416291   \n",
      "23          23       0.758529      0.001370           0.415742   \n",
      "24          24       0.758699      0.001304           0.415341   \n",
      "25          25       0.758890      0.001361           0.414978   \n",
      "26          26       0.759153      0.001434           0.414628   \n",
      "27          27       0.759442      0.001441           0.414332   \n",
      "28          28       0.759641      0.001369           0.414053   \n",
      "29          29       0.759833      0.001416           0.413810   \n",
      "30          30       0.759989      0.001413           0.413609   \n",
      "31          31       0.760171      0.001349           0.413403   \n",
      "32          32       0.760349      0.001364           0.413225   \n",
      "33          33       0.760515      0.001358           0.413036   \n",
      "34          34       0.760701      0.001419           0.412876   \n",
      "35          35       0.760805      0.001407           0.412744   \n",
      "36          36       0.760928      0.001431           0.412630   \n",
      "37          37       0.761098      0.001402           0.412488   \n",
      "38          38       0.761231      0.001416           0.412364   \n",
      "39          39       0.761378      0.001400           0.412257   \n",
      "40          40       0.761496      0.001419           0.412163   \n",
      "41          41       0.761634      0.001400           0.412053   \n",
      "42          42       0.761749      0.001397           0.411957   \n",
      "43          43       0.761897      0.001394           0.411866   \n",
      "44          44       0.761982      0.001420           0.411777   \n",
      "45          45       0.762112      0.001405           0.411693   \n",
      "46          46       0.762210      0.001419           0.411621   \n",
      "47          47       0.762310      0.001421           0.411560   \n",
      "48          48       0.762409      0.001426           0.411481   \n",
      "49          49       0.762498      0.001422           0.411420   \n",
      "50          50       0.762589      0.001403           0.411349   \n",
      "51          51       0.762677      0.001400           0.411294   \n",
      "52          52       0.762762      0.001393           0.411240   \n",
      "53          53       0.762862      0.001414           0.411175   \n",
      "54          54       0.762969      0.001416           0.411110   \n",
      "55          55       0.763059      0.001428           0.411033   \n",
      "56          56       0.763150      0.001440           0.410974   \n",
      "57          57       0.763254      0.001433           0.410912   \n",
      "58          58       0.763371      0.001454           0.410845   \n",
      "59          59       0.763467      0.001465           0.410790   \n",
      "60          60       0.763525      0.001467           0.410751   \n",
      "61          61       0.763606      0.001464           0.410704   \n",
      "62          62       0.763708      0.001470           0.410651   \n",
      "63          63       0.763777      0.001475           0.410605   \n",
      "64          64       0.763864      0.001464           0.410552   \n",
      "65          65       0.763952      0.001445           0.410498   \n",
      "66          66       0.764026      0.001441           0.410448   \n",
      "67          67       0.764114      0.001446           0.410400   \n",
      "68          68       0.764189      0.001442           0.410354   \n",
      "69          69       0.764259      0.001480           0.410309   \n",
      "70          70       0.764331      0.001475           0.410267   \n",
      "71          71       0.764394      0.001477           0.410225   \n",
      "72          72       0.764468      0.001463           0.410182   \n",
      "73          73       0.764514      0.001461           0.410151   \n",
      "74          74       0.764579      0.001453           0.410114   \n",
      "75          75       0.764615      0.001448           0.410089   \n",
      "76          76       0.764689      0.001456           0.410048   \n",
      "77          77       0.764739      0.001440           0.410015   \n",
      "78          78       0.764802      0.001451           0.409979   \n",
      "79          79       0.764869      0.001466           0.409938   \n",
      "80          80       0.764944      0.001475           0.409902   \n",
      "81          81       0.765002      0.001486           0.409869   \n",
      "82          82       0.765054      0.001504           0.409835   \n",
      "83          83       0.765106      0.001504           0.409805   \n",
      "84          84       0.765157      0.001493           0.409775   \n",
      "85          85       0.765223      0.001487           0.409739   \n",
      "86          86       0.765269      0.001495           0.409712   \n",
      "87          87       0.765321      0.001487           0.409682   \n",
      "88          88       0.765361      0.001475           0.409659   \n",
      "89          89       0.765418      0.001485           0.409628   \n",
      "90          90       0.765447      0.001497           0.409601   \n",
      "91          91       0.765503      0.001507           0.409567   \n",
      "92          92       0.765551      0.001511           0.409539   \n",
      "93          93       0.765599      0.001515           0.409510   \n",
      "94          94       0.765644      0.001530           0.409460   \n",
      "95          95       0.765685      0.001537           0.409431   \n",
      "96          96       0.765731      0.001558           0.409406   \n",
      "97          97       0.765772      0.001562           0.409385   \n",
      "98          98       0.765819      0.001569           0.409357   \n",
      "99          99       0.765846      0.001562           0.409336   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000263            0.629389           0.000278  \n",
      "1           0.000234            0.581788           0.000301  \n",
      "2           0.000129            0.545095           0.000324  \n",
      "3           0.000211            0.517426           0.000225  \n",
      "4           0.000570            0.497065           0.000421  \n",
      "5           0.000272            0.480979           0.000193  \n",
      "6           0.000533            0.468021           0.000196  \n",
      "7           0.000490            0.457564           0.000162  \n",
      "8           0.000674            0.449401           0.000049  \n",
      "9           0.000501            0.442619           0.000149  \n",
      "10          0.000601            0.437542           0.000335  \n",
      "11          0.000575            0.433596           0.000306  \n",
      "12          0.000509            0.430111           0.000259  \n",
      "13          0.000476            0.427232           0.000325  \n",
      "14          0.000641            0.424965           0.000379  \n",
      "15          0.000522            0.422951           0.000334  \n",
      "16          0.000606            0.421171           0.000293  \n",
      "17          0.000515            0.419794           0.000383  \n",
      "18          0.000537            0.418622           0.000368  \n",
      "19          0.000611            0.417639           0.000337  \n",
      "20          0.000583            0.416778           0.000392  \n",
      "21          0.000621            0.416031           0.000387  \n",
      "22          0.000639            0.415406           0.000381  \n",
      "23          0.000659            0.414808           0.000379  \n",
      "24          0.000651            0.414359           0.000405  \n",
      "25          0.000649            0.413940           0.000414  \n",
      "26          0.000681            0.413541           0.000375  \n",
      "27          0.000663            0.413193           0.000404  \n",
      "28          0.000669            0.412878           0.000417  \n",
      "29          0.000693            0.412587           0.000411  \n",
      "30          0.000678            0.412354           0.000439  \n",
      "31          0.000670            0.412087           0.000439  \n",
      "32          0.000673            0.411854           0.000455  \n",
      "33          0.000685            0.411610           0.000467  \n",
      "34          0.000707            0.411390           0.000458  \n",
      "35          0.000712            0.411205           0.000468  \n",
      "36          0.000721            0.411015           0.000465  \n",
      "37          0.000730            0.410823           0.000485  \n",
      "38          0.000738            0.410646           0.000475  \n",
      "39          0.000748            0.410490           0.000458  \n",
      "40          0.000759            0.410335           0.000445  \n",
      "41          0.000757            0.410174           0.000432  \n",
      "42          0.000762            0.410023           0.000430  \n",
      "43          0.000769            0.409878           0.000423  \n",
      "44          0.000780            0.409759           0.000410  \n",
      "45          0.000776            0.409616           0.000426  \n",
      "46          0.000777            0.409477           0.000424  \n",
      "47          0.000778            0.409353           0.000416  \n",
      "48          0.000769            0.409225           0.000415  \n",
      "49          0.000770            0.409104           0.000416  \n",
      "50          0.000764            0.408998           0.000427  \n",
      "51          0.000763            0.408885           0.000446  \n",
      "52          0.000762            0.408779           0.000449  \n",
      "53          0.000770            0.408659           0.000447  \n",
      "54          0.000769            0.408539           0.000455  \n",
      "55          0.000777            0.408414           0.000460  \n",
      "56          0.000779            0.408316           0.000463  \n",
      "57          0.000782            0.408217           0.000460  \n",
      "58          0.000796            0.408107           0.000454  \n",
      "59          0.000805            0.407996           0.000448  \n",
      "60          0.000804            0.407904           0.000464  \n",
      "61          0.000806            0.407821           0.000480  \n",
      "62          0.000809            0.407729           0.000477  \n",
      "63          0.000815            0.407638           0.000471  \n",
      "64          0.000816            0.407519           0.000477  \n",
      "65          0.000809            0.407400           0.000488  \n",
      "66          0.000802            0.407300           0.000506  \n",
      "67          0.000809            0.407192           0.000501  \n",
      "68          0.000810            0.407085           0.000494  \n",
      "69          0.000830            0.407001           0.000479  \n",
      "70          0.000829            0.406910           0.000481  \n",
      "71          0.000826            0.406801           0.000480  \n",
      "72          0.000821            0.406697           0.000494  \n",
      "73          0.000818            0.406629           0.000494  \n",
      "74          0.000811            0.406555           0.000489  \n",
      "75          0.000805            0.406467           0.000506  \n",
      "76          0.000806            0.406372           0.000509  \n",
      "77          0.000800            0.406301           0.000495  \n",
      "78          0.000805            0.406214           0.000480  \n",
      "79          0.000812            0.406110           0.000481  \n",
      "80          0.000815            0.406038           0.000476  \n",
      "81          0.000818            0.405963           0.000481  \n",
      "82          0.000828            0.405863           0.000469  \n",
      "83          0.000827            0.405773           0.000469  \n",
      "84          0.000821            0.405688           0.000474  \n",
      "85          0.000823            0.405618           0.000464  \n",
      "86          0.000829            0.405532           0.000447  \n",
      "87          0.000826            0.405455           0.000467  \n",
      "88          0.000821            0.405395           0.000476  \n",
      "89          0.000828            0.405316           0.000477  \n",
      "90          0.000833            0.405230           0.000482  \n",
      "91          0.000839            0.405155           0.000454  \n",
      "92          0.000840            0.405087           0.000459  \n",
      "93          0.000845            0.405000           0.000469  \n",
      "94          0.000822            0.404921           0.000481  \n",
      "95          0.000827            0.404843           0.000487  \n",
      "96          0.000840            0.404764           0.000478  \n",
      "97          0.000840            0.404697           0.000484  \n",
      "98          0.000844            0.404619           0.000470  \n",
      "99          0.000845            0.404554           0.000485  \n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7658   \u001b[0m | \u001b[0m2.454    \u001b[0m | \u001b[0m8.645    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655541754\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7675429872\n",
      "bestIteration = 98\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7684289483\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.576738   \n",
      "1            1       0.744740      0.000670           0.512706   \n",
      "2            2       0.749463      0.001013           0.475405   \n",
      "3            3       0.751354      0.001265           0.453182   \n",
      "4            4       0.751875      0.001071           0.439969   \n",
      "5            5       0.753205      0.001234           0.431267   \n",
      "6            6       0.754077      0.001066           0.425633   \n",
      "7            7       0.755244      0.001546           0.421795   \n",
      "8            8       0.756253      0.001553           0.419221   \n",
      "9            9       0.757041      0.001528           0.417502   \n",
      "10          10       0.757601      0.001346           0.416223   \n",
      "11          11       0.758302      0.001350           0.415245   \n",
      "12          12       0.758767      0.001240           0.414567   \n",
      "13          13       0.759222      0.001217           0.413983   \n",
      "14          14       0.759632      0.001250           0.413551   \n",
      "15          15       0.760032      0.001247           0.413154   \n",
      "16          16       0.760452      0.001252           0.412830   \n",
      "17          17       0.760799      0.001263           0.412572   \n",
      "18          18       0.760995      0.001314           0.412380   \n",
      "19          19       0.761246      0.001337           0.412174   \n",
      "20          20       0.761526      0.001310           0.411999   \n",
      "21          21       0.761744      0.001292           0.411823   \n",
      "22          22       0.761932      0.001285           0.411696   \n",
      "23          23       0.762130      0.001270           0.411568   \n",
      "24          24       0.762322      0.001345           0.411448   \n",
      "25          25       0.762559      0.001392           0.411315   \n",
      "26          26       0.762709      0.001372           0.411216   \n",
      "27          27       0.762875      0.001407           0.411104   \n",
      "28          28       0.763054      0.001423           0.411001   \n",
      "29          29       0.763153      0.001408           0.410940   \n",
      "30          30       0.763307      0.001388           0.410851   \n",
      "31          31       0.763426      0.001331           0.410781   \n",
      "32          32       0.763524      0.001370           0.410702   \n",
      "33          33       0.763692      0.001397           0.410596   \n",
      "34          34       0.763794      0.001344           0.410536   \n",
      "35          35       0.763964      0.001384           0.410448   \n",
      "36          36       0.764081      0.001341           0.410375   \n",
      "37          37       0.764169      0.001373           0.410328   \n",
      "38          38       0.764291      0.001331           0.410255   \n",
      "39          39       0.764397      0.001329           0.410202   \n",
      "40          40       0.764534      0.001334           0.410126   \n",
      "41          41       0.764616      0.001368           0.410084   \n",
      "42          42       0.764704      0.001342           0.410039   \n",
      "43          43       0.764800      0.001293           0.409988   \n",
      "44          44       0.764863      0.001322           0.409954   \n",
      "45          45       0.764913      0.001313           0.409933   \n",
      "46          46       0.764986      0.001365           0.409885   \n",
      "47          47       0.765084      0.001416           0.409830   \n",
      "48          48       0.765155      0.001417           0.409790   \n",
      "49          49       0.765238      0.001432           0.409749   \n",
      "50          50       0.765282      0.001470           0.409722   \n",
      "51          51       0.765373      0.001507           0.409671   \n",
      "52          52       0.765480      0.001482           0.409612   \n",
      "53          53       0.765539      0.001467           0.409575   \n",
      "54          54       0.765573      0.001483           0.409560   \n",
      "55          55       0.765613      0.001487           0.409535   \n",
      "56          56       0.765691      0.001471           0.409496   \n",
      "57          57       0.765690      0.001448           0.409492   \n",
      "58          58       0.765766      0.001418           0.409454   \n",
      "59          59       0.765835      0.001421           0.409415   \n",
      "60          60       0.765920      0.001449           0.409363   \n",
      "61          61       0.765963      0.001462           0.409338   \n",
      "62          62       0.766033      0.001464           0.409306   \n",
      "63          63       0.766069      0.001468           0.409290   \n",
      "64          64       0.766101      0.001472           0.409271   \n",
      "65          65       0.766151      0.001477           0.409251   \n",
      "66          66       0.766168      0.001458           0.409204   \n",
      "67          67       0.766231      0.001430           0.409173   \n",
      "68          68       0.766275      0.001403           0.409155   \n",
      "69          69       0.766341      0.001438           0.409121   \n",
      "70          70       0.766379      0.001431           0.409104   \n",
      "71          71       0.766408      0.001443           0.409092   \n",
      "72          72       0.766423      0.001426           0.409082   \n",
      "73          73       0.766477      0.001420           0.409018   \n",
      "74          74       0.766570      0.001434           0.408973   \n",
      "75          75       0.766634      0.001382           0.408940   \n",
      "76          76       0.766645      0.001375           0.408937   \n",
      "77          77       0.766685      0.001381           0.408913   \n",
      "78          78       0.766700      0.001385           0.408908   \n",
      "79          79       0.766765      0.001376           0.408871   \n",
      "80          80       0.766810      0.001370           0.408849   \n",
      "81          81       0.766824      0.001391           0.408845   \n",
      "82          82       0.766861      0.001368           0.408821   \n",
      "83          83       0.766896      0.001369           0.408811   \n",
      "84          84       0.766929      0.001337           0.408784   \n",
      "85          85       0.766970      0.001331           0.408767   \n",
      "86          86       0.766986      0.001332           0.408755   \n",
      "87          87       0.766994      0.001360           0.408720   \n",
      "88          88       0.767034      0.001382           0.408699   \n",
      "89          89       0.767026      0.001386           0.408703   \n",
      "90          90       0.767022      0.001391           0.408702   \n",
      "91          91       0.767032      0.001390           0.408698   \n",
      "92          92       0.767052      0.001389           0.408689   \n",
      "93          93       0.767054      0.001410           0.408687   \n",
      "94          94       0.767078      0.001414           0.408676   \n",
      "95          95       0.767103      0.001435           0.408664   \n",
      "96          96       0.767096      0.001429           0.408668   \n",
      "97          97       0.767122      0.001448           0.408657   \n",
      "98          98       0.767136      0.001448           0.408658   \n",
      "99          99       0.767173      0.001471           0.408641   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000377            0.576627           0.000468  \n",
      "1           0.000469            0.512498           0.000784  \n",
      "2           0.000332            0.475080           0.000759  \n",
      "3           0.000440            0.452778           0.000184  \n",
      "4           0.000368            0.439526           0.000193  \n",
      "5           0.000641            0.430727           0.000392  \n",
      "6           0.000211            0.424924           0.000530  \n",
      "7           0.000515            0.420906           0.000341  \n",
      "8           0.000502            0.418120           0.000290  \n",
      "9           0.000598            0.416289           0.000215  \n",
      "10          0.000580            0.414764           0.000272  \n",
      "11          0.000560            0.413575           0.000302  \n",
      "12          0.000575            0.412714           0.000241  \n",
      "13          0.000535            0.411975           0.000321  \n",
      "14          0.000525            0.411329           0.000375  \n",
      "15          0.000562            0.410748           0.000334  \n",
      "16          0.000589            0.410255           0.000311  \n",
      "17          0.000591            0.409849           0.000322  \n",
      "18          0.000626            0.409513           0.000260  \n",
      "19          0.000680            0.409115           0.000256  \n",
      "20          0.000684            0.408749           0.000281  \n",
      "21          0.000702            0.408403           0.000346  \n",
      "22          0.000705            0.408051           0.000315  \n",
      "23          0.000699            0.407778           0.000344  \n",
      "24          0.000735            0.407437           0.000358  \n",
      "25          0.000747            0.407118           0.000352  \n",
      "26          0.000747            0.406873           0.000352  \n",
      "27          0.000765            0.406523           0.000349  \n",
      "28          0.000773            0.406232           0.000406  \n",
      "29          0.000762            0.405979           0.000410  \n",
      "30          0.000759            0.405660           0.000416  \n",
      "31          0.000731            0.405424           0.000433  \n",
      "32          0.000749            0.405234           0.000460  \n",
      "33          0.000758            0.404937           0.000474  \n",
      "34          0.000741            0.404694           0.000554  \n",
      "35          0.000767            0.404400           0.000549  \n",
      "36          0.000751            0.404178           0.000563  \n",
      "37          0.000769            0.403931           0.000511  \n",
      "38          0.000760            0.403712           0.000485  \n",
      "39          0.000760            0.403496           0.000518  \n",
      "40          0.000759            0.403293           0.000467  \n",
      "41          0.000777            0.403058           0.000482  \n",
      "42          0.000766            0.402853           0.000556  \n",
      "43          0.000743            0.402656           0.000528  \n",
      "44          0.000765            0.402506           0.000528  \n",
      "45          0.000761            0.402284           0.000488  \n",
      "46          0.000790            0.402086           0.000519  \n",
      "47          0.000822            0.401847           0.000533  \n",
      "48          0.000825            0.401626           0.000482  \n",
      "49          0.000836            0.401374           0.000496  \n",
      "50          0.000851            0.401137           0.000468  \n",
      "51          0.000878            0.400967           0.000473  \n",
      "52          0.000862            0.400812           0.000473  \n",
      "53          0.000854            0.400607           0.000457  \n",
      "54          0.000867            0.400388           0.000518  \n",
      "55          0.000868            0.400171           0.000479  \n",
      "56          0.000863            0.399924           0.000557  \n",
      "57          0.000859            0.399759           0.000550  \n",
      "58          0.000849            0.399516           0.000511  \n",
      "59          0.000848            0.399347           0.000471  \n",
      "60          0.000852            0.399151           0.000439  \n",
      "61          0.000858            0.398976           0.000417  \n",
      "62          0.000855            0.398793           0.000401  \n",
      "63          0.000854            0.398619           0.000424  \n",
      "64          0.000853            0.398394           0.000453  \n",
      "65          0.000862            0.398151           0.000461  \n",
      "66          0.000813            0.397931           0.000507  \n",
      "67          0.000797            0.397739           0.000580  \n",
      "68          0.000781            0.397510           0.000589  \n",
      "69          0.000794            0.397277           0.000565  \n",
      "70          0.000789            0.397023           0.000528  \n",
      "71          0.000803            0.396816           0.000570  \n",
      "72          0.000793            0.396624           0.000578  \n",
      "73          0.000782            0.396447           0.000532  \n",
      "74          0.000791            0.396266           0.000522  \n",
      "75          0.000765            0.396076           0.000490  \n",
      "76          0.000757            0.395902           0.000451  \n",
      "77          0.000751            0.395676           0.000470  \n",
      "78          0.000758            0.395467           0.000505  \n",
      "79          0.000751            0.395337           0.000496  \n",
      "80          0.000741            0.395135           0.000504  \n",
      "81          0.000742            0.394947           0.000463  \n",
      "82          0.000736            0.394751           0.000484  \n",
      "83          0.000740            0.394511           0.000475  \n",
      "84          0.000727            0.394339           0.000474  \n",
      "85          0.000728            0.394148           0.000427  \n",
      "86          0.000732            0.393953           0.000399  \n",
      "87          0.000790            0.393745           0.000455  \n",
      "88          0.000800            0.393540           0.000470  \n",
      "89          0.000795            0.393345           0.000507  \n",
      "90          0.000801            0.393168           0.000487  \n",
      "91          0.000807            0.392995           0.000473  \n",
      "92          0.000803            0.392775           0.000460  \n",
      "93          0.000816            0.392619           0.000453  \n",
      "94          0.000812            0.392423           0.000437  \n",
      "95          0.000824            0.392233           0.000409  \n",
      "96          0.000817            0.392073           0.000417  \n",
      "97          0.000827            0.391935           0.000401  \n",
      "98          0.000827            0.391768           0.000411  \n",
      "99          0.000843            0.391604           0.000366  \n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7672   \u001b[0m | \u001b[0m5.27     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.2      \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7645282438\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7666009762\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7674804574\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.627960   \n",
      "1            1       0.744642      0.001012           0.579741   \n",
      "2            2       0.748985      0.001535           0.542861   \n",
      "3            3       0.750341      0.001300           0.514773   \n",
      "4            4       0.750738      0.000767           0.493975   \n",
      "5            5       0.751062      0.000893           0.477674   \n",
      "6            6       0.751729      0.000778           0.464856   \n",
      "7            7       0.752557      0.000965           0.454880   \n",
      "8            8       0.753529      0.001044           0.447280   \n",
      "9            9       0.754136      0.001297           0.440968   \n",
      "10          10       0.754252      0.001287           0.436017   \n",
      "11          11       0.754621      0.001200           0.431897   \n",
      "12          12       0.755114      0.001175           0.428793   \n",
      "13          13       0.755700      0.001014           0.426149   \n",
      "14          14       0.756176      0.001137           0.423915   \n",
      "15          15       0.756555      0.001120           0.422205   \n",
      "16          16       0.757041      0.001069           0.420687   \n",
      "17          17       0.757511      0.001082           0.419503   \n",
      "18          18       0.757613      0.001020           0.418524   \n",
      "19          19       0.757884      0.001087           0.417666   \n",
      "20          20       0.758227      0.001047           0.416892   \n",
      "21          21       0.758546      0.001093           0.416187   \n",
      "22          22       0.758802      0.001134           0.415663   \n",
      "23          23       0.759158      0.001180           0.415191   \n",
      "24          24       0.759442      0.001203           0.414787   \n",
      "25          25       0.759709      0.001208           0.414410   \n",
      "26          26       0.759936      0.001252           0.414049   \n",
      "27          27       0.760076      0.001253           0.413796   \n",
      "28          28       0.760304      0.001294           0.413545   \n",
      "29          29       0.760476      0.001312           0.413345   \n",
      "30          30       0.760696      0.001348           0.413159   \n",
      "31          31       0.760892      0.001403           0.412972   \n",
      "32          32       0.761096      0.001384           0.412759   \n",
      "33          33       0.761284      0.001417           0.412607   \n",
      "34          34       0.761389      0.001396           0.412470   \n",
      "35          35       0.761582      0.001428           0.412312   \n",
      "36          36       0.761731      0.001399           0.412187   \n",
      "37          37       0.761823      0.001404           0.412087   \n",
      "38          38       0.761968      0.001370           0.411983   \n",
      "39          39       0.762098      0.001373           0.411864   \n",
      "40          40       0.762246      0.001357           0.411762   \n",
      "41          41       0.762401      0.001402           0.411663   \n",
      "42          42       0.762500      0.001416           0.411590   \n",
      "43          43       0.762608      0.001397           0.411500   \n",
      "44          44       0.762692      0.001400           0.411435   \n",
      "45          45       0.762768      0.001410           0.411382   \n",
      "46          46       0.762855      0.001404           0.411315   \n",
      "47          47       0.762954      0.001417           0.411237   \n",
      "48          48       0.763072      0.001439           0.411164   \n",
      "49          49       0.763176      0.001456           0.411082   \n",
      "50          50       0.763268      0.001488           0.411020   \n",
      "51          51       0.763386      0.001498           0.410946   \n",
      "52          52       0.763479      0.001505           0.410882   \n",
      "53          53       0.763560      0.001511           0.410826   \n",
      "54          54       0.763626      0.001521           0.410780   \n",
      "55          55       0.763699      0.001530           0.410723   \n",
      "56          56       0.763811      0.001539           0.410649   \n",
      "57          57       0.763881      0.001554           0.410603   \n",
      "58          58       0.763940      0.001554           0.410561   \n",
      "59          59       0.764045      0.001587           0.410504   \n",
      "60          60       0.764124      0.001584           0.410455   \n",
      "61          61       0.764179      0.001559           0.410421   \n",
      "62          62       0.764249      0.001568           0.410385   \n",
      "63          63       0.764327      0.001558           0.410337   \n",
      "64          64       0.764420      0.001566           0.410284   \n",
      "65          65       0.764499      0.001578           0.410233   \n",
      "66          66       0.764559      0.001578           0.410199   \n",
      "67          67       0.764635      0.001603           0.410156   \n",
      "68          68       0.764690      0.001587           0.410126   \n",
      "69          69       0.764757      0.001590           0.410082   \n",
      "70          70       0.764824      0.001593           0.410039   \n",
      "71          71       0.764885      0.001590           0.409999   \n",
      "72          72       0.764938      0.001609           0.409964   \n",
      "73          73       0.765005      0.001601           0.409879   \n",
      "74          74       0.765058      0.001598           0.409846   \n",
      "75          75       0.765107      0.001601           0.409815   \n",
      "76          76       0.765165      0.001583           0.409784   \n",
      "77          77       0.765234      0.001581           0.409749   \n",
      "78          78       0.765290      0.001577           0.409714   \n",
      "79          79       0.765339      0.001559           0.409683   \n",
      "80          80       0.765390      0.001546           0.409650   \n",
      "81          81       0.765442      0.001523           0.409620   \n",
      "82          82       0.765505      0.001514           0.409584   \n",
      "83          83       0.765559      0.001512           0.409553   \n",
      "84          84       0.765612      0.001499           0.409524   \n",
      "85          85       0.765636      0.001516           0.409509   \n",
      "86          86       0.765675      0.001505           0.409488   \n",
      "87          87       0.765704      0.001506           0.409472   \n",
      "88          88       0.765740      0.001505           0.409449   \n",
      "89          89       0.765782      0.001493           0.409426   \n",
      "90          90       0.765831      0.001489           0.409397   \n",
      "91          91       0.765869      0.001502           0.409371   \n",
      "92          92       0.765906      0.001507           0.409350   \n",
      "93          93       0.765945      0.001519           0.409328   \n",
      "94          94       0.765978      0.001525           0.409306   \n",
      "95          95       0.766023      0.001531           0.409280   \n",
      "96          96       0.766072      0.001517           0.409253   \n",
      "97          97       0.766117      0.001517           0.409229   \n",
      "98          98       0.766148      0.001511           0.409177   \n",
      "99          99       0.766203      0.001516           0.409145   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000256            0.627866           0.000278  \n",
      "1           0.000708            0.579582           0.000491  \n",
      "2           0.000599            0.542670           0.000321  \n",
      "3           0.000261            0.514538           0.000145  \n",
      "4           0.000322            0.493713           0.000497  \n",
      "5           0.000607            0.477357           0.000590  \n",
      "6           0.000439            0.464482           0.000600  \n",
      "7           0.000437            0.454469           0.000482  \n",
      "8           0.000307            0.446798           0.000370  \n",
      "9           0.000550            0.440444           0.000193  \n",
      "10          0.000508            0.435449           0.000273  \n",
      "11          0.000530            0.431263           0.000226  \n",
      "12          0.000584            0.428087           0.000150  \n",
      "13          0.000521            0.425366           0.000228  \n",
      "14          0.000499            0.423024           0.000286  \n",
      "15          0.000506            0.421179           0.000282  \n",
      "16          0.000493            0.419574           0.000353  \n",
      "17          0.000477            0.418288           0.000355  \n",
      "18          0.000480            0.417284           0.000363  \n",
      "19          0.000506            0.416402           0.000371  \n",
      "20          0.000493            0.415485           0.000404  \n",
      "21          0.000514            0.414716           0.000399  \n",
      "22          0.000510            0.414080           0.000413  \n",
      "23          0.000546            0.413517           0.000412  \n",
      "24          0.000574            0.413060           0.000398  \n",
      "25          0.000533            0.412559           0.000463  \n",
      "26          0.000569            0.412118           0.000451  \n",
      "27          0.000576            0.411818           0.000468  \n",
      "28          0.000594            0.411435           0.000465  \n",
      "29          0.000614            0.411129           0.000459  \n",
      "30          0.000638            0.410821           0.000421  \n",
      "31          0.000653            0.410533           0.000404  \n",
      "32          0.000652            0.410239           0.000424  \n",
      "33          0.000664            0.409969           0.000419  \n",
      "34          0.000658            0.409771           0.000432  \n",
      "35          0.000687            0.409506           0.000420  \n",
      "36          0.000688            0.409294           0.000411  \n",
      "37          0.000702            0.409057           0.000412  \n",
      "38          0.000698            0.408852           0.000396  \n",
      "39          0.000711            0.408633           0.000438  \n",
      "40          0.000706            0.408475           0.000427  \n",
      "41          0.000737            0.408265           0.000420  \n",
      "42          0.000748            0.408070           0.000405  \n",
      "43          0.000742            0.407874           0.000418  \n",
      "44          0.000750            0.407745           0.000418  \n",
      "45          0.000755            0.407602           0.000418  \n",
      "46          0.000740            0.407451           0.000452  \n",
      "47          0.000746            0.407274           0.000453  \n",
      "48          0.000753            0.407123           0.000454  \n",
      "49          0.000760            0.406955           0.000444  \n",
      "50          0.000774            0.406803           0.000450  \n",
      "51          0.000775            0.406646           0.000451  \n",
      "52          0.000783            0.406468           0.000434  \n",
      "53          0.000787            0.406314           0.000459  \n",
      "54          0.000792            0.406201           0.000495  \n",
      "55          0.000802            0.406049           0.000482  \n",
      "56          0.000816            0.405913           0.000455  \n",
      "57          0.000823            0.405771           0.000462  \n",
      "58          0.000821            0.405653           0.000440  \n",
      "59          0.000839            0.405476           0.000422  \n",
      "60          0.000838            0.405329           0.000408  \n",
      "61          0.000825            0.405195           0.000405  \n",
      "62          0.000822            0.405056           0.000403  \n",
      "63          0.000814            0.404936           0.000406  \n",
      "64          0.000821            0.404771           0.000414  \n",
      "65          0.000835            0.404632           0.000402  \n",
      "66          0.000835            0.404534           0.000405  \n",
      "67          0.000851            0.404379           0.000392  \n",
      "68          0.000848            0.404235           0.000402  \n",
      "69          0.000849            0.404127           0.000406  \n",
      "70          0.000853            0.403979           0.000391  \n",
      "71          0.000855            0.403846           0.000378  \n",
      "72          0.000865            0.403735           0.000372  \n",
      "73          0.000870            0.403576           0.000364  \n",
      "74          0.000871            0.403434           0.000357  \n",
      "75          0.000875            0.403323           0.000366  \n",
      "76          0.000865            0.403200           0.000371  \n",
      "77          0.000865            0.403062           0.000341  \n",
      "78          0.000861            0.402953           0.000354  \n",
      "79          0.000855            0.402826           0.000351  \n",
      "80          0.000847            0.402734           0.000354  \n",
      "81          0.000839            0.402599           0.000379  \n",
      "82          0.000838            0.402510           0.000383  \n",
      "83          0.000839            0.402367           0.000371  \n",
      "84          0.000837            0.402259           0.000396  \n",
      "85          0.000844            0.402127           0.000400  \n",
      "86          0.000837            0.402023           0.000389  \n",
      "87          0.000841            0.401918           0.000388  \n",
      "88          0.000838            0.401791           0.000394  \n",
      "89          0.000830            0.401684           0.000403  \n",
      "90          0.000823            0.401555           0.000403  \n",
      "91          0.000830            0.401432           0.000417  \n",
      "92          0.000832            0.401320           0.000438  \n",
      "93          0.000836            0.401225           0.000439  \n",
      "94          0.000841            0.401096           0.000418  \n",
      "95          0.000842            0.400978           0.000421  \n",
      "96          0.000833            0.400855           0.000429  \n",
      "97          0.000833            0.400769           0.000437  \n",
      "98          0.000850            0.400641           0.000433  \n",
      "99          0.000849            0.400515           0.000457  \n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.7662   \u001b[0m | \u001b[0m1.539    \u001b[0m | \u001b[0m9.991    \u001b[0m | \u001b[0m0.1027   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655786159\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7673985185\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7684543914\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.738094      0.000608           0.595942   \n",
      "1            1       0.745397      0.000814           0.535797   \n",
      "2            2       0.747136      0.000654           0.497606   \n",
      "3            3       0.748766      0.000829           0.472051   \n",
      "4            4       0.748728      0.000940           0.455769   \n",
      "5            5       0.750324      0.000722           0.444039   \n",
      "6            6       0.751296      0.000906           0.436108   \n",
      "7            7       0.752703      0.001258           0.430271   \n",
      "8            8       0.753686      0.001176           0.425993   \n",
      "9            9       0.754574      0.001166           0.423038   \n",
      "10          10       0.755193      0.001061           0.420894   \n",
      "11          11       0.755731      0.001189           0.419213   \n",
      "12          12       0.756447      0.001131           0.417978   \n",
      "13          13       0.756811      0.001023           0.416948   \n",
      "14          14       0.757318      0.001051           0.416169   \n",
      "15          15       0.757923      0.001158           0.415502   \n",
      "16          16       0.758305      0.001229           0.415042   \n",
      "17          17       0.758706      0.001264           0.414546   \n",
      "18          18       0.758981      0.001245           0.414181   \n",
      "19          19       0.759308      0.001259           0.413854   \n",
      "20          20       0.759716      0.001286           0.413515   \n",
      "21          21       0.759944      0.001312           0.413302   \n",
      "22          22       0.760170      0.001305           0.413053   \n",
      "23          23       0.760369      0.001275           0.412861   \n",
      "24          24       0.760584      0.001286           0.412677   \n",
      "25          25       0.760827      0.001330           0.412502   \n",
      "26          26       0.761058      0.001312           0.412340   \n",
      "27          27       0.761258      0.001308           0.412174   \n",
      "28          28       0.761412      0.001283           0.412056   \n",
      "29          29       0.761547      0.001258           0.411951   \n",
      "30          30       0.761751      0.001280           0.411826   \n",
      "31          31       0.761941      0.001313           0.411707   \n",
      "32          32       0.762073      0.001368           0.411615   \n",
      "33          33       0.762212      0.001378           0.411521   \n",
      "34          34       0.762347      0.001352           0.411445   \n",
      "35          35       0.762519      0.001329           0.411341   \n",
      "36          36       0.762667      0.001329           0.411252   \n",
      "37          37       0.762810      0.001314           0.411169   \n",
      "38          38       0.762937      0.001309           0.411095   \n",
      "39          39       0.763057      0.001294           0.411013   \n",
      "40          40       0.763187      0.001345           0.410931   \n",
      "41          41       0.763272      0.001333           0.410872   \n",
      "42          42       0.763370      0.001331           0.410816   \n",
      "43          43       0.763481      0.001297           0.410748   \n",
      "44          44       0.763627      0.001292           0.410670   \n",
      "45          45       0.763704      0.001299           0.410620   \n",
      "46          46       0.763795      0.001277           0.410567   \n",
      "47          47       0.763897      0.001261           0.410515   \n",
      "48          48       0.764004      0.001297           0.410440   \n",
      "49          49       0.764078      0.001310           0.410393   \n",
      "50          50       0.764156      0.001335           0.410346   \n",
      "51          51       0.764245      0.001349           0.410290   \n",
      "52          52       0.764328      0.001360           0.410238   \n",
      "53          53       0.764424      0.001374           0.410184   \n",
      "54          54       0.764513      0.001374           0.410136   \n",
      "55          55       0.764576      0.001367           0.410097   \n",
      "56          56       0.764676      0.001375           0.410041   \n",
      "57          57       0.764740      0.001386           0.410007   \n",
      "58          58       0.764810      0.001382           0.409965   \n",
      "59          59       0.764894      0.001361           0.409920   \n",
      "60          60       0.764953      0.001336           0.409883   \n",
      "61          61       0.765038      0.001355           0.409836   \n",
      "62          62       0.765113      0.001347           0.409793   \n",
      "63          63       0.765179      0.001373           0.409751   \n",
      "64          64       0.765247      0.001381           0.409718   \n",
      "65          65       0.765330      0.001375           0.409629   \n",
      "66          66       0.765399      0.001402           0.409592   \n",
      "67          67       0.765450      0.001402           0.409560   \n",
      "68          68       0.765505      0.001380           0.409526   \n",
      "69          69       0.765558      0.001381           0.409495   \n",
      "70          70       0.765639      0.001382           0.409454   \n",
      "71          71       0.765748      0.001368           0.409390   \n",
      "72          72       0.765808      0.001364           0.409354   \n",
      "73          73       0.765868      0.001359           0.409320   \n",
      "74          74       0.765921      0.001386           0.409290   \n",
      "75          75       0.765945      0.001398           0.409276   \n",
      "76          76       0.765991      0.001404           0.409246   \n",
      "77          77       0.766051      0.001422           0.409210   \n",
      "78          78       0.766121      0.001414           0.409170   \n",
      "79          79       0.766194      0.001429           0.409125   \n",
      "80          80       0.766247      0.001405           0.409095   \n",
      "81          81       0.766284      0.001390           0.409067   \n",
      "82          82       0.766320      0.001383           0.409047   \n",
      "83          83       0.766372      0.001395           0.409019   \n",
      "84          84       0.766407      0.001381           0.408998   \n",
      "85          85       0.766466      0.001385           0.408961   \n",
      "86          86       0.766498      0.001392           0.408940   \n",
      "87          87       0.766545      0.001391           0.408911   \n",
      "88          88       0.766592      0.001399           0.408881   \n",
      "89          89       0.766657      0.001415           0.408846   \n",
      "90          90       0.766710      0.001391           0.408815   \n",
      "91          91       0.766748      0.001384           0.408795   \n",
      "92          92       0.766805      0.001403           0.408762   \n",
      "93          93       0.766847      0.001412           0.408734   \n",
      "94          94       0.766904      0.001444           0.408699   \n",
      "95          95       0.766938      0.001451           0.408679   \n",
      "96          96       0.766999      0.001471           0.408647   \n",
      "97          97       0.767038      0.001474           0.408625   \n",
      "98          98       0.767088      0.001465           0.408592   \n",
      "99          99       0.767144      0.001455           0.408562   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000399            0.595858           0.000499  \n",
      "1           0.000394            0.535716           0.000551  \n",
      "2           0.000290            0.497530           0.000567  \n",
      "3           0.000522            0.471966           0.000291  \n",
      "4           0.000461            0.455662           0.000317  \n",
      "5           0.000228            0.443897           0.000289  \n",
      "6           0.000324            0.435917           0.000531  \n",
      "7           0.000437            0.430037           0.000426  \n",
      "8           0.000217            0.425698           0.000536  \n",
      "9           0.000441            0.422692           0.000400  \n",
      "10          0.000357            0.420500           0.000470  \n",
      "11          0.000423            0.418758           0.000437  \n",
      "12          0.000453            0.417484           0.000421  \n",
      "13          0.000488            0.416404           0.000436  \n",
      "14          0.000425            0.415565           0.000534  \n",
      "15          0.000550            0.414836           0.000397  \n",
      "16          0.000533            0.414339           0.000439  \n",
      "17          0.000567            0.413793           0.000425  \n",
      "18          0.000588            0.413362           0.000429  \n",
      "19          0.000573            0.412996           0.000447  \n",
      "20          0.000612            0.412601           0.000431  \n",
      "21          0.000625            0.412335           0.000422  \n",
      "22          0.000638            0.412030           0.000436  \n",
      "23          0.000635            0.411787           0.000443  \n",
      "24          0.000615            0.411569           0.000471  \n",
      "25          0.000667            0.411362           0.000425  \n",
      "26          0.000645            0.411140           0.000455  \n",
      "27          0.000637            0.410931           0.000491  \n",
      "28          0.000638            0.410755           0.000501  \n",
      "29          0.000641            0.410581           0.000508  \n",
      "30          0.000648            0.410399           0.000506  \n",
      "31          0.000664            0.410219           0.000500  \n",
      "32          0.000685            0.410079           0.000475  \n",
      "33          0.000700            0.409947           0.000488  \n",
      "34          0.000687            0.409806           0.000513  \n",
      "35          0.000688            0.409642           0.000534  \n",
      "36          0.000691            0.409505           0.000532  \n",
      "37          0.000686            0.409371           0.000533  \n",
      "38          0.000686            0.409246           0.000530  \n",
      "39          0.000690            0.409110           0.000536  \n",
      "40          0.000697            0.408990           0.000535  \n",
      "41          0.000689            0.408873           0.000541  \n",
      "42          0.000697            0.408767           0.000533  \n",
      "43          0.000689            0.408644           0.000541  \n",
      "44          0.000690            0.408507           0.000539  \n",
      "45          0.000695            0.408402           0.000543  \n",
      "46          0.000684            0.408290           0.000537  \n",
      "47          0.000680            0.408182           0.000540  \n",
      "48          0.000705            0.408070           0.000516  \n",
      "49          0.000711            0.407991           0.000525  \n",
      "50          0.000724            0.407882           0.000522  \n",
      "51          0.000733            0.407762           0.000518  \n",
      "52          0.000739            0.407649           0.000519  \n",
      "53          0.000755            0.407545           0.000514  \n",
      "54          0.000759            0.407447           0.000523  \n",
      "55          0.000757            0.407363           0.000530  \n",
      "56          0.000764            0.407256           0.000519  \n",
      "57          0.000765            0.407174           0.000521  \n",
      "58          0.000762            0.407083           0.000520  \n",
      "59          0.000750            0.406979           0.000519  \n",
      "60          0.000741            0.406900           0.000528  \n",
      "61          0.000748            0.406797           0.000522  \n",
      "62          0.000740            0.406711           0.000540  \n",
      "63          0.000741            0.406620           0.000535  \n",
      "64          0.000744            0.406535           0.000516  \n",
      "65          0.000738            0.406415           0.000519  \n",
      "66          0.000753            0.406328           0.000540  \n",
      "67          0.000753            0.406239           0.000541  \n",
      "68          0.000747            0.406133           0.000554  \n",
      "69          0.000746            0.406053           0.000551  \n",
      "70          0.000741            0.405961           0.000563  \n",
      "71          0.000734            0.405866           0.000570  \n",
      "72          0.000726            0.405779           0.000579  \n",
      "73          0.000724            0.405689           0.000583  \n",
      "74          0.000739            0.405604           0.000562  \n",
      "75          0.000744            0.405527           0.000560  \n",
      "76          0.000752            0.405429           0.000566  \n",
      "77          0.000760            0.405355           0.000560  \n",
      "78          0.000757            0.405278           0.000562  \n",
      "79          0.000767            0.405189           0.000559  \n",
      "80          0.000755            0.405109           0.000568  \n",
      "81          0.000754            0.405022           0.000569  \n",
      "82          0.000750            0.404950           0.000556  \n",
      "83          0.000754            0.404875           0.000548  \n",
      "84          0.000748            0.404812           0.000554  \n",
      "85          0.000748            0.404730           0.000556  \n",
      "86          0.000752            0.404649           0.000564  \n",
      "87          0.000754            0.404570           0.000553  \n",
      "88          0.000760            0.404493           0.000545  \n",
      "89          0.000766            0.404397           0.000548  \n",
      "90          0.000754            0.404317           0.000551  \n",
      "91          0.000750            0.404240           0.000554  \n",
      "92          0.000763            0.404150           0.000547  \n",
      "93          0.000778            0.404065           0.000544  \n",
      "94          0.000796            0.403976           0.000554  \n",
      "95          0.000796            0.403891           0.000553  \n",
      "96          0.000809            0.403803           0.000543  \n",
      "97          0.000810            0.403712           0.000531  \n",
      "98          0.000809            0.403621           0.000543  \n",
      "99          0.000803            0.403513           0.000542  \n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7671   \u001b[0m | \u001b[0m0.1175   \u001b[0m | \u001b[0m8.058    \u001b[0m | \u001b[0m0.1612   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655541754\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7675429872\n",
      "bestIteration = 98\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7684289483\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.576738   \n",
      "1            1       0.744740      0.000670           0.512706   \n",
      "2            2       0.749463      0.001013           0.475405   \n",
      "3            3       0.751354      0.001265           0.453182   \n",
      "4            4       0.751875      0.001071           0.439969   \n",
      "5            5       0.753205      0.001234           0.431267   \n",
      "6            6       0.754077      0.001066           0.425633   \n",
      "7            7       0.755244      0.001546           0.421795   \n",
      "8            8       0.756253      0.001553           0.419221   \n",
      "9            9       0.757041      0.001528           0.417502   \n",
      "10          10       0.757601      0.001346           0.416223   \n",
      "11          11       0.758302      0.001350           0.415245   \n",
      "12          12       0.758767      0.001240           0.414567   \n",
      "13          13       0.759222      0.001217           0.413983   \n",
      "14          14       0.759632      0.001250           0.413551   \n",
      "15          15       0.760032      0.001247           0.413154   \n",
      "16          16       0.760452      0.001252           0.412830   \n",
      "17          17       0.760799      0.001263           0.412572   \n",
      "18          18       0.760995      0.001314           0.412380   \n",
      "19          19       0.761246      0.001337           0.412174   \n",
      "20          20       0.761526      0.001310           0.411999   \n",
      "21          21       0.761744      0.001292           0.411823   \n",
      "22          22       0.761932      0.001285           0.411696   \n",
      "23          23       0.762130      0.001270           0.411568   \n",
      "24          24       0.762322      0.001345           0.411448   \n",
      "25          25       0.762559      0.001392           0.411315   \n",
      "26          26       0.762709      0.001372           0.411216   \n",
      "27          27       0.762875      0.001407           0.411104   \n",
      "28          28       0.763054      0.001423           0.411001   \n",
      "29          29       0.763153      0.001408           0.410940   \n",
      "30          30       0.763307      0.001388           0.410851   \n",
      "31          31       0.763426      0.001331           0.410781   \n",
      "32          32       0.763524      0.001370           0.410702   \n",
      "33          33       0.763692      0.001397           0.410596   \n",
      "34          34       0.763794      0.001344           0.410536   \n",
      "35          35       0.763964      0.001384           0.410448   \n",
      "36          36       0.764081      0.001341           0.410375   \n",
      "37          37       0.764169      0.001373           0.410328   \n",
      "38          38       0.764291      0.001331           0.410255   \n",
      "39          39       0.764397      0.001329           0.410202   \n",
      "40          40       0.764534      0.001334           0.410126   \n",
      "41          41       0.764616      0.001368           0.410084   \n",
      "42          42       0.764704      0.001342           0.410039   \n",
      "43          43       0.764800      0.001293           0.409988   \n",
      "44          44       0.764863      0.001322           0.409954   \n",
      "45          45       0.764913      0.001313           0.409933   \n",
      "46          46       0.764986      0.001365           0.409885   \n",
      "47          47       0.765084      0.001416           0.409830   \n",
      "48          48       0.765155      0.001417           0.409790   \n",
      "49          49       0.765238      0.001432           0.409749   \n",
      "50          50       0.765282      0.001470           0.409722   \n",
      "51          51       0.765373      0.001507           0.409671   \n",
      "52          52       0.765480      0.001482           0.409612   \n",
      "53          53       0.765539      0.001467           0.409575   \n",
      "54          54       0.765573      0.001483           0.409560   \n",
      "55          55       0.765613      0.001487           0.409535   \n",
      "56          56       0.765691      0.001471           0.409496   \n",
      "57          57       0.765690      0.001448           0.409492   \n",
      "58          58       0.765766      0.001418           0.409454   \n",
      "59          59       0.765835      0.001421           0.409415   \n",
      "60          60       0.765920      0.001449           0.409363   \n",
      "61          61       0.765963      0.001462           0.409338   \n",
      "62          62       0.766033      0.001464           0.409306   \n",
      "63          63       0.766069      0.001468           0.409290   \n",
      "64          64       0.766101      0.001472           0.409271   \n",
      "65          65       0.766151      0.001477           0.409251   \n",
      "66          66       0.766168      0.001458           0.409204   \n",
      "67          67       0.766231      0.001430           0.409173   \n",
      "68          68       0.766275      0.001403           0.409155   \n",
      "69          69       0.766341      0.001438           0.409121   \n",
      "70          70       0.766379      0.001431           0.409104   \n",
      "71          71       0.766408      0.001443           0.409092   \n",
      "72          72       0.766423      0.001426           0.409082   \n",
      "73          73       0.766477      0.001420           0.409018   \n",
      "74          74       0.766570      0.001434           0.408973   \n",
      "75          75       0.766634      0.001382           0.408940   \n",
      "76          76       0.766645      0.001375           0.408937   \n",
      "77          77       0.766685      0.001381           0.408913   \n",
      "78          78       0.766700      0.001385           0.408908   \n",
      "79          79       0.766765      0.001376           0.408871   \n",
      "80          80       0.766810      0.001370           0.408849   \n",
      "81          81       0.766824      0.001391           0.408845   \n",
      "82          82       0.766861      0.001368           0.408821   \n",
      "83          83       0.766896      0.001369           0.408811   \n",
      "84          84       0.766929      0.001337           0.408784   \n",
      "85          85       0.766970      0.001331           0.408767   \n",
      "86          86       0.766986      0.001332           0.408755   \n",
      "87          87       0.766994      0.001360           0.408720   \n",
      "88          88       0.767034      0.001382           0.408699   \n",
      "89          89       0.767026      0.001386           0.408703   \n",
      "90          90       0.767022      0.001391           0.408702   \n",
      "91          91       0.767032      0.001390           0.408698   \n",
      "92          92       0.767052      0.001389           0.408689   \n",
      "93          93       0.767054      0.001410           0.408687   \n",
      "94          94       0.767078      0.001414           0.408676   \n",
      "95          95       0.767103      0.001435           0.408664   \n",
      "96          96       0.767096      0.001429           0.408668   \n",
      "97          97       0.767122      0.001448           0.408657   \n",
      "98          98       0.767136      0.001448           0.408658   \n",
      "99          99       0.767173      0.001471           0.408641   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000377            0.576627           0.000468  \n",
      "1           0.000469            0.512498           0.000784  \n",
      "2           0.000332            0.475080           0.000759  \n",
      "3           0.000440            0.452778           0.000184  \n",
      "4           0.000368            0.439526           0.000193  \n",
      "5           0.000641            0.430727           0.000392  \n",
      "6           0.000211            0.424924           0.000530  \n",
      "7           0.000515            0.420906           0.000341  \n",
      "8           0.000502            0.418120           0.000290  \n",
      "9           0.000598            0.416289           0.000215  \n",
      "10          0.000580            0.414764           0.000272  \n",
      "11          0.000560            0.413575           0.000302  \n",
      "12          0.000575            0.412714           0.000241  \n",
      "13          0.000535            0.411975           0.000321  \n",
      "14          0.000525            0.411329           0.000375  \n",
      "15          0.000562            0.410748           0.000334  \n",
      "16          0.000589            0.410255           0.000311  \n",
      "17          0.000591            0.409849           0.000322  \n",
      "18          0.000626            0.409513           0.000260  \n",
      "19          0.000680            0.409115           0.000256  \n",
      "20          0.000684            0.408749           0.000281  \n",
      "21          0.000702            0.408403           0.000346  \n",
      "22          0.000705            0.408051           0.000315  \n",
      "23          0.000699            0.407778           0.000344  \n",
      "24          0.000735            0.407437           0.000358  \n",
      "25          0.000747            0.407118           0.000352  \n",
      "26          0.000747            0.406873           0.000352  \n",
      "27          0.000765            0.406523           0.000349  \n",
      "28          0.000773            0.406232           0.000406  \n",
      "29          0.000762            0.405979           0.000410  \n",
      "30          0.000759            0.405660           0.000416  \n",
      "31          0.000731            0.405424           0.000433  \n",
      "32          0.000749            0.405234           0.000460  \n",
      "33          0.000758            0.404937           0.000474  \n",
      "34          0.000741            0.404694           0.000554  \n",
      "35          0.000767            0.404400           0.000549  \n",
      "36          0.000751            0.404178           0.000563  \n",
      "37          0.000769            0.403931           0.000511  \n",
      "38          0.000760            0.403712           0.000485  \n",
      "39          0.000760            0.403496           0.000518  \n",
      "40          0.000759            0.403293           0.000467  \n",
      "41          0.000777            0.403058           0.000482  \n",
      "42          0.000766            0.402853           0.000556  \n",
      "43          0.000743            0.402656           0.000528  \n",
      "44          0.000765            0.402506           0.000528  \n",
      "45          0.000761            0.402284           0.000488  \n",
      "46          0.000790            0.402086           0.000519  \n",
      "47          0.000822            0.401847           0.000533  \n",
      "48          0.000825            0.401626           0.000482  \n",
      "49          0.000836            0.401374           0.000496  \n",
      "50          0.000851            0.401137           0.000468  \n",
      "51          0.000878            0.400967           0.000473  \n",
      "52          0.000862            0.400812           0.000473  \n",
      "53          0.000854            0.400607           0.000457  \n",
      "54          0.000867            0.400388           0.000518  \n",
      "55          0.000868            0.400171           0.000479  \n",
      "56          0.000863            0.399924           0.000557  \n",
      "57          0.000859            0.399759           0.000550  \n",
      "58          0.000849            0.399516           0.000511  \n",
      "59          0.000848            0.399347           0.000471  \n",
      "60          0.000852            0.399151           0.000439  \n",
      "61          0.000858            0.398976           0.000417  \n",
      "62          0.000855            0.398793           0.000401  \n",
      "63          0.000854            0.398619           0.000424  \n",
      "64          0.000853            0.398394           0.000453  \n",
      "65          0.000862            0.398151           0.000461  \n",
      "66          0.000813            0.397931           0.000507  \n",
      "67          0.000797            0.397739           0.000580  \n",
      "68          0.000781            0.397510           0.000589  \n",
      "69          0.000794            0.397277           0.000565  \n",
      "70          0.000789            0.397023           0.000528  \n",
      "71          0.000803            0.396816           0.000570  \n",
      "72          0.000793            0.396624           0.000578  \n",
      "73          0.000782            0.396447           0.000532  \n",
      "74          0.000791            0.396266           0.000522  \n",
      "75          0.000765            0.396076           0.000490  \n",
      "76          0.000757            0.395902           0.000451  \n",
      "77          0.000751            0.395676           0.000470  \n",
      "78          0.000758            0.395467           0.000505  \n",
      "79          0.000751            0.395337           0.000496  \n",
      "80          0.000741            0.395135           0.000504  \n",
      "81          0.000742            0.394947           0.000463  \n",
      "82          0.000736            0.394751           0.000484  \n",
      "83          0.000740            0.394511           0.000475  \n",
      "84          0.000727            0.394339           0.000474  \n",
      "85          0.000728            0.394148           0.000427  \n",
      "86          0.000732            0.393953           0.000399  \n",
      "87          0.000790            0.393745           0.000455  \n",
      "88          0.000800            0.393540           0.000470  \n",
      "89          0.000795            0.393345           0.000507  \n",
      "90          0.000801            0.393168           0.000487  \n",
      "91          0.000807            0.392995           0.000473  \n",
      "92          0.000803            0.392775           0.000460  \n",
      "93          0.000816            0.392619           0.000453  \n",
      "94          0.000812            0.392423           0.000437  \n",
      "95          0.000824            0.392233           0.000409  \n",
      "96          0.000817            0.392073           0.000417  \n",
      "97          0.000827            0.391935           0.000401  \n",
      "98          0.000827            0.391768           0.000411  \n",
      "99          0.000843            0.391604           0.000366  \n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7672   \u001b[0m | \u001b[0m7.933    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.2      \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655541754\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7675429872\n",
      "bestIteration = 98\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7684289483\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.576738   \n",
      "1            1       0.744740      0.000670           0.512706   \n",
      "2            2       0.749463      0.001013           0.475405   \n",
      "3            3       0.751354      0.001265           0.453182   \n",
      "4            4       0.751875      0.001071           0.439969   \n",
      "5            5       0.753205      0.001234           0.431267   \n",
      "6            6       0.754077      0.001066           0.425633   \n",
      "7            7       0.755244      0.001546           0.421795   \n",
      "8            8       0.756253      0.001553           0.419221   \n",
      "9            9       0.757041      0.001528           0.417502   \n",
      "10          10       0.757601      0.001346           0.416223   \n",
      "11          11       0.758302      0.001350           0.415245   \n",
      "12          12       0.758767      0.001240           0.414567   \n",
      "13          13       0.759222      0.001217           0.413983   \n",
      "14          14       0.759632      0.001250           0.413551   \n",
      "15          15       0.760032      0.001247           0.413154   \n",
      "16          16       0.760452      0.001252           0.412830   \n",
      "17          17       0.760799      0.001263           0.412572   \n",
      "18          18       0.760995      0.001314           0.412380   \n",
      "19          19       0.761246      0.001337           0.412174   \n",
      "20          20       0.761526      0.001310           0.411999   \n",
      "21          21       0.761744      0.001292           0.411823   \n",
      "22          22       0.761932      0.001285           0.411696   \n",
      "23          23       0.762130      0.001270           0.411568   \n",
      "24          24       0.762322      0.001345           0.411448   \n",
      "25          25       0.762559      0.001392           0.411315   \n",
      "26          26       0.762709      0.001372           0.411216   \n",
      "27          27       0.762875      0.001407           0.411104   \n",
      "28          28       0.763054      0.001423           0.411001   \n",
      "29          29       0.763153      0.001408           0.410940   \n",
      "30          30       0.763307      0.001388           0.410851   \n",
      "31          31       0.763426      0.001331           0.410781   \n",
      "32          32       0.763524      0.001370           0.410702   \n",
      "33          33       0.763692      0.001397           0.410596   \n",
      "34          34       0.763794      0.001344           0.410536   \n",
      "35          35       0.763964      0.001384           0.410448   \n",
      "36          36       0.764081      0.001341           0.410375   \n",
      "37          37       0.764169      0.001373           0.410328   \n",
      "38          38       0.764291      0.001331           0.410255   \n",
      "39          39       0.764397      0.001329           0.410202   \n",
      "40          40       0.764534      0.001334           0.410126   \n",
      "41          41       0.764616      0.001368           0.410084   \n",
      "42          42       0.764704      0.001342           0.410039   \n",
      "43          43       0.764800      0.001293           0.409988   \n",
      "44          44       0.764863      0.001322           0.409954   \n",
      "45          45       0.764913      0.001313           0.409933   \n",
      "46          46       0.764986      0.001365           0.409885   \n",
      "47          47       0.765084      0.001416           0.409830   \n",
      "48          48       0.765155      0.001417           0.409790   \n",
      "49          49       0.765238      0.001432           0.409749   \n",
      "50          50       0.765282      0.001470           0.409722   \n",
      "51          51       0.765373      0.001507           0.409671   \n",
      "52          52       0.765480      0.001482           0.409612   \n",
      "53          53       0.765539      0.001467           0.409575   \n",
      "54          54       0.765573      0.001483           0.409560   \n",
      "55          55       0.765613      0.001487           0.409535   \n",
      "56          56       0.765691      0.001471           0.409496   \n",
      "57          57       0.765690      0.001448           0.409492   \n",
      "58          58       0.765766      0.001418           0.409454   \n",
      "59          59       0.765835      0.001421           0.409415   \n",
      "60          60       0.765920      0.001449           0.409363   \n",
      "61          61       0.765963      0.001462           0.409338   \n",
      "62          62       0.766033      0.001464           0.409306   \n",
      "63          63       0.766069      0.001468           0.409290   \n",
      "64          64       0.766101      0.001472           0.409271   \n",
      "65          65       0.766151      0.001477           0.409251   \n",
      "66          66       0.766168      0.001458           0.409204   \n",
      "67          67       0.766231      0.001430           0.409173   \n",
      "68          68       0.766275      0.001403           0.409155   \n",
      "69          69       0.766341      0.001438           0.409121   \n",
      "70          70       0.766379      0.001431           0.409104   \n",
      "71          71       0.766408      0.001443           0.409092   \n",
      "72          72       0.766423      0.001426           0.409082   \n",
      "73          73       0.766477      0.001420           0.409018   \n",
      "74          74       0.766570      0.001434           0.408973   \n",
      "75          75       0.766634      0.001382           0.408940   \n",
      "76          76       0.766645      0.001375           0.408937   \n",
      "77          77       0.766685      0.001381           0.408913   \n",
      "78          78       0.766700      0.001385           0.408908   \n",
      "79          79       0.766765      0.001376           0.408871   \n",
      "80          80       0.766810      0.001370           0.408849   \n",
      "81          81       0.766824      0.001391           0.408845   \n",
      "82          82       0.766861      0.001368           0.408821   \n",
      "83          83       0.766896      0.001369           0.408811   \n",
      "84          84       0.766929      0.001337           0.408784   \n",
      "85          85       0.766970      0.001331           0.408767   \n",
      "86          86       0.766986      0.001332           0.408755   \n",
      "87          87       0.766994      0.001360           0.408720   \n",
      "88          88       0.767034      0.001382           0.408699   \n",
      "89          89       0.767026      0.001386           0.408703   \n",
      "90          90       0.767022      0.001391           0.408702   \n",
      "91          91       0.767032      0.001390           0.408698   \n",
      "92          92       0.767052      0.001389           0.408689   \n",
      "93          93       0.767054      0.001410           0.408687   \n",
      "94          94       0.767078      0.001414           0.408676   \n",
      "95          95       0.767103      0.001435           0.408664   \n",
      "96          96       0.767096      0.001429           0.408668   \n",
      "97          97       0.767122      0.001448           0.408657   \n",
      "98          98       0.767136      0.001448           0.408658   \n",
      "99          99       0.767173      0.001471           0.408641   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000377            0.576627           0.000468  \n",
      "1           0.000469            0.512498           0.000784  \n",
      "2           0.000332            0.475080           0.000759  \n",
      "3           0.000440            0.452778           0.000184  \n",
      "4           0.000368            0.439526           0.000193  \n",
      "5           0.000641            0.430727           0.000392  \n",
      "6           0.000211            0.424924           0.000530  \n",
      "7           0.000515            0.420906           0.000341  \n",
      "8           0.000502            0.418120           0.000290  \n",
      "9           0.000598            0.416289           0.000215  \n",
      "10          0.000580            0.414764           0.000272  \n",
      "11          0.000560            0.413575           0.000302  \n",
      "12          0.000575            0.412714           0.000241  \n",
      "13          0.000535            0.411975           0.000321  \n",
      "14          0.000525            0.411329           0.000375  \n",
      "15          0.000562            0.410748           0.000334  \n",
      "16          0.000589            0.410255           0.000311  \n",
      "17          0.000591            0.409849           0.000322  \n",
      "18          0.000626            0.409513           0.000260  \n",
      "19          0.000680            0.409115           0.000256  \n",
      "20          0.000684            0.408749           0.000281  \n",
      "21          0.000702            0.408403           0.000346  \n",
      "22          0.000705            0.408051           0.000315  \n",
      "23          0.000699            0.407778           0.000344  \n",
      "24          0.000735            0.407437           0.000358  \n",
      "25          0.000747            0.407118           0.000352  \n",
      "26          0.000747            0.406873           0.000352  \n",
      "27          0.000765            0.406523           0.000349  \n",
      "28          0.000773            0.406232           0.000406  \n",
      "29          0.000762            0.405979           0.000410  \n",
      "30          0.000759            0.405660           0.000416  \n",
      "31          0.000731            0.405424           0.000433  \n",
      "32          0.000749            0.405234           0.000460  \n",
      "33          0.000758            0.404937           0.000474  \n",
      "34          0.000741            0.404694           0.000554  \n",
      "35          0.000767            0.404400           0.000549  \n",
      "36          0.000751            0.404178           0.000563  \n",
      "37          0.000769            0.403931           0.000511  \n",
      "38          0.000760            0.403712           0.000485  \n",
      "39          0.000760            0.403496           0.000518  \n",
      "40          0.000759            0.403293           0.000467  \n",
      "41          0.000777            0.403058           0.000482  \n",
      "42          0.000766            0.402853           0.000556  \n",
      "43          0.000743            0.402656           0.000528  \n",
      "44          0.000765            0.402506           0.000528  \n",
      "45          0.000761            0.402284           0.000488  \n",
      "46          0.000790            0.402086           0.000519  \n",
      "47          0.000822            0.401847           0.000533  \n",
      "48          0.000825            0.401626           0.000482  \n",
      "49          0.000836            0.401374           0.000496  \n",
      "50          0.000851            0.401137           0.000468  \n",
      "51          0.000878            0.400967           0.000473  \n",
      "52          0.000862            0.400812           0.000473  \n",
      "53          0.000854            0.400607           0.000457  \n",
      "54          0.000867            0.400388           0.000518  \n",
      "55          0.000868            0.400171           0.000479  \n",
      "56          0.000863            0.399924           0.000557  \n",
      "57          0.000859            0.399759           0.000550  \n",
      "58          0.000849            0.399516           0.000511  \n",
      "59          0.000848            0.399347           0.000471  \n",
      "60          0.000852            0.399151           0.000439  \n",
      "61          0.000858            0.398976           0.000417  \n",
      "62          0.000855            0.398793           0.000401  \n",
      "63          0.000854            0.398619           0.000424  \n",
      "64          0.000853            0.398394           0.000453  \n",
      "65          0.000862            0.398151           0.000461  \n",
      "66          0.000813            0.397931           0.000507  \n",
      "67          0.000797            0.397739           0.000580  \n",
      "68          0.000781            0.397510           0.000589  \n",
      "69          0.000794            0.397277           0.000565  \n",
      "70          0.000789            0.397023           0.000528  \n",
      "71          0.000803            0.396816           0.000570  \n",
      "72          0.000793            0.396624           0.000578  \n",
      "73          0.000782            0.396447           0.000532  \n",
      "74          0.000791            0.396266           0.000522  \n",
      "75          0.000765            0.396076           0.000490  \n",
      "76          0.000757            0.395902           0.000451  \n",
      "77          0.000751            0.395676           0.000470  \n",
      "78          0.000758            0.395467           0.000505  \n",
      "79          0.000751            0.395337           0.000496  \n",
      "80          0.000741            0.395135           0.000504  \n",
      "81          0.000742            0.394947           0.000463  \n",
      "82          0.000736            0.394751           0.000484  \n",
      "83          0.000740            0.394511           0.000475  \n",
      "84          0.000727            0.394339           0.000474  \n",
      "85          0.000728            0.394148           0.000427  \n",
      "86          0.000732            0.393953           0.000399  \n",
      "87          0.000790            0.393745           0.000455  \n",
      "88          0.000800            0.393540           0.000470  \n",
      "89          0.000795            0.393345           0.000507  \n",
      "90          0.000801            0.393168           0.000487  \n",
      "91          0.000807            0.392995           0.000473  \n",
      "92          0.000803            0.392775           0.000460  \n",
      "93          0.000816            0.392619           0.000453  \n",
      "94          0.000812            0.392423           0.000437  \n",
      "95          0.000824            0.392233           0.000409  \n",
      "96          0.000817            0.392073           0.000417  \n",
      "97          0.000827            0.391935           0.000401  \n",
      "98          0.000827            0.391768           0.000411  \n",
      "99          0.000843            0.391604           0.000366  \n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7672   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.2      \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7656390593\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7676070887\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7683572364\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.738094      0.000608           0.597700   \n",
      "1            1       0.745281      0.000752           0.538102   \n",
      "2            2       0.746746      0.001035           0.499635   \n",
      "3            3       0.748208      0.001643           0.474240   \n",
      "4            4       0.748445      0.001173           0.457241   \n",
      "5            5       0.750215      0.000980           0.445198   \n",
      "6            6       0.751041      0.001149           0.437114   \n",
      "7            7       0.752192      0.001368           0.431263   \n",
      "8            8       0.753282      0.001298           0.426879   \n",
      "9            9       0.754236      0.001483           0.423647   \n",
      "10          10       0.755054      0.001782           0.421339   \n",
      "11          11       0.755680      0.001690           0.419562   \n",
      "12          12       0.756395      0.001714           0.418226   \n",
      "13          13       0.756859      0.001601           0.417276   \n",
      "14          14       0.757330      0.001583           0.416459   \n",
      "15          15       0.757768      0.001497           0.415711   \n",
      "16          16       0.758105      0.001447           0.415153   \n",
      "17          17       0.758455      0.001529           0.414688   \n",
      "18          18       0.758762      0.001503           0.414297   \n",
      "19          19       0.759124      0.001507           0.413948   \n",
      "20          20       0.759384      0.001498           0.413658   \n",
      "21          21       0.759649      0.001484           0.413429   \n",
      "22          22       0.759915      0.001476           0.413169   \n",
      "23          23       0.760140      0.001458           0.412989   \n",
      "24          24       0.760389      0.001505           0.412806   \n",
      "25          25       0.760630      0.001513           0.412613   \n",
      "26          26       0.760868      0.001549           0.412443   \n",
      "27          27       0.761059      0.001551           0.412292   \n",
      "28          28       0.761247      0.001538           0.412160   \n",
      "29          29       0.761397      0.001505           0.412041   \n",
      "30          30       0.761594      0.001473           0.411927   \n",
      "31          31       0.761760      0.001467           0.411815   \n",
      "32          32       0.761949      0.001482           0.411698   \n",
      "33          33       0.762089      0.001486           0.411602   \n",
      "34          34       0.762240      0.001505           0.411514   \n",
      "35          35       0.762356      0.001513           0.411419   \n",
      "36          36       0.762548      0.001514           0.411312   \n",
      "37          37       0.762686      0.001483           0.411231   \n",
      "38          38       0.762804      0.001504           0.411156   \n",
      "39          39       0.762918      0.001491           0.411080   \n",
      "40          40       0.763014      0.001497           0.411015   \n",
      "41          41       0.763137      0.001496           0.410932   \n",
      "42          42       0.763242      0.001466           0.410876   \n",
      "43          43       0.763353      0.001417           0.410803   \n",
      "44          44       0.763452      0.001439           0.410744   \n",
      "45          45       0.763559      0.001426           0.410681   \n",
      "46          46       0.763675      0.001431           0.410616   \n",
      "47          47       0.763794      0.001461           0.410548   \n",
      "48          48       0.763911      0.001498           0.410480   \n",
      "49          49       0.763989      0.001490           0.410428   \n",
      "50          50       0.764088      0.001488           0.410365   \n",
      "51          51       0.764179      0.001492           0.410308   \n",
      "52          52       0.764248      0.001502           0.410265   \n",
      "53          53       0.764328      0.001502           0.410217   \n",
      "54          54       0.764397      0.001509           0.410173   \n",
      "55          55       0.764479      0.001511           0.410122   \n",
      "56          56       0.764567      0.001480           0.410073   \n",
      "57          57       0.764619      0.001495           0.410041   \n",
      "58          58       0.764695      0.001496           0.409995   \n",
      "59          59       0.764779      0.001491           0.409950   \n",
      "60          60       0.764873      0.001467           0.409901   \n",
      "61          61       0.764925      0.001464           0.409868   \n",
      "62          62       0.765004      0.001457           0.409822   \n",
      "63          63       0.765082      0.001459           0.409778   \n",
      "64          64       0.765139      0.001444           0.409742   \n",
      "65          65       0.765206      0.001412           0.409705   \n",
      "66          66       0.765264      0.001418           0.409668   \n",
      "67          67       0.765324      0.001414           0.409632   \n",
      "68          68       0.765385      0.001430           0.409598   \n",
      "69          69       0.765454      0.001421           0.409557   \n",
      "70          70       0.765535      0.001427           0.409515   \n",
      "71          71       0.765628      0.001392           0.409466   \n",
      "72          72       0.765685      0.001378           0.409428   \n",
      "73          73       0.765746      0.001385           0.409392   \n",
      "74          74       0.765817      0.001406           0.409349   \n",
      "75          75       0.765887      0.001441           0.409306   \n",
      "76          76       0.765934      0.001452           0.409279   \n",
      "77          77       0.765982      0.001454           0.409253   \n",
      "78          78       0.766040      0.001421           0.409219   \n",
      "79          79       0.766100      0.001418           0.409186   \n",
      "80          80       0.766161      0.001424           0.409154   \n",
      "81          81       0.766221      0.001410           0.409120   \n",
      "82          82       0.766270      0.001399           0.409091   \n",
      "83          83       0.766332      0.001404           0.409056   \n",
      "84          84       0.766379      0.001428           0.409025   \n",
      "85          85       0.766466      0.001404           0.408979   \n",
      "86          86       0.766520      0.001413           0.408951   \n",
      "87          87       0.766565      0.001421           0.408920   \n",
      "88          88       0.766613      0.001422           0.408890   \n",
      "89          89       0.766658      0.001420           0.408865   \n",
      "90          90       0.766747      0.001422           0.408814   \n",
      "91          91       0.766791      0.001431           0.408789   \n",
      "92          92       0.766854      0.001405           0.408753   \n",
      "93          93       0.766912      0.001391           0.408720   \n",
      "94          94       0.766968      0.001399           0.408685   \n",
      "95          95       0.767028      0.001408           0.408651   \n",
      "96          96       0.767079      0.001422           0.408624   \n",
      "97          97       0.767113      0.001419           0.408607   \n",
      "98          98       0.767150      0.001412           0.408589   \n",
      "99          99       0.767201      0.001404           0.408560   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000392            0.597616           0.000490  \n",
      "1           0.000477            0.538018           0.000600  \n",
      "2           0.000433            0.499556           0.000762  \n",
      "3           0.000919            0.474171           0.000628  \n",
      "4           0.000124            0.457144           0.000345  \n",
      "5           0.000176            0.445071           0.000384  \n",
      "6           0.000290            0.436929           0.000294  \n",
      "7           0.000377            0.431038           0.000410  \n",
      "8           0.000366            0.426587           0.000394  \n",
      "9           0.000494            0.423310           0.000301  \n",
      "10          0.000525            0.420942           0.000422  \n",
      "11          0.000586            0.419107           0.000306  \n",
      "12          0.000636            0.417730           0.000267  \n",
      "13          0.000559            0.416730           0.000360  \n",
      "14          0.000624            0.415863           0.000307  \n",
      "15          0.000603            0.415077           0.000359  \n",
      "16          0.000654            0.414461           0.000328  \n",
      "17          0.000664            0.413938           0.000343  \n",
      "18          0.000694            0.413510           0.000356  \n",
      "19          0.000751            0.413102           0.000331  \n",
      "20          0.000725            0.412757           0.000376  \n",
      "21          0.000743            0.412476           0.000375  \n",
      "22          0.000758            0.412176           0.000370  \n",
      "23          0.000738            0.411951           0.000379  \n",
      "24          0.000718            0.411721           0.000397  \n",
      "25          0.000745            0.411496           0.000393  \n",
      "26          0.000764            0.411277           0.000385  \n",
      "27          0.000767            0.411075           0.000403  \n",
      "28          0.000786            0.410911           0.000373  \n",
      "29          0.000776            0.410744           0.000378  \n",
      "30          0.000765            0.410576           0.000394  \n",
      "31          0.000757            0.410420           0.000411  \n",
      "32          0.000762            0.410258           0.000406  \n",
      "33          0.000766            0.410121           0.000388  \n",
      "34          0.000781            0.409974           0.000385  \n",
      "35          0.000777            0.409842           0.000398  \n",
      "36          0.000783            0.409695           0.000406  \n",
      "37          0.000769            0.409555           0.000420  \n",
      "38          0.000782            0.409426           0.000415  \n",
      "39          0.000780            0.409293           0.000440  \n",
      "40          0.000785            0.409184           0.000434  \n",
      "41          0.000785            0.409063           0.000429  \n",
      "42          0.000771            0.408952           0.000430  \n",
      "43          0.000754            0.408812           0.000455  \n",
      "44          0.000758            0.408692           0.000461  \n",
      "45          0.000760            0.408568           0.000451  \n",
      "46          0.000774            0.408453           0.000445  \n",
      "47          0.000790            0.408339           0.000433  \n",
      "48          0.000814            0.408211           0.000413  \n",
      "49          0.000809            0.408128           0.000419  \n",
      "50          0.000815            0.408024           0.000431  \n",
      "51          0.000829            0.407917           0.000432  \n",
      "52          0.000833            0.407827           0.000426  \n",
      "53          0.000838            0.407719           0.000433  \n",
      "54          0.000844            0.407620           0.000431  \n",
      "55          0.000843            0.407526           0.000435  \n",
      "56          0.000825            0.407418           0.000443  \n",
      "57          0.000835            0.407341           0.000440  \n",
      "58          0.000826            0.407248           0.000460  \n",
      "59          0.000824            0.407156           0.000467  \n",
      "60          0.000816            0.407063           0.000473  \n",
      "61          0.000817            0.406975           0.000482  \n",
      "62          0.000815            0.406876           0.000479  \n",
      "63          0.000813            0.406785           0.000493  \n",
      "64          0.000802            0.406715           0.000502  \n",
      "65          0.000788            0.406639           0.000514  \n",
      "66          0.000795            0.406555           0.000507  \n",
      "67          0.000797            0.406466           0.000499  \n",
      "68          0.000804            0.406392           0.000489  \n",
      "69          0.000801            0.406294           0.000489  \n",
      "70          0.000800            0.406203           0.000483  \n",
      "71          0.000788            0.406117           0.000498  \n",
      "72          0.000779            0.406039           0.000523  \n",
      "73          0.000786            0.405956           0.000514  \n",
      "74          0.000797            0.405861           0.000511  \n",
      "75          0.000818            0.405783           0.000486  \n",
      "76          0.000827            0.405696           0.000477  \n",
      "77          0.000826            0.405627           0.000465  \n",
      "78          0.000809            0.405544           0.000482  \n",
      "79          0.000799            0.405445           0.000489  \n",
      "80          0.000799            0.405362           0.000493  \n",
      "81          0.000798            0.405281           0.000493  \n",
      "82          0.000793            0.405203           0.000499  \n",
      "83          0.000793            0.405129           0.000487  \n",
      "84          0.000807            0.405055           0.000474  \n",
      "85          0.000793            0.404964           0.000489  \n",
      "86          0.000796            0.404890           0.000497  \n",
      "87          0.000801            0.404813           0.000496  \n",
      "88          0.000799            0.404721           0.000501  \n",
      "89          0.000792            0.404636           0.000505  \n",
      "90          0.000798            0.404540           0.000506  \n",
      "91          0.000802            0.404475           0.000501  \n",
      "92          0.000789            0.404398           0.000516  \n",
      "93          0.000778            0.404323           0.000532  \n",
      "94          0.000782            0.404248           0.000535  \n",
      "95          0.000790            0.404161           0.000508  \n",
      "96          0.000802            0.404084           0.000503  \n",
      "97          0.000804            0.404003           0.000504  \n",
      "98          0.000798            0.403931           0.000516  \n",
      "99          0.000792            0.403842           0.000515  \n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7672   \u001b[0m | \u001b[0m9.983    \u001b[0m | \u001b[0m8.436    \u001b[0m | \u001b[0m0.1577   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7641465872\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7661725075\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7672187996\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740342      0.000950           0.629466   \n",
      "1            1       0.745957      0.000900           0.581898   \n",
      "2            2       0.748516      0.000896           0.545215   \n",
      "3            3       0.748830      0.001320           0.517556   \n",
      "4            4       0.749213      0.001823           0.497202   \n",
      "5            5       0.750303      0.001447           0.481134   \n",
      "6            6       0.751086      0.001421           0.468206   \n",
      "7            7       0.751603      0.001385           0.457789   \n",
      "8            8       0.752252      0.001590           0.449648   \n",
      "9            9       0.753248      0.001456           0.442911   \n",
      "10          10       0.753765      0.001669           0.437856   \n",
      "11          11       0.754225      0.001420           0.433962   \n",
      "12          12       0.754470      0.001282           0.430512   \n",
      "13          13       0.755073      0.001260           0.427687   \n",
      "14          14       0.755506      0.001313           0.425451   \n",
      "15          15       0.756011      0.001319           0.423464   \n",
      "16          16       0.756466      0.001388           0.421737   \n",
      "17          17       0.756907      0.001468           0.420394   \n",
      "18          18       0.757207      0.001401           0.419293   \n",
      "19          19       0.757396      0.001451           0.418344   \n",
      "20          20       0.757688      0.001399           0.417538   \n",
      "21          21       0.757990      0.001260           0.416853   \n",
      "22          22       0.758300      0.001340           0.416291   \n",
      "23          23       0.758529      0.001370           0.415742   \n",
      "24          24       0.758699      0.001304           0.415341   \n",
      "25          25       0.758890      0.001361           0.414978   \n",
      "26          26       0.759153      0.001434           0.414628   \n",
      "27          27       0.759442      0.001441           0.414332   \n",
      "28          28       0.759641      0.001369           0.414053   \n",
      "29          29       0.759833      0.001416           0.413810   \n",
      "30          30       0.759989      0.001413           0.413609   \n",
      "31          31       0.760171      0.001349           0.413403   \n",
      "32          32       0.760349      0.001364           0.413225   \n",
      "33          33       0.760515      0.001358           0.413036   \n",
      "34          34       0.760701      0.001419           0.412876   \n",
      "35          35       0.760805      0.001407           0.412744   \n",
      "36          36       0.760928      0.001431           0.412630   \n",
      "37          37       0.761098      0.001402           0.412488   \n",
      "38          38       0.761231      0.001416           0.412364   \n",
      "39          39       0.761378      0.001400           0.412257   \n",
      "40          40       0.761496      0.001419           0.412163   \n",
      "41          41       0.761634      0.001400           0.412053   \n",
      "42          42       0.761749      0.001397           0.411957   \n",
      "43          43       0.761897      0.001394           0.411866   \n",
      "44          44       0.761982      0.001420           0.411777   \n",
      "45          45       0.762112      0.001405           0.411693   \n",
      "46          46       0.762210      0.001419           0.411621   \n",
      "47          47       0.762310      0.001421           0.411560   \n",
      "48          48       0.762409      0.001426           0.411481   \n",
      "49          49       0.762498      0.001422           0.411420   \n",
      "50          50       0.762589      0.001403           0.411349   \n",
      "51          51       0.762677      0.001400           0.411294   \n",
      "52          52       0.762762      0.001393           0.411240   \n",
      "53          53       0.762862      0.001414           0.411175   \n",
      "54          54       0.762969      0.001416           0.411110   \n",
      "55          55       0.763059      0.001428           0.411033   \n",
      "56          56       0.763150      0.001440           0.410974   \n",
      "57          57       0.763254      0.001433           0.410912   \n",
      "58          58       0.763371      0.001454           0.410845   \n",
      "59          59       0.763467      0.001465           0.410790   \n",
      "60          60       0.763525      0.001467           0.410751   \n",
      "61          61       0.763606      0.001464           0.410704   \n",
      "62          62       0.763708      0.001470           0.410651   \n",
      "63          63       0.763777      0.001475           0.410605   \n",
      "64          64       0.763864      0.001464           0.410552   \n",
      "65          65       0.763952      0.001445           0.410498   \n",
      "66          66       0.764026      0.001441           0.410448   \n",
      "67          67       0.764114      0.001446           0.410400   \n",
      "68          68       0.764189      0.001442           0.410354   \n",
      "69          69       0.764259      0.001480           0.410309   \n",
      "70          70       0.764331      0.001475           0.410267   \n",
      "71          71       0.764394      0.001477           0.410225   \n",
      "72          72       0.764468      0.001463           0.410182   \n",
      "73          73       0.764514      0.001461           0.410151   \n",
      "74          74       0.764579      0.001453           0.410114   \n",
      "75          75       0.764615      0.001448           0.410089   \n",
      "76          76       0.764689      0.001456           0.410048   \n",
      "77          77       0.764739      0.001440           0.410015   \n",
      "78          78       0.764802      0.001451           0.409979   \n",
      "79          79       0.764869      0.001466           0.409938   \n",
      "80          80       0.764944      0.001475           0.409902   \n",
      "81          81       0.765002      0.001486           0.409869   \n",
      "82          82       0.765054      0.001504           0.409835   \n",
      "83          83       0.765106      0.001504           0.409805   \n",
      "84          84       0.765157      0.001493           0.409775   \n",
      "85          85       0.765223      0.001487           0.409739   \n",
      "86          86       0.765269      0.001495           0.409712   \n",
      "87          87       0.765321      0.001487           0.409682   \n",
      "88          88       0.765361      0.001475           0.409659   \n",
      "89          89       0.765418      0.001485           0.409628   \n",
      "90          90       0.765447      0.001497           0.409601   \n",
      "91          91       0.765503      0.001507           0.409567   \n",
      "92          92       0.765551      0.001511           0.409539   \n",
      "93          93       0.765599      0.001515           0.409510   \n",
      "94          94       0.765644      0.001530           0.409460   \n",
      "95          95       0.765685      0.001537           0.409431   \n",
      "96          96       0.765731      0.001558           0.409406   \n",
      "97          97       0.765772      0.001562           0.409385   \n",
      "98          98       0.765819      0.001569           0.409357   \n",
      "99          99       0.765846      0.001562           0.409336   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000263            0.629389           0.000278  \n",
      "1           0.000234            0.581788           0.000301  \n",
      "2           0.000129            0.545095           0.000324  \n",
      "3           0.000211            0.517426           0.000225  \n",
      "4           0.000570            0.497065           0.000421  \n",
      "5           0.000272            0.480979           0.000193  \n",
      "6           0.000533            0.468021           0.000196  \n",
      "7           0.000490            0.457564           0.000162  \n",
      "8           0.000674            0.449401           0.000049  \n",
      "9           0.000501            0.442619           0.000149  \n",
      "10          0.000601            0.437542           0.000335  \n",
      "11          0.000575            0.433596           0.000306  \n",
      "12          0.000509            0.430111           0.000259  \n",
      "13          0.000476            0.427232           0.000325  \n",
      "14          0.000641            0.424965           0.000379  \n",
      "15          0.000522            0.422951           0.000334  \n",
      "16          0.000606            0.421171           0.000293  \n",
      "17          0.000515            0.419794           0.000383  \n",
      "18          0.000537            0.418622           0.000368  \n",
      "19          0.000611            0.417639           0.000337  \n",
      "20          0.000583            0.416778           0.000392  \n",
      "21          0.000621            0.416031           0.000387  \n",
      "22          0.000639            0.415406           0.000381  \n",
      "23          0.000659            0.414808           0.000379  \n",
      "24          0.000651            0.414359           0.000405  \n",
      "25          0.000649            0.413940           0.000414  \n",
      "26          0.000681            0.413541           0.000375  \n",
      "27          0.000663            0.413193           0.000404  \n",
      "28          0.000669            0.412878           0.000417  \n",
      "29          0.000693            0.412587           0.000411  \n",
      "30          0.000678            0.412354           0.000439  \n",
      "31          0.000670            0.412087           0.000439  \n",
      "32          0.000673            0.411854           0.000455  \n",
      "33          0.000685            0.411610           0.000467  \n",
      "34          0.000707            0.411390           0.000458  \n",
      "35          0.000712            0.411205           0.000468  \n",
      "36          0.000721            0.411015           0.000465  \n",
      "37          0.000730            0.410823           0.000485  \n",
      "38          0.000738            0.410646           0.000475  \n",
      "39          0.000748            0.410490           0.000458  \n",
      "40          0.000759            0.410335           0.000445  \n",
      "41          0.000757            0.410174           0.000432  \n",
      "42          0.000762            0.410023           0.000430  \n",
      "43          0.000769            0.409878           0.000423  \n",
      "44          0.000780            0.409759           0.000410  \n",
      "45          0.000776            0.409616           0.000426  \n",
      "46          0.000777            0.409477           0.000424  \n",
      "47          0.000778            0.409353           0.000416  \n",
      "48          0.000769            0.409225           0.000415  \n",
      "49          0.000770            0.409104           0.000416  \n",
      "50          0.000764            0.408998           0.000427  \n",
      "51          0.000763            0.408885           0.000446  \n",
      "52          0.000762            0.408779           0.000449  \n",
      "53          0.000770            0.408659           0.000447  \n",
      "54          0.000769            0.408539           0.000455  \n",
      "55          0.000777            0.408414           0.000460  \n",
      "56          0.000779            0.408316           0.000463  \n",
      "57          0.000782            0.408217           0.000460  \n",
      "58          0.000796            0.408107           0.000454  \n",
      "59          0.000805            0.407996           0.000448  \n",
      "60          0.000804            0.407904           0.000464  \n",
      "61          0.000806            0.407821           0.000480  \n",
      "62          0.000809            0.407729           0.000477  \n",
      "63          0.000815            0.407638           0.000471  \n",
      "64          0.000816            0.407519           0.000477  \n",
      "65          0.000809            0.407400           0.000488  \n",
      "66          0.000802            0.407300           0.000506  \n",
      "67          0.000809            0.407192           0.000501  \n",
      "68          0.000810            0.407085           0.000494  \n",
      "69          0.000830            0.407001           0.000479  \n",
      "70          0.000829            0.406910           0.000481  \n",
      "71          0.000826            0.406801           0.000480  \n",
      "72          0.000821            0.406697           0.000494  \n",
      "73          0.000818            0.406629           0.000494  \n",
      "74          0.000811            0.406555           0.000489  \n",
      "75          0.000805            0.406467           0.000506  \n",
      "76          0.000806            0.406372           0.000509  \n",
      "77          0.000800            0.406301           0.000495  \n",
      "78          0.000805            0.406214           0.000480  \n",
      "79          0.000812            0.406110           0.000481  \n",
      "80          0.000815            0.406038           0.000476  \n",
      "81          0.000818            0.405963           0.000481  \n",
      "82          0.000828            0.405863           0.000469  \n",
      "83          0.000827            0.405773           0.000469  \n",
      "84          0.000821            0.405688           0.000474  \n",
      "85          0.000823            0.405618           0.000464  \n",
      "86          0.000829            0.405532           0.000447  \n",
      "87          0.000826            0.405455           0.000467  \n",
      "88          0.000821            0.405395           0.000476  \n",
      "89          0.000828            0.405316           0.000477  \n",
      "90          0.000833            0.405230           0.000482  \n",
      "91          0.000839            0.405155           0.000454  \n",
      "92          0.000840            0.405087           0.000459  \n",
      "93          0.000845            0.405000           0.000469  \n",
      "94          0.000822            0.404921           0.000481  \n",
      "95          0.000827            0.404843           0.000487  \n",
      "96          0.000840            0.404764           0.000478  \n",
      "97          0.000840            0.404697           0.000484  \n",
      "98          0.000844            0.404619           0.000470  \n",
      "99          0.000845            0.404554           0.000485  \n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.7658   \u001b[0m | \u001b[0m8.59     \u001b[0m | \u001b[0m8.773    \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7655370084\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7671426437\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7687734449\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.740796      0.000126           0.595385   \n",
      "1            1       0.745290      0.001185           0.534508   \n",
      "2            2       0.749966      0.001362           0.495370   \n",
      "3            3       0.751483      0.001155           0.469738   \n",
      "4            4       0.752302      0.000820           0.453191   \n",
      "5            5       0.753272      0.000947           0.442371   \n",
      "6            6       0.753871      0.000828           0.434360   \n",
      "7            7       0.754548      0.001026           0.428946   \n",
      "8            8       0.755576      0.001376           0.424742   \n",
      "9            9       0.756408      0.001295           0.421839   \n",
      "10          10       0.756701      0.001377           0.419835   \n",
      "11          11       0.757279      0.001419           0.418099   \n",
      "12          12       0.757790      0.001294           0.416934   \n",
      "13          13       0.758289      0.001241           0.415935   \n",
      "14          14       0.758794      0.001382           0.415120   \n",
      "15          15       0.759240      0.001347           0.414480   \n",
      "16          16       0.759625      0.001364           0.414032   \n",
      "17          17       0.759958      0.001380           0.413664   \n",
      "18          18       0.760191      0.001364           0.413357   \n",
      "19          19       0.760461      0.001424           0.413040   \n",
      "20          20       0.760762      0.001424           0.412784   \n",
      "21          21       0.761001      0.001385           0.412549   \n",
      "22          22       0.761206      0.001343           0.412375   \n",
      "23          23       0.761460      0.001325           0.412193   \n",
      "24          24       0.761706      0.001357           0.412007   \n",
      "25          25       0.761951      0.001325           0.411859   \n",
      "26          26       0.762092      0.001362           0.411715   \n",
      "27          27       0.762261      0.001374           0.411594   \n",
      "28          28       0.762451      0.001364           0.411471   \n",
      "29          29       0.762575      0.001370           0.411385   \n",
      "30          30       0.762728      0.001348           0.411287   \n",
      "31          31       0.762880      0.001371           0.411188   \n",
      "32          32       0.763017      0.001357           0.411102   \n",
      "33          33       0.763132      0.001348           0.411030   \n",
      "34          34       0.763216      0.001348           0.410957   \n",
      "35          35       0.763344      0.001338           0.410872   \n",
      "36          36       0.763497      0.001330           0.410785   \n",
      "37          37       0.763601      0.001329           0.410710   \n",
      "38          38       0.763706      0.001341           0.410649   \n",
      "39          39       0.763828      0.001349           0.410586   \n",
      "40          40       0.763943      0.001359           0.410513   \n",
      "41          41       0.764049      0.001361           0.410456   \n",
      "42          42       0.764150      0.001403           0.410400   \n",
      "43          43       0.764250      0.001437           0.410335   \n",
      "44          44       0.764333      0.001437           0.410286   \n",
      "45          45       0.764431      0.001412           0.410234   \n",
      "46          46       0.764510      0.001415           0.410193   \n",
      "47          47       0.764622      0.001392           0.410128   \n",
      "48          48       0.764687      0.001383           0.410086   \n",
      "49          49       0.764783      0.001409           0.410034   \n",
      "50          50       0.764837      0.001438           0.410002   \n",
      "51          51       0.764902      0.001453           0.409956   \n",
      "52          52       0.765010      0.001474           0.409894   \n",
      "53          53       0.765069      0.001477           0.409867   \n",
      "54          54       0.765109      0.001484           0.409845   \n",
      "55          55       0.765162      0.001503           0.409811   \n",
      "56          56       0.765257      0.001535           0.409756   \n",
      "57          57       0.765306      0.001532           0.409721   \n",
      "58          58       0.765391      0.001522           0.409675   \n",
      "59          59       0.765468      0.001496           0.409635   \n",
      "60          60       0.765564      0.001518           0.409587   \n",
      "61          61       0.765616      0.001541           0.409554   \n",
      "62          62       0.765653      0.001540           0.409536   \n",
      "63          63       0.765707      0.001547           0.409502   \n",
      "64          64       0.765747      0.001554           0.409480   \n",
      "65          65       0.765818      0.001568           0.409438   \n",
      "66          66       0.765888      0.001585           0.409337   \n",
      "67          67       0.765948      0.001592           0.409306   \n",
      "68          68       0.765979      0.001593           0.409287   \n",
      "69          69       0.766042      0.001563           0.409252   \n",
      "70          70       0.766072      0.001560           0.409229   \n",
      "71          71       0.766101      0.001560           0.409214   \n",
      "72          72       0.766137      0.001568           0.409194   \n",
      "73          73       0.766178      0.001539           0.409133   \n",
      "74          74       0.766230      0.001548           0.409103   \n",
      "75          75       0.766261      0.001540           0.409083   \n",
      "76          76       0.766288      0.001525           0.409062   \n",
      "77          77       0.766320      0.001540           0.409046   \n",
      "78          78       0.766367      0.001553           0.409023   \n",
      "79          79       0.766394      0.001563           0.409012   \n",
      "80          80       0.766431      0.001563           0.408987   \n",
      "81          81       0.766467      0.001564           0.408968   \n",
      "82          82       0.766514      0.001560           0.408943   \n",
      "83          83       0.766558      0.001548           0.408917   \n",
      "84          84       0.766601      0.001551           0.408896   \n",
      "85          85       0.766658      0.001562           0.408869   \n",
      "86          86       0.766700      0.001574           0.408852   \n",
      "87          87       0.766707      0.001565           0.408850   \n",
      "88          88       0.766728      0.001567           0.408841   \n",
      "89          89       0.766758      0.001581           0.408826   \n",
      "90          90       0.766819      0.001570           0.408796   \n",
      "91          91       0.766852      0.001593           0.408778   \n",
      "92          92       0.766885      0.001633           0.408759   \n",
      "93          93       0.766935      0.001672           0.408731   \n",
      "94          94       0.766978      0.001652           0.408712   \n",
      "95          95       0.767009      0.001660           0.408694   \n",
      "96          96       0.767058      0.001625           0.408669   \n",
      "97          97       0.767083      0.001650           0.408655   \n",
      "98          98       0.767102      0.001628           0.408649   \n",
      "99          99       0.767151      0.001618           0.408626   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.000342            0.595275           0.000402  \n",
      "1           0.000754            0.534345           0.000539  \n",
      "2           0.000524            0.495117           0.000445  \n",
      "3           0.000407            0.469388           0.000057  \n",
      "4           0.000264            0.452794           0.000245  \n",
      "5           0.000188            0.441895           0.000582  \n",
      "6           0.000234            0.433791           0.000632  \n",
      "7           0.000236            0.428222           0.000604  \n",
      "8           0.000233            0.423866           0.000496  \n",
      "9           0.000335            0.420869           0.000509  \n",
      "10          0.000360            0.418747           0.000508  \n",
      "11          0.000440            0.416869           0.000424  \n",
      "12          0.000499            0.415541           0.000386  \n",
      "13          0.000466            0.414409           0.000419  \n",
      "14          0.000577            0.413401           0.000352  \n",
      "15          0.000560            0.412642           0.000407  \n",
      "16          0.000577            0.412069           0.000386  \n",
      "17          0.000561            0.411548           0.000421  \n",
      "18          0.000597            0.411139           0.000389  \n",
      "19          0.000636            0.410683           0.000390  \n",
      "20          0.000660            0.410202           0.000403  \n",
      "21          0.000662            0.409844           0.000398  \n",
      "22          0.000647            0.409484           0.000426  \n",
      "23          0.000649            0.409213           0.000451  \n",
      "24          0.000656            0.408861           0.000493  \n",
      "25          0.000642            0.408536           0.000508  \n",
      "26          0.000661            0.408255           0.000480  \n",
      "27          0.000679            0.407980           0.000507  \n",
      "28          0.000687            0.407670           0.000501  \n",
      "29          0.000695            0.407414           0.000497  \n",
      "30          0.000695            0.407191           0.000491  \n",
      "31          0.000706            0.406991           0.000483  \n",
      "32          0.000712            0.406760           0.000487  \n",
      "33          0.000714            0.406493           0.000474  \n",
      "34          0.000714            0.406319           0.000431  \n",
      "35          0.000725            0.406032           0.000435  \n",
      "36          0.000721            0.405797           0.000420  \n",
      "37          0.000725            0.405554           0.000439  \n",
      "38          0.000729            0.405306           0.000475  \n",
      "39          0.000734            0.405106           0.000463  \n",
      "40          0.000736            0.404949           0.000453  \n",
      "41          0.000734            0.404719           0.000433  \n",
      "42          0.000758            0.404487           0.000445  \n",
      "43          0.000769            0.404329           0.000440  \n",
      "44          0.000776            0.404179           0.000448  \n",
      "45          0.000764            0.404038           0.000434  \n",
      "46          0.000761            0.403884           0.000472  \n",
      "47          0.000757            0.403675           0.000483  \n",
      "48          0.000750            0.403468           0.000440  \n",
      "49          0.000765            0.403263           0.000408  \n",
      "50          0.000778            0.403110           0.000416  \n",
      "51          0.000793            0.402917           0.000417  \n",
      "52          0.000800            0.402735           0.000379  \n",
      "53          0.000795            0.402543           0.000403  \n",
      "54          0.000795            0.402333           0.000411  \n",
      "55          0.000800            0.402173           0.000437  \n",
      "56          0.000823            0.401991           0.000471  \n",
      "57          0.000831            0.401811           0.000489  \n",
      "58          0.000830            0.401645           0.000492  \n",
      "59          0.000824            0.401476           0.000501  \n",
      "60          0.000843            0.401242           0.000480  \n",
      "61          0.000859            0.401057           0.000517  \n",
      "62          0.000858            0.400868           0.000518  \n",
      "63          0.000864            0.400727           0.000498  \n",
      "64          0.000876            0.400530           0.000501  \n",
      "65          0.000887            0.400378           0.000487  \n",
      "66          0.000943            0.400188           0.000465  \n",
      "67          0.000947            0.400011           0.000470  \n",
      "68          0.000946            0.399854           0.000466  \n",
      "69          0.000930            0.399667           0.000437  \n",
      "70          0.000929            0.399498           0.000390  \n",
      "71          0.000933            0.399348           0.000381  \n",
      "72          0.000935            0.399207           0.000392  \n",
      "73          0.000891            0.399042           0.000421  \n",
      "74          0.000895            0.398856           0.000441  \n",
      "75          0.000893            0.398725           0.000427  \n",
      "76          0.000889            0.398557           0.000486  \n",
      "77          0.000899            0.398377           0.000488  \n",
      "78          0.000909            0.398221           0.000489  \n",
      "79          0.000913            0.398065           0.000492  \n",
      "80          0.000914            0.397935           0.000515  \n",
      "81          0.000914            0.397799           0.000549  \n",
      "82          0.000912            0.397664           0.000526  \n",
      "83          0.000909            0.397477           0.000506  \n",
      "84          0.000912            0.397336           0.000546  \n",
      "85          0.000916            0.397114           0.000514  \n",
      "86          0.000924            0.396919           0.000538  \n",
      "87          0.000917            0.396756           0.000522  \n",
      "88          0.000917            0.396613           0.000530  \n",
      "89          0.000921            0.396453           0.000571  \n",
      "90          0.000915            0.396299           0.000580  \n",
      "91          0.000924            0.396158           0.000582  \n",
      "92          0.000949            0.396009           0.000564  \n",
      "93          0.000968            0.395818           0.000590  \n",
      "94          0.000959            0.395683           0.000590  \n",
      "95          0.000967            0.395498           0.000549  \n",
      "96          0.000950            0.395281           0.000486  \n",
      "97          0.000960            0.395105           0.000470  \n",
      "98          0.000945            0.394911           0.000510  \n",
      "99          0.000934            0.394735           0.000454  \n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.7672   \u001b[0m | \u001b[0m0.1058   \u001b[0m | \u001b[0m9.927    \u001b[0m | \u001b[0m0.1624   \u001b[0m |\n",
      "Training on fold [0/3]\n",
      "\n",
      "bestTest = 0.7631051543\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [1/3]\n",
      "\n",
      "bestTest = 0.7648492092\n",
      "bestIteration = 99\n",
      "\n",
      "Training on fold [2/3]\n",
      "\n",
      "bestTest = 0.7660053431\n",
      "bestIteration = 99\n",
      "\n",
      "    iterations  test-AUC-mean  test-AUC-std  test-Logloss-mean  \\\n",
      "0            0       0.715524      0.007015           0.581043   \n",
      "1            1       0.727186      0.001714           0.517966   \n",
      "2            2       0.734646      0.002829           0.481958   \n",
      "3            3       0.739277      0.003036           0.459984   \n",
      "4            4       0.742708      0.001584           0.446013   \n",
      "5            5       0.745194      0.002168           0.436853   \n",
      "6            6       0.746645      0.002015           0.431122   \n",
      "7            7       0.748123      0.001658           0.427318   \n",
      "8            8       0.749401      0.001609           0.424463   \n",
      "9            9       0.750417      0.001628           0.422494   \n",
      "10          10       0.751338      0.001499           0.420991   \n",
      "11          11       0.752141      0.001432           0.419978   \n",
      "12          12       0.752718      0.001395           0.419102   \n",
      "13          13       0.753383      0.001609           0.418413   \n",
      "14          14       0.753835      0.001569           0.417875   \n",
      "15          15       0.754268      0.001483           0.417386   \n",
      "16          16       0.754834      0.001589           0.416919   \n",
      "17          17       0.755195      0.001551           0.416600   \n",
      "18          18       0.755535      0.001454           0.416254   \n",
      "19          19       0.755865      0.001393           0.416005   \n",
      "20          20       0.756117      0.001351           0.415590   \n",
      "21          21       0.756361      0.001355           0.415353   \n",
      "22          22       0.756590      0.001358           0.415156   \n",
      "23          23       0.756845      0.001406           0.414970   \n",
      "24          24       0.757118      0.001348           0.414788   \n",
      "25          25       0.757377      0.001384           0.414603   \n",
      "26          26       0.757600      0.001376           0.414464   \n",
      "27          27       0.757816      0.001378           0.414324   \n",
      "28          28       0.758007      0.001419           0.414186   \n",
      "29          29       0.758171      0.001415           0.414084   \n",
      "30          30       0.758315      0.001326           0.413964   \n",
      "31          31       0.758480      0.001328           0.413859   \n",
      "32          32       0.758663      0.001348           0.413756   \n",
      "33          33       0.758814      0.001383           0.413655   \n",
      "34          34       0.758961      0.001401           0.413562   \n",
      "35          35       0.759093      0.001415           0.413480   \n",
      "36          36       0.759240      0.001386           0.413379   \n",
      "37          37       0.759353      0.001361           0.413312   \n",
      "38          38       0.759459      0.001331           0.413243   \n",
      "39          39       0.759613      0.001372           0.413142   \n",
      "40          40       0.759770      0.001392           0.413053   \n",
      "41          41       0.759887      0.001423           0.412987   \n",
      "42          42       0.760035      0.001457           0.412914   \n",
      "43          43       0.760149      0.001462           0.412837   \n",
      "44          44       0.760244      0.001449           0.412763   \n",
      "45          45       0.760347      0.001441           0.412696   \n",
      "46          46       0.760459      0.001410           0.412638   \n",
      "47          47       0.760557      0.001440           0.412582   \n",
      "48          48       0.760638      0.001456           0.412529   \n",
      "49          49       0.760760      0.001448           0.412459   \n",
      "50          50       0.760843      0.001421           0.412409   \n",
      "51          51       0.760927      0.001422           0.412358   \n",
      "52          52       0.760995      0.001434           0.412303   \n",
      "53          53       0.761093      0.001421           0.412249   \n",
      "54          54       0.761188      0.001414           0.412193   \n",
      "55          55       0.761319      0.001424           0.412120   \n",
      "56          56       0.761403      0.001442           0.412070   \n",
      "57          57       0.761489      0.001457           0.412022   \n",
      "58          58       0.761616      0.001480           0.411956   \n",
      "59          59       0.761742      0.001480           0.411887   \n",
      "60          60       0.761839      0.001523           0.411826   \n",
      "61          61       0.761918      0.001529           0.411780   \n",
      "62          62       0.762008      0.001490           0.411735   \n",
      "63          63       0.762106      0.001484           0.411681   \n",
      "64          64       0.762162      0.001472           0.411576   \n",
      "65          65       0.762257      0.001475           0.411520   \n",
      "66          66       0.762348      0.001496           0.411472   \n",
      "67          67       0.762417      0.001530           0.411430   \n",
      "68          68       0.762500      0.001524           0.411377   \n",
      "69          69       0.762562      0.001504           0.411339   \n",
      "70          70       0.762629      0.001530           0.411302   \n",
      "71          71       0.762689      0.001547           0.411187   \n",
      "72          72       0.762738      0.001560           0.411076   \n",
      "73          73       0.762802      0.001561           0.411035   \n",
      "74          74       0.762915      0.001581           0.410978   \n",
      "75          75       0.762957      0.001571           0.410945   \n",
      "76          76       0.763051      0.001539           0.410888   \n",
      "77          77       0.763146      0.001534           0.410835   \n",
      "78          78       0.763250      0.001511           0.410784   \n",
      "79          79       0.763344      0.001505           0.410737   \n",
      "80          80       0.763403      0.001502           0.410688   \n",
      "81          81       0.763473      0.001496           0.410652   \n",
      "82          82       0.763558      0.001471           0.410602   \n",
      "83          83       0.763627      0.001445           0.410561   \n",
      "84          84       0.763725      0.001435           0.410512   \n",
      "85          85       0.763799      0.001470           0.410464   \n",
      "86          86       0.763874      0.001477           0.410420   \n",
      "87          87       0.763931      0.001474           0.410383   \n",
      "88          88       0.764005      0.001462           0.410343   \n",
      "89          89       0.764044      0.001449           0.410318   \n",
      "90          90       0.764119      0.001455           0.410282   \n",
      "91          91       0.764196      0.001444           0.410240   \n",
      "92          92       0.764245      0.001444           0.410215   \n",
      "93          93       0.764317      0.001436           0.410176   \n",
      "94          94       0.764370      0.001454           0.410146   \n",
      "95          95       0.764437      0.001472           0.410108   \n",
      "96          96       0.764503      0.001473           0.410073   \n",
      "97          97       0.764559      0.001459           0.410038   \n",
      "98          98       0.764622      0.001454           0.410006   \n",
      "99          99       0.764653      0.001460           0.409982   \n",
      "\n",
      "    test-Logloss-std  train-Logloss-mean  train-Logloss-std  \n",
      "0           0.001567            0.580978           0.001327  \n",
      "1           0.001716            0.517935           0.001298  \n",
      "2           0.001574            0.481905           0.001313  \n",
      "3           0.000671            0.459920           0.000180  \n",
      "4           0.000807            0.445950           0.000674  \n",
      "5           0.000626            0.436764           0.000049  \n",
      "6           0.000667            0.431019           0.000421  \n",
      "7           0.000305            0.427201           0.000485  \n",
      "8           0.000251            0.424316           0.000534  \n",
      "9           0.000333            0.422341           0.000496  \n",
      "10          0.000311            0.420841           0.000627  \n",
      "11          0.000270            0.419836           0.000606  \n",
      "12          0.000314            0.418941           0.000645  \n",
      "13          0.000463            0.418236           0.000461  \n",
      "14          0.000397            0.417690           0.000534  \n",
      "15          0.000439            0.417193           0.000507  \n",
      "16          0.000423            0.416724           0.000543  \n",
      "17          0.000417            0.416390           0.000574  \n",
      "18          0.000391            0.416041           0.000621  \n",
      "19          0.000369            0.415787           0.000639  \n",
      "20          0.000670            0.415362           0.000363  \n",
      "21          0.000714            0.415115           0.000322  \n",
      "22          0.000727            0.414914           0.000317  \n",
      "23          0.000764            0.414727           0.000305  \n",
      "24          0.000739            0.414532           0.000337  \n",
      "25          0.000728            0.414336           0.000371  \n",
      "26          0.000745            0.414192           0.000355  \n",
      "27          0.000752            0.414048           0.000362  \n",
      "28          0.000765            0.413909           0.000353  \n",
      "29          0.000757            0.413794           0.000368  \n",
      "30          0.000760            0.413662           0.000359  \n",
      "31          0.000764            0.413553           0.000365  \n",
      "32          0.000768            0.413438           0.000361  \n",
      "33          0.000770            0.413325           0.000360  \n",
      "34          0.000778            0.413227           0.000363  \n",
      "35          0.000787            0.413135           0.000363  \n",
      "36          0.000768            0.413027           0.000383  \n",
      "37          0.000758            0.412944           0.000385  \n",
      "38          0.000753            0.412860           0.000399  \n",
      "39          0.000752            0.412754           0.000399  \n",
      "40          0.000758            0.412663           0.000396  \n",
      "41          0.000766            0.412581           0.000399  \n",
      "42          0.000774            0.412501           0.000384  \n",
      "43          0.000790            0.412423           0.000376  \n",
      "44          0.000783            0.412342           0.000382  \n",
      "45          0.000774            0.412268           0.000389  \n",
      "46          0.000771            0.412205           0.000388  \n",
      "47          0.000779            0.412138           0.000382  \n",
      "48          0.000798            0.412071           0.000360  \n",
      "49          0.000808            0.411999           0.000355  \n",
      "50          0.000806            0.411942           0.000355  \n",
      "51          0.000810            0.411882           0.000356  \n",
      "52          0.000807            0.411822           0.000368  \n",
      "53          0.000802            0.411764           0.000375  \n",
      "54          0.000794            0.411704           0.000387  \n",
      "55          0.000803            0.411629           0.000384  \n",
      "56          0.000811            0.411575           0.000376  \n",
      "57          0.000814            0.411520           0.000378  \n",
      "58          0.000828            0.411447           0.000367  \n",
      "59          0.000830            0.411375           0.000368  \n",
      "60          0.000857            0.411300           0.000347  \n",
      "61          0.000854            0.411246           0.000359  \n",
      "62          0.000842            0.411192           0.000370  \n",
      "63          0.000848            0.411131           0.000357  \n",
      "64          0.000841            0.411015           0.000406  \n",
      "65          0.000847            0.410952           0.000398  \n",
      "66          0.000855            0.410905           0.000396  \n",
      "67          0.000854            0.410860           0.000391  \n",
      "68          0.000846            0.410803           0.000403  \n",
      "69          0.000849            0.410759           0.000397  \n",
      "70          0.000854            0.410709           0.000396  \n",
      "71          0.000741            0.410582           0.000480  \n",
      "72          0.000863            0.410467           0.000356  \n",
      "73          0.000869            0.410407           0.000353  \n",
      "74          0.000878            0.410346           0.000349  \n",
      "75          0.000870            0.410300           0.000346  \n",
      "76          0.000850            0.410237           0.000362  \n",
      "77          0.000847            0.410179           0.000365  \n",
      "78          0.000836            0.410113           0.000374  \n",
      "79          0.000830            0.410058           0.000382  \n",
      "80          0.000834            0.410000           0.000389  \n",
      "81          0.000832            0.409954           0.000396  \n",
      "82          0.000826            0.409893           0.000412  \n",
      "83          0.000815            0.409840           0.000419  \n",
      "84          0.000824            0.409786           0.000413  \n",
      "85          0.000830            0.409730           0.000402  \n",
      "86          0.000832            0.409678           0.000401  \n",
      "87          0.000830            0.409632           0.000407  \n",
      "88          0.000826            0.409580           0.000411  \n",
      "89          0.000823            0.409542           0.000415  \n",
      "90          0.000825            0.409499           0.000411  \n",
      "91          0.000821            0.409451           0.000405  \n",
      "92          0.000818            0.409417           0.000403  \n",
      "93          0.000820            0.409374           0.000408  \n",
      "94          0.000831            0.409332           0.000405  \n",
      "95          0.000838            0.409279           0.000395  \n",
      "96          0.000840            0.409231           0.000397  \n",
      "97          0.000833            0.409189           0.000402  \n",
      "98          0.000830            0.409143           0.000400  \n",
      "99          0.000827            0.409109           0.000395  \n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.7647   \u001b[0m | \u001b[0m3.057    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.2      \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(cat_hyp, pds, random_state=SEED)\n",
    "optimizer.maximize(init_points=10, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f90c5d-243e-43b2-bf85-0302f39384bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e101fac-131b-42c2-9297-a627377961a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_temperature': 3.807947176588889,\n",
       " 'depth': 9.704285838459498,\n",
       " 'learning_rate': 0.17319939418114053}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687054c-7b6d-46d5-ac52-37900d087db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cefca01-8605-491e-85ca-b8bcf5ede157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CatBoostClassifier in module catboost.core:\n",
      "\n",
      "class CatBoostClassifier(CatBoost)\n",
      " |  CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for CatBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  iterations : int, [default=500]\n",
      " |      Max count of trees.\n",
      " |      range: [1,+inf]\n",
      " |  learning_rate : float, [default value is selected automatically for binary classification with other parameters set to default. In all other cases default is 0.03]\n",
      " |      Step size shrinkage used in update to prevents overfitting.\n",
      " |      range: (0,1]\n",
      " |  depth : int, [default=6]\n",
      " |      Depth of a tree. All trees are the same depth.\n",
      " |      range: [1,+inf]\n",
      " |  l2_leaf_reg : float, [default=3.0]\n",
      " |      Coefficient at the L2 regularization term of the cost function.\n",
      " |      range: [0,+inf]\n",
      " |  model_size_reg : float, [default=None]\n",
      " |      Model size regularization coefficient.\n",
      " |      range: [0,+inf]\n",
      " |  rsm : float, [default=None]\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |      range: (0,1]\n",
      " |  loss_function : string or object, [default='Logloss']\n",
      " |      The metric to use in training and also selector of the machine learning\n",
      " |      problem to solve. If string, then the name of a supported metric,\n",
      " |      optionally suffixed with parameter description.\n",
      " |      If object, it shall provide methods 'calc_ders_range' or 'calc_ders_multi'.\n",
      " |  border_count : int, [default = 254 for training on CPU or 128 for training on GPU]\n",
      " |      The number of partitions in numeric features binarization. Used in the preliminary calculation.\n",
      " |      range: [1,65535] on CPU, [1,255] on GPU\n",
      " |  feature_border_type : string, [default='GreedyLogSum']\n",
      " |      The binarization mode in numeric features binarization. Used in the preliminary calculation.\n",
      " |      Possible values:\n",
      " |          - 'Median'\n",
      " |          - 'Uniform'\n",
      " |          - 'UniformAndQuantiles'\n",
      " |          - 'GreedyLogSum'\n",
      " |          - 'MaxLogSum'\n",
      " |          - 'MinEntropy'\n",
      " |  per_float_feature_quantization : list of strings, [default=None]\n",
      " |      List of float binarization descriptions.\n",
      " |      Format : described in documentation on catboost.ai\n",
      " |      Example 1: ['0:1024'] means that feature 0 will have 1024 borders.\n",
      " |      Example 2: ['0:border_count=1024', '1:border_count=1024', ...] means that two first features have 1024 borders.\n",
      " |      Example 3: ['0:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum',\n",
      " |                  '1:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum'] - defines more quantization properties for first two features.\n",
      " |  input_borders : string or pathlib.Path, [default=None]\n",
      " |      input file with borders used in numeric features binarization.\n",
      " |  output_borders : string, [default=None]\n",
      " |      output file for borders that were used in numeric features binarization.\n",
      " |  fold_permutation_block : int, [default=1]\n",
      " |      To accelerate the learning.\n",
      " |      The recommended value is within [1, 256]. On small samples, must be set to 1.\n",
      " |      range: [1,+inf]\n",
      " |  od_pval : float, [default=None]\n",
      " |      Use overfitting detector to stop training when reaching a specified threshold.\n",
      " |      Can be used only with eval_set.\n",
      " |      range: [0,1]\n",
      " |  od_wait : int, [default=None]\n",
      " |      Number of iterations which overfitting detector will wait after new best error.\n",
      " |  od_type : string, [default=None]\n",
      " |      Type of overfitting detector which will be used in program.\n",
      " |      Posible values:\n",
      " |          - 'IncToDec'\n",
      " |          - 'Iter'\n",
      " |      For 'Iter' type od_pval must not be set.\n",
      " |      If None, then od_type=IncToDec.\n",
      " |  nan_mode : string, [default=None]\n",
      " |      Way to process missing values for numeric features.\n",
      " |      Possible values:\n",
      " |          - 'Forbidden' - raises an exception if there is a missing value for a numeric feature in a dataset.\n",
      " |          - 'Min' - each missing value will be processed as the minimum numerical value.\n",
      " |          - 'Max' - each missing value will be processed as the maximum numerical value.\n",
      " |      If None, then nan_mode=Min.\n",
      " |  counter_calc_method : string, [default=None]\n",
      " |      The method used to calculate counters for dataset with Counter type.\n",
      " |      Possible values:\n",
      " |          - 'PrefixTest' - only objects up to current in the test dataset are considered\n",
      " |          - 'FullTest' - all objects are considered in the test dataset\n",
      " |          - 'SkipTest' - Objects from test dataset are not considered\n",
      " |          - 'Full' - all objects are considered for both learn and test dataset\n",
      " |      If None, then counter_calc_method=PrefixTest.\n",
      " |  leaf_estimation_iterations : int, [default=None]\n",
      " |      The number of steps in the gradient when calculating the values in the leaves.\n",
      " |      If None, then leaf_estimation_iterations=1.\n",
      " |      range: [1,+inf]\n",
      " |  leaf_estimation_method : string, [default=None]\n",
      " |      The method used to calculate the values in the leaves.\n",
      " |      Possible values:\n",
      " |          - 'Newton'\n",
      " |          - 'Gradient'\n",
      " |  thread_count : int, [default=None]\n",
      " |      Number of parallel threads used to run CatBoost.\n",
      " |      If None or -1, then the number of threads is set to the number of CPU cores.\n",
      " |      range: [1,+inf]\n",
      " |  random_seed : int, [default=None]\n",
      " |      Random number seed.\n",
      " |      If None, 0 is used.\n",
      " |      range: [0,+inf]\n",
      " |  use_best_model : bool, [default=None]\n",
      " |      To limit the number of trees in predict() using information about the optimal value of the error function.\n",
      " |      Can be used only with eval_set.\n",
      " |  best_model_min_trees : int, [default=None]\n",
      " |      The minimal number of trees the best model should have.\n",
      " |  verbose: bool\n",
      " |      When set to True, logging_level is set to 'Verbose'.\n",
      " |      When set to False, logging_level is set to 'Silent'.\n",
      " |  silent: bool, synonym for verbose\n",
      " |  logging_level : string, [default='Verbose']\n",
      " |      Possible values:\n",
      " |          - 'Silent'\n",
      " |          - 'Verbose'\n",
      " |          - 'Info'\n",
      " |          - 'Debug'\n",
      " |  metric_period : int, [default=1]\n",
      " |      The frequency of iterations to print the information to stdout. The value should be a positive integer.\n",
      " |  simple_ctr: list of strings, [default=None]\n",
      " |      Binarization settings for categorical features.\n",
      " |          Format : see documentation\n",
      " |          Example: ['Borders:CtrBorderCount=5:Prior=0:Prior=0.5', 'BinarizedTargetMeanValue:TargetBorderCount=10:TargetBorderType=MinEntropy', ...]\n",
      " |          CTR types:\n",
      " |              CPU and GPU\n",
      " |              - 'Borders'\n",
      " |              - 'Buckets'\n",
      " |              CPU only\n",
      " |              - 'BinarizedTargetMeanValue'\n",
      " |              - 'Counter'\n",
      " |              GPU only\n",
      " |              - 'FloatTargetMeanValue'\n",
      " |              - 'FeatureFreq'\n",
      " |          Number_of_borders, binarization type, target borders and binarizations, priors are optional parametrs\n",
      " |  combinations_ctr: list of strings, [default=None]\n",
      " |  per_feature_ctr: list of strings, [default=None]\n",
      " |  ctr_target_border_count: int, [default=None]\n",
      " |      Maximum number of borders used in target binarization for categorical features that need it.\n",
      " |      If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it\n",
      " |      overrides this value.\n",
      " |      range: [1, 255]\n",
      " |  ctr_leaf_count_limit : int, [default=None]\n",
      " |      The maximum number of leaves with categorical features.\n",
      " |      If the number of leaves exceeds the specified limit, some leaves are discarded.\n",
      " |      The leaves to be discarded are selected as follows:\n",
      " |          - The leaves are sorted by the frequency of the values.\n",
      " |          - The top N leaves are selected, where N is the value specified in the parameter.\n",
      " |          - All leaves starting from N+1 are discarded.\n",
      " |      This option reduces the resulting model size\n",
      " |      and the amount of memory required for training.\n",
      " |      Note that the resulting quality of the model can be affected.\n",
      " |      range: [1,+inf] (for zero limit use ignored_features)\n",
      " |  store_all_simple_ctr : bool, [default=None]\n",
      " |      Ignore categorical features, which are not used in feature combinations,\n",
      " |      when choosing candidates for exclusion.\n",
      " |      Use this parameter with ctr_leaf_count_limit only.\n",
      " |  max_ctr_complexity : int, [default=4]\n",
      " |      The maximum number of Categ features that can be combined.\n",
      " |      range: [0,+inf]\n",
      " |  has_time : bool, [default=False]\n",
      " |      To use the order in which objects are represented in the input data\n",
      " |      (do not perform a random permutation of the dataset at the preprocessing stage).\n",
      " |  allow_const_label : bool, [default=False]\n",
      " |      To allow the constant label value in dataset.\n",
      " |  target_border: float, [default=None]\n",
      " |      Border for target binarization.\n",
      " |  classes_count : int, [default=None]\n",
      " |      The upper limit for the numeric class label.\n",
      " |      Defines the number of classes for multiclassification.\n",
      " |      Only non-negative integers can be specified.\n",
      " |      The given integer should be greater than any of the target values.\n",
      " |      If this parameter is specified the labels for all classes in the input dataset\n",
      " |      should be smaller than the given value.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  class_weights : list or dict, [default=None]\n",
      " |      Classes weights. The values are used as multipliers for the object weights.\n",
      " |      If None, all classes are supposed to have weight one.\n",
      " |      If list - class weights in order of class_names or sequential classes if class_names is undefined\n",
      " |      If dict - dict of class_name -> class_weight.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  auto_class_weights : string [default=None]\n",
      " |      Enables automatic class weights calculation. Possible values:\n",
      " |          - Balanced  # weight = maxSummaryClassWeight / summaryClassWeight, statistics determined from train pool\n",
      " |          - SqrtBalanced  # weight = sqrt(maxSummaryClassWeight / summaryClassWeight)\n",
      " |  class_names: list of strings, [default=None]\n",
      " |      Class names. Allows to redefine the default values for class labels (integer numbers).\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  one_hot_max_size : int, [default=None]\n",
      " |      Convert the feature to float\n",
      " |      if the number of different values that it takes exceeds the specified value.\n",
      " |      Ctrs are not calculated for such features.\n",
      " |  random_strength : float, [default=1]\n",
      " |      Score standard deviation multiplier.\n",
      " |  name : string, [default='experiment']\n",
      " |      The name that should be displayed in the visualization tools.\n",
      " |  ignored_features : list, [default=None]\n",
      " |      Indices or names of features that should be excluded when training.\n",
      " |  train_dir : string or pathlib.Path, [default=None]\n",
      " |      The directory in which you want to record generated in the process of learning files.\n",
      " |  custom_metric : string or list of strings, [default=None]\n",
      " |      To use your own metric function.\n",
      " |  custom_loss: alias to custom_metric\n",
      " |  eval_metric : string or object, [default=None]\n",
      " |      To optimize your custom metric in loss.\n",
      " |  bagging_temperature : float, [default=None]\n",
      " |      Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.\n",
      " |      Typical values are in range [0, 1] (0 - no bagging, 1 - default).\n",
      " |  save_snapshot : bool, [default=None]\n",
      " |      Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |  snapshot_file : string or pathlib.Path, [default=None]\n",
      " |      Learn progress snapshot file path, if None will use default filename\n",
      " |  snapshot_interval: int, [default=600]\n",
      " |      Interval between saving snapshots (seconds)\n",
      " |  fold_len_multiplier : float, [default=None]\n",
      " |      Fold length multiplier. Should be greater than 1\n",
      " |  used_ram_limit : string or number, [default=None]\n",
      " |      Set a limit on memory consumption (value like '1.2gb' or 1.2e9).\n",
      " |      WARNING: Currently this option affects CTR memory usage only.\n",
      " |  gpu_ram_part : float, [default=0.95]\n",
      " |      Fraction of the GPU RAM to use for training, a value from (0, 1].\n",
      " |  pinned_memory_size: int [default=None]\n",
      " |      Size of additional CPU pinned memory used for GPU learning,\n",
      " |      usually is estimated automatically, thus usually should not be set.\n",
      " |  allow_writing_files : bool, [default=True]\n",
      " |      If this flag is set to False, no files with different diagnostic info will be created during training.\n",
      " |      With this flag no snapshotting can be done. Plus visualisation will not\n",
      " |      work, because visualisation uses files that are created and updated during training.\n",
      " |  final_ctr_computation_mode : string, [default='Default']\n",
      " |      Possible values:\n",
      " |          - 'Default' - Compute final ctrs for all pools.\n",
      " |          - 'Skip' - Skip final ctr computation. WARNING: model without ctrs can't be applied.\n",
      " |  approx_on_full_history : bool, [default=False]\n",
      " |      If this flag is set to True, each approximated value is calculated using all the preceeding rows in the fold (slower, more accurate).\n",
      " |      If this flag is set to False, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).\n",
      " |  boosting_type : string, default value depends on object count and feature count in train dataset and on learning mode.\n",
      " |      Boosting scheme.\n",
      " |      Possible values:\n",
      " |          - 'Ordered' - Gives better quality, but may slow down the training.\n",
      " |          - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.\n",
      " |  task_type : string, [default=None]\n",
      " |      The calcer type used to train the model.\n",
      " |      Possible values:\n",
      " |          - 'CPU'\n",
      " |          - 'GPU'\n",
      " |  device_config : string, [default=None], deprecated, use devices instead\n",
      " |  devices : list or string, [default=None], GPU devices to use.\n",
      " |      String format is: '0' for 1 device or '0:1:3' for multiple devices or '0-3' for range of devices.\n",
      " |      List format is : [0] for 1 device or [0,1,3] for multiple devices.\n",
      " |  \n",
      " |  bootstrap_type : string, Bayesian, Bernoulli, Poisson, MVS.\n",
      " |      Default bootstrap is Bayesian for GPU and MVS for CPU.\n",
      " |      Poisson bootstrap is supported only on GPU.\n",
      " |      MVS bootstrap is supported only on CPU.\n",
      " |  \n",
      " |  subsample : float, [default=None]\n",
      " |      Sample rate for bagging. This parameter can be used Poisson or Bernoully bootstrap types.\n",
      " |  \n",
      " |  mvs_reg : float, [default is set automatically at each iteration based on gradient distribution]\n",
      " |      Regularization parameter for MVS sampling algorithm\n",
      " |  \n",
      " |  monotone_constraints : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Monotone constraints for features.\n",
      " |  \n",
      " |  feature_weights : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Coefficient to multiply split gain with specific feature use. Should be non-negative.\n",
      " |  \n",
      " |  penalties_coefficient : float, [default=1]\n",
      " |      Common coefficient for all penalties. Should be non-negative.\n",
      " |  \n",
      " |  first_feature_use_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties to first use of specific feature in model. Should be non-negative.\n",
      " |  \n",
      " |  per_object_feature_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties for first use of feature for each object. Should be non-negative.\n",
      " |  \n",
      " |  sampling_frequency : string, [default=PerTree]\n",
      " |      Frequency to sample weights and objects when building trees.\n",
      " |      Possible values:\n",
      " |          - 'PerTree' - Before constructing each new tree\n",
      " |          - 'PerTreeLevel' - Before choosing each new split of a tree\n",
      " |  \n",
      " |  sampling_unit : string, [default='Object'].\n",
      " |      Possible values:\n",
      " |          - 'Object'\n",
      " |          - 'Group'\n",
      " |      The parameter allows to specify the sampling scheme:\n",
      " |      sample weights for each object individually or for an entire group of objects together.\n",
      " |  \n",
      " |  dev_score_calc_obj_block_size: int, [default=5000000]\n",
      " |      CPU only. Size of block of samples in score calculation. Should be > 0\n",
      " |      Used only for learning speed tuning.\n",
      " |      Changing this parameter can affect results due to numerical accuracy differences\n",
      " |  \n",
      " |  dev_efb_max_buckets : int, [default=1024]\n",
      " |      CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.\n",
      " |      Used only for learning speed tuning.\n",
      " |  \n",
      " |  sparse_features_conflict_fraction : float, [default=0.0]\n",
      " |      CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.\n",
      " |      Should be a real value in [0, 1) interval.\n",
      " |  \n",
      " |  grow_policy : string, [SymmetricTree,Lossguide,Depthwise], [default=SymmetricTree]\n",
      " |      The tree growing policy. It describes how to perform greedy tree construction.\n",
      " |  \n",
      " |  min_data_in_leaf : int, [default=1].\n",
      " |      The minimum training samples count in leaf.\n",
      " |      CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.\n",
      " |      This parameter is used only for Depthwise and Lossguide growing policies.\n",
      " |  \n",
      " |  max_leaves : int, [default=31],\n",
      " |      The maximum leaf count in resulting tree.\n",
      " |      This parameter is used only for Lossguide growing policy.\n",
      " |  \n",
      " |  score_function : string, possible values L2, Cosine, NewtonL2, NewtonCosine, [default=Cosine]\n",
      " |      For growing policy Lossguide default=NewtonL2.\n",
      " |      GPU only. Score that is used during tree construction to select the next tree split.\n",
      " |  \n",
      " |  max_depth : int, Synonym for depth.\n",
      " |  \n",
      " |  n_estimators : int, synonym for iterations.\n",
      " |  \n",
      " |  num_trees : int, synonym for iterations.\n",
      " |  \n",
      " |  num_boost_round : int, synonym for iterations.\n",
      " |  \n",
      " |  colsample_bylevel : float, synonym for rsm.\n",
      " |  \n",
      " |  random_state : int, synonym for random_seed.\n",
      " |  \n",
      " |  reg_lambda : float, synonym for l2_leaf_reg.\n",
      " |  \n",
      " |  objective : string, synonym for loss_function.\n",
      " |  \n",
      " |  num_leaves : int, synonym for max_leaves.\n",
      " |  \n",
      " |  min_child_samples : int, synonym for min_data_in_leaf\n",
      " |  \n",
      " |  eta : float, synonym for learning_rate.\n",
      " |  \n",
      " |  max_bin : float, synonym for border_count.\n",
      " |  \n",
      " |  scale_pos_weight : float, synonym for class_weights.\n",
      " |      Can be used only for binary classification. Sets weight multiplier for\n",
      " |      class 1 to scale_pos_weight value.\n",
      " |  \n",
      " |  metadata : dict, string to string key-value pairs to be stored in model metadata storage\n",
      " |  \n",
      " |  early_stopping_rounds : int\n",
      " |      Synonym for od_wait. Only one of these parameters should be set.\n",
      " |  \n",
      " |  cat_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Categ features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  text_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Text features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  embedding_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Embedding features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  leaf_estimation_backtracking : string, [default=None]\n",
      " |      Type of backtracking during gradient descent.\n",
      " |      Possible values:\n",
      " |          - 'No' - never backtrack; supported on CPU and GPU\n",
      " |          - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU\n",
      " |          - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only\n",
      " |  \n",
      " |  model_shrink_rate : float, [default=0]\n",
      " |      This parameter enables shrinkage of model at the start of each iteration. CPU only.\n",
      " |      For Constant mode shrinkage coefficient is calculated as (1 - model_shrink_rate * learning_rate).\n",
      " |      For Decreasing mode shrinkage coefficient is calculated as (1 - model_shrink_rate / iteration).\n",
      " |      Shrinkage coefficient should be in [0, 1).\n",
      " |  \n",
      " |  model_shrink_mode : string, [default=None]\n",
      " |      Mode of shrinkage coefficient calculation. CPU only.\n",
      " |      Possible values:\n",
      " |          - 'Constant' - Shrinkage coefficient is constant at each iteration.\n",
      " |          - 'Decreasing' - Shrinkage coefficient decreases at each iteration.\n",
      " |  \n",
      " |  langevin : bool, [default=False]\n",
      " |      Enables the Stochastic Gradient Langevin Boosting. CPU only.\n",
      " |  \n",
      " |  diffusion_temperature : float, [default=0]\n",
      " |      Langevin boosting diffusion temperature. CPU only.\n",
      " |  \n",
      " |  posterior_sampling : bool, [default=False]\n",
      " |      Set group of parameters for further use Uncertainty prediction:\n",
      " |          - Langevin = True\n",
      " |          - Model Shrink Rate = 1/(2N), where N is dataset size\n",
      " |          - Model Shrink Mode = Constant\n",
      " |          - Diffusion-temperature = N, where N is dataset size. CPU only.\n",
      " |  \n",
      " |  boost_from_average : bool, [default=True for RMSE, False for other losses]\n",
      " |      Enables to initialize approx values by best constant value for specified loss function.\n",
      " |      Available for RMSE, Logloss, CrossEntropy, Quantile and MAE.\n",
      " |  \n",
      " |  tokenizers : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'tokenizer_id': 'Tokenizer',  # Tokeinzer identifier.\n",
      " |              'lowercasing': 'false',  # Possible values: 'true', 'false'.\n",
      " |              'number_process_policy': 'LeaveAsIs',  # Possible values: 'Skip', 'LeaveAsIs', 'Replace'.\n",
      " |              'number_token': '%',  # Rarely used character. Used in conjunction with Replace NumberProcessPolicy.\n",
      " |              'separator_type': 'ByDelimiter',  # Possible values: 'ByDelimiter', 'BySense'.\n",
      " |              'delimiter': ' ',  # Used in conjunction with ByDelimiter SeparatorType.\n",
      " |              'split_by_set': 'false',  # Each single character in delimiter used as individual delimiter.\n",
      " |              'skip_empty': 'true',  # Possible values: 'true', 'false'.\n",
      " |              'token_types': ['Word', 'Number', 'Unknown'],  # Used in conjunction with BySense SeparatorType.\n",
      " |                  # Possible values: 'Word', 'Number', 'Punctuation', 'SentenceBreak', 'ParagraphBreak', 'Unknown'.\n",
      " |              'subtokens_policy': 'SingleToken',  # Possible values:\n",
      " |                  # 'SingleToken' - All subtokens are interpreted as single token).\n",
      " |                  # 'SeveralTokens' - All subtokens are interpreted as several token.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  dictionaries : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'dictionary_id': 'Dictionary',  # Dictionary identifier.\n",
      " |              'token_level_type': 'Word',  # Possible values: 'Word', 'Letter'.\n",
      " |              'gram_order': '1',  # 1 for Unigram, 2 for Bigram, ...\n",
      " |              'skip_step': '0',  # 1 for 1-skip-gram, ...\n",
      " |              'end_of_word_token_policy': 'Insert',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'end_of_sentence_token_policy': 'Skip',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'occurrence_lower_bound': '3',  # The lower bound of token occurrences in the text to include it in the dictionary.\n",
      " |              'max_dictionary_size': '50000',  # The max dictionary size.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  feature_calcers : list of strings,\n",
      " |      Each string is a calcer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          'NaiveBayes',\n",
      " |          'BM25',\n",
      " |          'BoW:top_tokens_count=2000',\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  text_processing : dict,\n",
      " |      Text processging description.\n",
      " |  \n",
      " |  eval_fraction : float, [default=None]\n",
      " |      Fraction of the train dataset to be used as the evaluation dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CatBoostClassifier\n",
      " |      CatBoost\n",
      " |      _CatBoostBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |      Initialize the CatBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for CatBoost.\n",
      " |          If  None, all params are set to their defaults.\n",
      " |          If  dict, overriding parameters present in dict.\n",
      " |  \n",
      " |  fit(self, X, y=None, cat_features=None, text_features=None, embedding_features=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None, plot=False, plot_file=None, column_description=None, verbose_eval=None, metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, init_model=None, callbacks=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Fit the CatBoostClassifier model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      cat_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Categ columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      text_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Text columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      embedding_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Embedding columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      sample_weight : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Instance weights, 1 dimensional array like.\n",
      " |      \n",
      " |      baseline : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving 2 dimensional array like data.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      use_best_model : bool, optional (default=None)\n",
      " |          Flag to use best model\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for early-stopping\n",
      " |      \n",
      " |      metric_period : int\n",
      " |          Frequency of evaluating metrics.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      silent : bool\n",
      " |          If silent is True, logging_level is set to Silent.\n",
      " |          If silent is False, logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      verbose_eval : bool or int\n",
      " |          Synonym for verbose. Only one of these parameters should be set.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n",
      " |      \n",
      " |      save_snapshot : bool, [default=None]\n",
      " |          Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |      \n",
      " |      snapshot_file : string or pathlib.Path, [default=None]\n",
      " |          Learn progress snapshot file path, if None will use default filename\n",
      " |      \n",
      " |      snapshot_interval: int, [default=600]\n",
      " |          Interval between saving snapshots (seconds)\n",
      " |      \n",
      " |      init_model : CatBoost class or string or pathlib.Path, [default=None]\n",
      " |          Continue training starting from the existing model.\n",
      " |          If this parameter is a string or pathlib.Path, load initial model from the path specified by this string.\n",
      " |      \n",
      " |      callbacks : list, optional (default=None)\n",
      " |          List of callback objects that are applied at end of each iteration.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : CatBoost\n",
      " |  \n",
      " |  get_probability_threshold(self)\n",
      " |      Get a threshold for class separation in binary classification task\n",
      " |  \n",
      " |  predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return class label.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_log_proba(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class log probability with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_proba(self, X, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class probability with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If X is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If X is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate accuracy.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          Data to apply model on.\n",
      " |      y : list or numpy.ndarray\n",
      " |          True labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy : float\n",
      " |  \n",
      " |  set_probability_threshold(self, binclass_probability_threshold=None)\n",
      " |      Set a threshold for class separation in binary classification task for a trained model.\n",
      " |      :param binclass_probability_threshold: float number in [0, 1] or None to discard it\n",
      " |  \n",
      " |  staged_predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return majority vote class.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_log_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CatBoost:\n",
      " |  \n",
      " |  calc_feature_statistics(self, data, target=None, feature=None, prediction_type=None, cat_feature_values=None, plot=True, max_cat_features_on_plot=10, thread_count=-1, plot_file=None)\n",
      " |      Get statistics for the feature using the model, dataset and target.\n",
      " |      To use this function, you should install plotly.\n",
      " |      \n",
      " |      The catboost model has borders for the float features used in it. The borders divide\n",
      " |      feature values into bins, and the model's prediction depends on the number of the bin where the\n",
      " |      feature value falls in.\n",
      " |      \n",
      " |      For float features this function takes model's borders and computes\n",
      " |      1) Mean target value for every bin;\n",
      " |      2) Mean model prediction for every bin;\n",
      " |      3) The number of objects in dataset which fall into each bin;\n",
      " |      4) Predictions on varying feature. For every object, varies the feature value\n",
      " |      so that it falls into bin #0, bin #1, ... and counts model predictions.\n",
      " |      Then counts average prediction for each bin.\n",
      " |      \n",
      " |      For categorical features (only one-hot supported) does the same, but takes feature values\n",
      " |      provided in cat_feature_values instead of borders.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost. Pool or dict {'pool_name': pool} if you want several pools\n",
      " |          Data to compute statistics on\n",
      " |      target: numpy.ndarray or pandas.Series or dict {'pool_name': target} if you want several pools or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      feature: None, int, string, or list of int or strings\n",
      " |          Features indexes or names in pd.DataFrame for which you want to get statistics.\n",
      " |          None, if you need statistics for all features.\n",
      " |      prediction_type: str\n",
      " |          Prediction type used for counting mean_prediction: 'Class', 'Probability' or 'RawFormulaVal'.\n",
      " |          If not specified, is derived from the model.\n",
      " |      cat_feature_values: list or numpy.ndarray or pandas.Series or\n",
      " |                          dict: int or string to list or numpy.ndarray or pandas.Series\n",
      " |          Contains categorical feature values you need to get statistics on.\n",
      " |          Use dict, when parameter 'feature' is a list to specify cat values for different features.\n",
      " |          When parameter 'feature' is int or str, you can just pass list of cat values.\n",
      " |      plot: bool\n",
      " |          Plot statistics.\n",
      " |      max_cat_features_on_plot: int\n",
      " |          If categorical feature takes more than max_cat_features_on_plot different unique values,\n",
      " |          output result on several plots, not more than max_cat_features_on_plot feature values on each.\n",
      " |          Used only if plot=True or plot_file is not None.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use for getting statistics.\n",
      " |      plot_file: str\n",
      " |          Output file for plot statistics.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict if parameter 'feature' is int or string, else dict of dicts:\n",
      " |          For each unique feature contain\n",
      " |          python dict with binarized feature statistics.\n",
      " |          For float feature, includes\n",
      " |                  'borders' -- borders for the specified feature in model\n",
      " |                  'binarized_feature' -- numbers of bins where feature values fall\n",
      " |                  'mean_target' -- mean value of target over each bin\n",
      " |                  'mean_prediction' -- mean value of model prediction over each bin\n",
      " |                  'objects_per_bin' -- number of objects per bin\n",
      " |                  'predictions_on_varying_feature' -- averaged over dataset predictions for\n",
      " |                  varying feature (see above)\n",
      " |          For one-hot feature, returns the same, but with 'cat_values' instead of 'borders'\n",
      " |  \n",
      " |  calc_leaf_indexes(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=False)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool (default=False)\n",
      " |          Enable debug logging level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : 2-dimensional numpy.ndarray of numpy.uint32 with shape (object count, ntree_end - ntree_start).\n",
      " |          i-th row is an array of leaf indexes for i-th object.\n",
      " |  \n",
      " |  compare(self, model, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Draw train and eval errors in Jupyter notebook for both models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model: CatBoost model\n",
      " |          Another model to draw metrics\n",
      " |      \n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |  \n",
      " |  create_metric_calcer(self, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None)\n",
      " |      Create batch metric calcer. Could be used to aggregate metric on several pools\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |          Same as in eval_metrics except data\n",
      " |      Returns\n",
      " |      -------\n",
      " |          BatchMetricCalcer object\n",
      " |      \n",
      " |      Usage example\n",
      " |      -------\n",
      " |      # Large dataset is partitioned into parts [part1, part2]\n",
      " |      model.fit(params)\n",
      " |      batch_calcer = model.create_metric_calcer(['Logloss'])\n",
      " |      batch_calcer.add(part1)\n",
      " |      batch_calcer.add(part2)\n",
      " |      metrics = batch_calcer.eval_metrics()\n",
      " |  \n",
      " |  drop_unused_features(self)\n",
      " |      Drop unused features information from model\n",
      " |  \n",
      " |  eval_metrics(self, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Calculate metrics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : dict: metric -> array of shape [(ntree_end - ntree_start) / eval_period]\n",
      " |  \n",
      " |  get_all_params(self)\n",
      " |      Get all params (specified by user and default params) that were set in training from CatBoost model.\n",
      " |      Full parameters documentation could be found here: https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_borders(self)\n",
      " |      Return map feature_index: borders for float features.\n",
      " |  \n",
      " |  get_cat_feature_indices(self)\n",
      " |  \n",
      " |  get_embedding_feature_indices(self)\n",
      " |  \n",
      " |  get_feature_importance(self, data=None, type=<EFstrType.FeatureImportance: 2>, prettified=False, thread_count=-1, verbose=False, fstr_type=None, shap_mode='Auto', model_output='Raw', interaction_indices=None, shap_calc_type='Regular', reference_data=None, sage_n_samples=128, sage_batch_size=512, sage_detect_convergence=True, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          Data to get feature importance.\n",
      " |          If type in ('LossFunctionChange', 'ShapValues', 'ShapInteractionValues') data must of Pool type.\n",
      " |              For every object in this dataset feature importances will be calculated.\n",
      " |          if type == 'SageValues' data must of Pool type.\n",
      " |              For every feature in this dataset importance will be calculated.\n",
      " |          If type == 'PredictionValuesChange', data is None or a dataset of Pool type\n",
      " |              Dataset specification is needed only in case if the model does not contain leaf weight information (trained with CatBoost v < 0.9).\n",
      " |          If type == 'PredictionDiff' data must contain a matrix of feature values of shape (2, n_features).\n",
      " |              Possible types are catboost.Pool or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData or pandas.SparseDataFrame or scipy.sparse.spmatrix\n",
      " |          If type == 'FeatureImportance'\n",
      " |              See 'PredictionValuesChange' for non-ranking metrics and 'LossFunctionChange' for ranking metrics.\n",
      " |          If type == 'Interaction'\n",
      " |              This parameter is not used.\n",
      " |      \n",
      " |      type : EFstrType or string (converted to EFstrType), optional\n",
      " |                  (default=EFstrType.FeatureImportance)\n",
      " |          Possible values:\n",
      " |              - PredictionValuesChange\n",
      " |                  Calculate score for every feature.\n",
      " |              - LossFunctionChange\n",
      " |                  Calculate score for every feature by loss.\n",
      " |              - FeatureImportance\n",
      " |                  PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics\n",
      " |              - ShapValues\n",
      " |                  Calculate SHAP Values for every object.\n",
      " |              - ShapInteractionValues\n",
      " |                  Calculate SHAP Interaction Values between each pair of features for every object\n",
      " |              - Interaction\n",
      " |                  Calculate pairwise score between every feature.\n",
      " |              - PredictionDiff\n",
      " |                  Calculate most important features explaining difference in predictions for a pair of documents.\n",
      " |              - SageValues\n",
      " |                  Calculate SAGE value for every feature\n",
      " |      \n",
      " |      prettified : bool, optional (default=False)\n",
      " |          change returned data format to the list of (feature_id, importance) pairs sorted by importance\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      fstr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      shap_mode : string, optional (default=\"Auto\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Auto\"\n",
      " |                  Use direct SHAP Values calculation only if data size is smaller than average leaves number\n",
      " |                  (the best of two strategies below is chosen).\n",
      " |              - \"UsePreCalc\"\n",
      " |                  Calculate SHAP Values for every leaf in preprocessing. Final complexity is\n",
      " |                  O(NT(D+F))+O(TL^2 D^2) where N is the number of documents(objects), T - number of trees,\n",
      " |                  D - average tree depth, F - average number of features in tree, L - average number of leaves in tree\n",
      " |                  This is much faster (because of a smaller constant) than direct calculation when N >> L\n",
      " |              - \"NoPreCalc\"\n",
      " |                  Use direct SHAP Values calculation calculation with complexity O(NTLD^2). Direct algorithm\n",
      " |                  is faster when N < L (algorithm from https://arxiv.org/abs/1802.03888)\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=\"Regular\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Regular\"\n",
      " |                  Calculate regular SHAP values\n",
      " |              - \"Approximate\"\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - \"Exact\"\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      interaction_indices : list of int or string (feature_idx_1, feature_idx_2), optional (default=None)\n",
      " |          used only for ShapInteractionValues type\n",
      " |          Calculate SHAP Interaction Values between pair of features feature_idx_1 and feature_idx_2 for every object\n",
      " |      \n",
      " |      reference_data: catboost.Pool or None\n",
      " |          Reference data for Independent Tree SHAP values from https://arxiv.org/abs/1905.04610v1\n",
      " |          if type == 'ShapValues' and reference_data is not None, then Independent Tree SHAP values are calculated\n",
      " |      \n",
      " |      sage_n_samples: int, optional (default=32)\n",
      " |          Number of outer samples used in SAGE values approximation algorithm\n",
      " |      sage_batch_size: int, optional (default=min(512, number of samples in dataset))\n",
      " |          Number of samples used on each step of SAGE values approximation algorithm\n",
      " |      sage_detect_convergence: bool, optional (default=False)\n",
      " |          If set True, sage values calculation will be stopped either when sage values converge\n",
      " |          or when sage_n_samples iterations of algorithm pass\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      depends on type:\n",
      " |          - FeatureImportance\n",
      " |              See PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics.\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=False (default)\n",
      " |              list of length [n_features] with feature_importance values (float) for feature\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=True\n",
      " |              list of length [n_features] with (feature_id (string), feature_importance (float)) pairs, sorted by feature_importance in descending order\n",
      " |          - ShapValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1) with Shap values (float) for (object, feature).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1). For each object it contains Shap values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - ShapInteractionValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1, n_features + 1) with Shap interaction values (float) for (object, feature(i), feature(j)).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1, n_features + 1). For each object it contains Shap interaction values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - Interaction\n",
      " |              list of length [n_features] of 3-element lists of (first_feature_index, second_feature_index, interaction_score (float))\n",
      " |  \n",
      " |  get_object_importance(self, pool, train_pool, top_size=-1, type='Average', update_method='SinglePoint', importance_values_sign='All', thread_count=-1, verbose=False, ostr_type=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      This is the implementation of the LeafInfluence algorithm from the following paper:\n",
      " |      https://arxiv.org/pdf/1802.06640.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pool : Pool\n",
      " |          The pool for which you want to evaluate the object importances.\n",
      " |      \n",
      " |      train_pool : Pool\n",
      " |          The pool on which the model has been trained.\n",
      " |      \n",
      " |      top_size : int (default=-1)\n",
      " |          Method returns the result of the top_size most important train objects.\n",
      " |          If -1, then the top size is not limited.\n",
      " |      \n",
      " |      type : string, optional (default='Average')\n",
      " |          Possible values:\n",
      " |              - Average (Method returns the mean train objects scores for all input objects)\n",
      " |              - PerObject (Method returns the train objects scores for every input object)\n",
      " |      \n",
      " |      importance_values_sign : string, optional (default='All')\n",
      " |          Method returns only Positive, Negative or All values.\n",
      " |          Possible values:\n",
      " |              - Positive\n",
      " |              - Negative\n",
      " |              - All\n",
      " |      \n",
      " |      update_method : string, optional (default='SinglePoint')\n",
      " |          Possible values:\n",
      " |              - SinglePoint\n",
      " |              - TopKLeaves (It is posible to set top size : TopKLeaves:top=2)\n",
      " |              - AllPoints\n",
      " |          Description of the update set methods are given in section 3.1.3 of the paper.\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      ostr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object_importances : tuple of two arrays (indices and scores) of shape = [top_size]\n",
      " |  \n",
      " |  get_param(self, key)\n",
      " |      Get param value from CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : string\n",
      " |          The key to get param value from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value :\n",
      " |          The param value of the key, returns None if param do not exist.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get all params from CatBoost model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_text_feature_indices(self)\n",
      " |  \n",
      " |  grid_search(self, param_grid, X, y=None, cv=3, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Exhaustive search over specified parameter values for a model.\n",
      " |      Aafter calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_grid: dict or list of dictionaries\n",
      " |          Dictionary with parameters names (string) as keys and lists of parameter settings\n",
      " |          to try as values, or a list of such dictionaries, in which case the grids spanned by each\n",
      " |          dictionary in the list are explored.\n",
      " |          This enables searching over any sequence of parameter settings.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for final cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  iterate_leaf_indexes(self, data, ntree_start=0, ntree_end=0)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : generator. For each object in pool yields one-dimensional numpy.ndarray of leaf indexes.\n",
      " |  \n",
      " |  load_model(self, fname=None, format='cbm', stream=None, blob=None)\n",
      " |      Load model from a file, stream or blob.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  plot_partial_dependence(self, data, features, plot=True, plot_file=None, thread_count=-1)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features: int, str, list<int>, tuple<int>, list<string>, tuple<string>\n",
      " |          Float features to calculate partial dependence for. Number of features should be 1 or 2.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use. If -1 use maximum available number of threads.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          If number of features is one - 1d numpy array and figure with line plot.\n",
      " |          If number of features is two - 2d numpy array and figure with 2d heatmap.\n",
      " |  \n",
      " |  plot_predictions(self, data, features_to_change, plot=True, plot_file=None)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      feature:\n",
      " |          Float features indexes in pd.DataFrame for which you want vary prediction value.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          List of list of predictions for all buckets for all documents in data\n",
      " |  \n",
      " |  plot_tree(self, tree_idx, pool=None)\n",
      " |  \n",
      " |  randomized_search(self, param_distributions, X, y=None, cv=3, n_iter=10, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>)\n",
      " |      Randomized search on hyper parameters.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      In contrast to grid_search, not all parameter values are tried out,\n",
      " |      but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
      " |      The number of parameter settings that are tried is given by n_iter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_distributions: dict\n",
      " |          Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
      " |          Distributions must provide a rvs method for sampling (such as those from scipy.stats.distributions).\n",
      " |          If a list is given, it is sampled uniformly.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      n_iter: int\n",
      " |          Number of parameter settings that are sampled.\n",
      " |          n_iter trades off runtime vs quality of the solution.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  save_borders(self, fname)\n",
      " |      Save the model borders to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or pathlib.Path\n",
      " |          Output file name.\n",
      " |  \n",
      " |  save_model(self, fname, format='cbm', export_parameters=None, pool=None)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name.\n",
      " |      format : string\n",
      " |          Possible values:\n",
      " |              * 'cbm' for catboost binary format,\n",
      " |              * 'coreml' to export into Apple CoreML format\n",
      " |              * 'onnx' to export into ONNX-ML format\n",
      " |              * 'pmml' to export into PMML format\n",
      " |              * 'cpp' to export as C++ code\n",
      " |              * 'python' to export as Python code.\n",
      " |      export_parameters : dict\n",
      " |          Parameters for CoreML export:\n",
      " |              * prediction_type : string - either 'probability' or 'raw'\n",
      " |              * coreml_description : string\n",
      " |              * coreml_model_version : string\n",
      " |              * coreml_model_author : string\n",
      " |              * coreml_model_license: string\n",
      " |          Parameters for PMML export:\n",
      " |              * pmml_copyright : string\n",
      " |              * pmml_description : string\n",
      " |              * pmml_model_version : string\n",
      " |      pool : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series or catboost.FeaturesData\n",
      " |          Training pool.\n",
      " |  \n",
      " |  select_features(self, X, y=None, eval_set=None, features_for_select=None, num_features_to_select=None, algorithm=None, steps=None, shap_calc_type=None, train_final_model=True, verbose=None, logging_level=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x107c96640>, log_cerr=<ipykernel.iostream.OutStream object at 0x107c96670>, grouping=None, features_tags_for_select=None, num_features_tags_to_select=None)\n",
      " |      Select best features from pool according to loss value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      eval_set : catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Dataset for evaluation.\n",
      " |      \n",
      " |      features_for_select : str or list of feature indices, names or ranges\n",
      " |          (for grouping = Individual)\n",
      " |          Which features should participate in the selection.\n",
      " |          Format examples:\n",
      " |              - [0, 2, 3, 4, 17]\n",
      " |              - [0, \"2-4\", 17] (both ends in ranges are inclusive)\n",
      " |              - \"0,2-4,20\"\n",
      " |              - [\"Name0\", \"Name2\", \"Name3\", \"Name4\", \"Name20\"]\n",
      " |      \n",
      " |      num_features_to_select : positive int\n",
      " |          (for grouping = Individual)\n",
      " |          How many features to select from features_for_select.\n",
      " |      \n",
      " |      algorithm : EFeaturesSelectionAlgorithm or string, optional (default=RecursiveByShapValues)\n",
      " |          Which algorithm to use for features selection.\n",
      " |          Possible values:\n",
      " |              - RecursiveByPredictionValuesChange\n",
      " |                  Use prediction values change as feature strength, eliminate batch of features at once.\n",
      " |              - RecursiveByLossFunctionChange\n",
      " |                  Use loss function change as feature strength, eliminate batch of features at each step.\n",
      " |              - RecursiveByShapValues\n",
      " |                  Use shap values to estimate loss function change, eliminate features one by one.\n",
      " |      \n",
      " |      steps : positive int, optional (default=1)\n",
      " |          How many steps should be performed. In other words, how many times a full model will be trained.\n",
      " |          More steps give more accurate results.\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=Regular)\n",
      " |          Which method to use for calculation of shap values.\n",
      " |          Possible values:\n",
      " |              - Regular\n",
      " |                  Calculate regular SHAP values\n",
      " |              - Approximate\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - Exact\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      train_final_model : bool, optional (default=True)\n",
      " |          Need to fit model with selected features.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      grouping : EFeaturesSelectionGrouping or string, optional (default=Individual)\n",
      " |          Which grouping to use for features selection.\n",
      " |          Possible values:\n",
      " |              - Individual\n",
      " |                  Select individual features\n",
      " |              - ByTags\n",
      " |                  Select feature groups (marked by tags)\n",
      " |      \n",
      " |      features_tags_for_select : list of strings\n",
      " |          (for grouping = ByTags)\n",
      " |          Which features tags should participate in the selection.\n",
      " |      \n",
      " |      num_features_tags_to_select : positive int\n",
      " |          (for grouping = ByTags)\n",
      " |          How many features tags to select from features_tags_for_select.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with fields:\n",
      " |          'selected_features': list of selected features indices\n",
      " |          'eliminated_features': list of eliminated features indices\n",
      " |          'selected_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |          'eliminated_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set parameters into CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : key=value format\n",
      " |          List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\n",
      " |  \n",
      " |  shrink(self, ntree_end, ntree_start=0)\n",
      " |      Shrink the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ntree_end: int\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |  \n",
      " |  virtual_ensembles_predict(self, data, prediction_type='VirtEnsembles', ntree_end=0, virtual_ensembles_count=10, thread_count=-1, verbose=None)\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'VirtEnsembles': return V (virtual_ensembles_count) predictions.\n",
      " |              k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant.\n",
      " |          - 'TotalUncertainty': see returned predictions format in 'Returns' part\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      virtual_ensembles_count: int, optional (default=10)\n",
      " |          virtual ensembles count for 'TotalUncertainty' and 'VirtEnsembles' prediction types.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          (with V as virtual_ensembles_count and T as trees count,\n",
      " |          k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant)\n",
      " |          If data is for a single object, return 1-dimensional array of predictions with size depends on prediction type,\n",
      " |          otherwise return 2-dimensional numpy.ndarray with shape (number_of_objects x size depends on prediction type);\n",
      " |          Returned predictions depends on prediction type:\n",
      " |          If loss-function was RMSEWithUncertainty:\n",
      " |              - 'VirtEnsembles': [mean0, var0, mean1, var1, ..., vark-1].\n",
      " |              - 'TotalUncertainty': [mean_predict, KnowledgeUnc, DataUnc].\n",
      " |          otherwise for regression:\n",
      " |              - 'VirtEnsembles':  [mean0, mean1, ...].\n",
      " |              - 'TotalUncertainty': [mean_predicts, KnowledgeUnc].\n",
      " |          otherwise for binary classification:\n",
      " |              - 'VirtEnsembles':  [ApproxRawFormulaVal0, ApproxRawFormulaVal1, ..., ApproxRawFormulaValk-1].\n",
      " |              - 'TotalUncertainty':  [DataUnc, TotalUnc].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from CatBoost:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  copy(self)\n",
      " |  \n",
      " |  get_best_iteration(self)\n",
      " |  \n",
      " |  get_best_score(self)\n",
      " |  \n",
      " |  get_evals_result(self)\n",
      " |  \n",
      " |  get_leaf_values(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_values : 1d-array of leaf values for all trees.\n",
      " |      Value corresponding to j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_leaf_weights(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_weights : 1d-array of leaf weights for all trees.\n",
      " |      Weight of j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_metadata(self)\n",
      " |  \n",
      " |  get_n_features_in(self)\n",
      " |  \n",
      " |  get_scale_and_bias(self)\n",
      " |  \n",
      " |  get_test_eval(self)\n",
      " |  \n",
      " |  get_test_evals(self)\n",
      " |  \n",
      " |  get_tree_leaf_counts(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tree_leaf_counts : 1d-array of numpy.uint32 of size tree_count_.\n",
      " |      tree_leaf_counts[i] equals to the number of leafs in i-th tree of the ensemble.\n",
      " |  \n",
      " |  is_fitted(self)\n",
      " |  \n",
      " |  set_feature_names(self, feature_names)\n",
      " |      Sets feature names equal to feature_names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_names: 1-d array of strings with new feature names in the same order as in pool\n",
      " |  \n",
      " |  set_leaf_values(self, new_leaf_values)\n",
      " |      Sets values at tree leafs of ensemble equal to new_leaf_values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_leaf_values : 1d-array with new leaf values for all trees.\n",
      " |      It's size should be equal to sum(get_tree_leaf_counts()).\n",
      " |      Value corresponding to j-th leaf of i-th tree should be at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  set_scale_and_bias(self, scale, bias)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _CatBoostBase:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |  \n",
      " |  best_score_\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  evals_result_\n",
      " |  \n",
      " |  feature_names_\n",
      " |  \n",
      " |  learning_rate_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  random_seed_\n",
      " |  \n",
      " |  tree_count_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2c438-a946-4792-9aeb-51ac65bec34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df6651be",
   "metadata": {
    "papermill": {
     "duration": 0.042998,
     "end_time": "2021-06-10T06:08:58.405041",
     "exception": false,
     "start_time": "2021-06-10T06:08:58.362043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# catboost model params\n",
    "cat_params = {\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': optimizer.max['params']['learning_rate'],\n",
    "    'eval_metric': 'AUC',\n",
    "    'loss_function': 'Logloss',\n",
    "    'random_seed': SEED,\n",
    "    'metric_period': 100,\n",
    "     ## 'pymnt_plan' : \n",
    "     ## 'disbursement_method' :\n",
    "    'od_wait': 100,\n",
    "    'depth': int(optimizer.max['params']['depth']),\n",
    "    'bagging_temperature': optimizer.max['params']['bagging_temperature']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e71c7c8c",
   "metadata": {
    "papermill": {
     "duration": 0.521071,
     "end_time": "2021-06-10T06:08:58.959997",
     "exception": false,
     "start_time": "2021-06-10T06:08:58.438926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = train.pop('loan_status')\n",
    "y_test = test.pop('loan_status')\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da0dd286",
   "metadata": {
    "papermill": {
     "duration": 0.05581,
     "end_time": "2021-06-10T06:08:59.049211",
     "exception": false,
     "start_time": "2021-06-10T06:08:58.993401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_cat_prediction(train, y, test, features, categorical_features=None, model_params=None, folds=5):\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n",
    "    \n",
    "    x_train = train[features]\n",
    "    x_test = test[features]\n",
    "\n",
    "    y_preds = np.zeros(x_test.shape[0])\n",
    "    y_oof = np.zeros(x_train.shape[0])\n",
    "    score = 0\n",
    "\n",
    "    feature_importance = pd.DataFrame()\n",
    "    feature_importance['feature'] = features\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(x_train, y)):\n",
    "        print(f'Fold: {fold+1}')\n",
    "\n",
    "        x_tr, x_val = x_train.loc[tr_idx, features], x_train.loc[val_idx, features]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        print(x_tr.shape, x_val.shape)\n",
    "        \n",
    "        clf = CatBoostClassifier(**model_params)\n",
    "        clf.fit(x_tr, y_tr, eval_set=(x_val, y_val),\n",
    "                cat_features=categorical_features,\n",
    "                use_best_model=True,\n",
    "                verbose=True)\n",
    "\n",
    "        feature_importance[f'fold_{fold+1}'] = clf.feature_importances_\n",
    "\n",
    "        best_iteration = clf.best_iteration_\n",
    "        y_pred_val = clf.predict_proba(x_val)[:,1]\n",
    "\n",
    "        y_oof[val_idx] = y_pred_val\n",
    "        print(f\"Fold {fold + 1} | AUC Score: {roc_auc_score(y_val, y_pred_val)}\")\n",
    "\n",
    "        score += roc_auc_score(y_val, y_pred_val) / folds\n",
    "        y_preds += clf.predict_proba(x_test)[:,1] / folds\n",
    "\n",
    "        del x_tr, x_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\nMean AUC score = {score}\")\n",
    "    print(f\"OOF AUC score = {roc_auc_score(y, y_oof)}\")\n",
    "    \n",
    "    return y_oof, y_preds, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499e3686",
   "metadata": {
    "papermill": {
     "duration": 2920.400213,
     "end_time": "2021-06-10T06:57:39.483367",
     "exception": false,
     "start_time": "2021-06-10T06:08:59.083154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "(1051432, 60) (262858, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7328113\tbest: 0.7328113 (0)\ttotal: 382ms\tremaining: 1h 3m 38s\n",
      "100:\ttest: 0.7657883\tbest: 0.7657883 (100)\ttotal: 32.9s\tremaining: 53m 47s\n",
      "200:\ttest: 0.7676141\tbest: 0.7676141 (200)\ttotal: 1m 6s\tremaining: 53m 55s\n",
      "300:\ttest: 0.7680037\tbest: 0.7680098 (298)\ttotal: 1m 39s\tremaining: 53m 15s\n",
      "400:\ttest: 0.7679161\tbest: 0.7681480 (326)\ttotal: 2m 11s\tremaining: 52m 25s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7681479764\n",
      "bestIteration = 326\n",
      "\n",
      "Shrink model to first 327 iterations.\n",
      "Fold 1 | AUC Score: 0.7681479764231331\n",
      "Fold: 2\n",
      "(1051432, 60) (262858, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7355645\tbest: 0.7355645 (0)\ttotal: 424ms\tremaining: 1h 10m 36s\n",
      "100:\ttest: 0.7686149\tbest: 0.7686149 (100)\ttotal: 32.8s\tremaining: 53m 33s\n",
      "200:\ttest: 0.7706742\tbest: 0.7706760 (199)\ttotal: 1m 5s\tremaining: 53m 29s\n",
      "300:\ttest: 0.7711488\tbest: 0.7712002 (291)\ttotal: 1m 38s\tremaining: 52m 47s\n",
      "400:\ttest: 0.7710147\tbest: 0.7712621 (310)\ttotal: 2m 10s\tremaining: 51m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7712621346\n",
      "bestIteration = 310\n",
      "\n",
      "Shrink model to first 311 iterations.\n",
      "Fold 2 | AUC Score: 0.7712621346176636\n",
      "Fold: 3\n",
      "(1051432, 60) (262858, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7340486\tbest: 0.7340486 (0)\ttotal: 411ms\tremaining: 1h 8m 31s\n",
      "100:\ttest: 0.7687331\tbest: 0.7687331 (100)\ttotal: 33.4s\tremaining: 54m 37s\n",
      "200:\ttest: 0.7710036\tbest: 0.7710036 (200)\ttotal: 1m 6s\tremaining: 53m 50s\n",
      "300:\ttest: 0.7712325\tbest: 0.7713026 (267)\ttotal: 1m 38s\tremaining: 52m 42s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7713025658\n",
      "bestIteration = 267\n",
      "\n",
      "Shrink model to first 268 iterations.\n",
      "Fold 3 | AUC Score: 0.771302565828486\n",
      "Fold: 4\n",
      "(1051432, 60) (262858, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7363081\tbest: 0.7363081 (0)\ttotal: 405ms\tremaining: 1h 7m 32s\n",
      "100:\ttest: 0.7688365\tbest: 0.7688365 (100)\ttotal: 32.2s\tremaining: 52m 31s\n",
      "200:\ttest: 0.7706431\tbest: 0.7706431 (200)\ttotal: 1m 5s\tremaining: 52m 56s\n",
      "300:\ttest: 0.7710706\tbest: 0.7710919 (297)\ttotal: 1m 38s\tremaining: 52m 38s\n",
      "400:\ttest: 0.7710450\tbest: 0.7711337 (314)\ttotal: 2m 11s\tremaining: 52m 25s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7711337442\n",
      "bestIteration = 314\n",
      "\n",
      "Shrink model to first 315 iterations.\n",
      "Fold 4 | AUC Score: 0.7711337442039483\n",
      "Fold: 5\n",
      "(1051432, 60) (262858, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7327484\tbest: 0.7327484 (0)\ttotal: 395ms\tremaining: 1h 5m 49s\n",
      "100:\ttest: 0.7669345\tbest: 0.7669345 (100)\ttotal: 32.5s\tremaining: 53m\n",
      "200:\ttest: 0.7691875\tbest: 0.7691875 (200)\ttotal: 1m 5s\tremaining: 53m 2s\n",
      "300:\ttest: 0.7700351\tbest: 0.7700448 (292)\ttotal: 1m 38s\tremaining: 53m 2s\n",
      "400:\ttest: 0.7701860\tbest: 0.7702004 (395)\ttotal: 2m 11s\tremaining: 52m 27s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7702004186\n",
      "bestIteration = 395\n",
      "\n",
      "Shrink model to first 396 iterations.\n",
      "Fold 5 | AUC Score: 0.7702004186435023\n",
      "\n",
      "Mean AUC score = 0.7704093679433467\n",
      "OOF AUC score = 0.7704060281171885\n"
     ]
    }
   ],
   "source": [
    "y_oof_cat, y_preds_cat, fi_cat = make_cat_prediction(train, y_train, test, features, \\\n",
    "                                                     categorical_features=cat_feat, model_params=cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb08adc",
   "metadata": {
    "papermill": {
     "duration": 0.082641,
     "end_time": "2021-06-10T06:57:39.616680",
     "exception": false,
     "start_time": "2021-06-10T06:57:39.534039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST AUC score = 0.73955779986333\n"
     ]
    }
   ],
   "source": [
    "print(f\"TEST AUC score = {roc_auc_score(answer['loan_status'], y_preds_cat)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37ebd6",
   "metadata": {
    "papermill": {
     "duration": 0.051184,
     "end_time": "2021-06-10T06:57:39.718884",
     "exception": false,
     "start_time": "2021-06-10T06:57:39.667700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5811.734432,
   "end_time": "2021-06-10T06:57:41.363915",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T05:20:49.629483",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
