{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323a5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb294f4-348f-4caa-ba4f-238d09990a07",
   "metadata": {},
   "source": [
    "# 1. 스파크 내부 메타데이터스토어 정보 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94788bfa-c132-4f48-87bd-fcc85ff48e89",
   "metadata": {},
   "source": [
    "## 스파크 세션생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "440ad315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_ss.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_ss.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class SparkSS :\n",
    "    def __init__(self, appl_name) :\n",
    "        self.appl_name = appl_name\n",
    "    \n",
    "    def getSpark(self) : \n",
    "         self._spark = (SparkSession.builder.appName(self.appl_name)\n",
    "                .config(\"spark.driver.host\",\"127.0.0.1\") \n",
    "                .config(\"spark.driver.bindAddress\",\"127.0.0.1\")\n",
    "                .getOrCreate())\n",
    "         \n",
    "         return self._spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e42af7-88f2-4b7d-94ba-be55cd920ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d91043-ab9d-464e-a2d1-c49bf6c50048",
   "metadata": {},
   "source": [
    "## 세션을 모듈로 작성 후 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30407792-3d5a-47f9-ab27-d030004fce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccfe0a-5f95-46ac-bcde-7ee19613ecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebee4dfe-ee7f-4455-967f-f8b0ab14b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/13 04:06:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/13 04:06:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/13 04:06:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/13 04:06:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = spark_ss.SparkSS(\"subquery_app\").getSpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22c8712-1d02-47a3-b51a-07e8cbecc7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>subquery_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10778b190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55c62c-abbd-43fe-92af-8fd17d46e9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335983c-ec71-4073-b435-db080ef9f0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00825b22-dc0d-4e16-a0f7-afcdba71ad09",
   "metadata": {},
   "source": [
    "# 2. 파이스파크(SQL):\n",
    "\n",
    "### SQL 기반: \n",
    "- 파이스파크 SQL은 SQL 쿼리를 사용하여 데이터를 처리합니다.\n",
    "- SQL은 많은 데이터베이스 시스템에서 널리 사용되는 표준 쿼리 언어이므로 SQL 문법에 익숙한 개발자에게는 익숙하고 쉽게 사용할 수 있습니다.\n",
    "\n",
    "### 선언적: \n",
    "- SQL은 선언적 언어로, 무엇을 원하는지만 명시하고 최적화 및 실행은 시스템이 처리합니다.\n",
    "- 개발자는 데이터 처리 과정을 자세히 제어할 필요가 없습니다.\n",
    "\n",
    "### 내장 함수 및 집계: \n",
    "- SQL은 다양한 내장 함수와 집계 함수를 제공하여 데이터 조작 및 집계를 쉽게 수행할 수 있습니다.\n",
    "\n",
    "### 최적화: \n",
    "- 파이스파크 SQL은 내부적으로 최적화 및 실행 계획을 생성하여 쿼리 성능을 향상시킵니다.\n",
    "\n",
    "### Structured APIs와 통합: \n",
    "- 파이스파크 SQL은 구조화된 API와 연동되어 사용자가 SQL과 메서드 기반 처리를 혼합하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cfb32-c2f3-48f5-9232-1db81d2844f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c9939d-ce24-4067-affc-d43b67b30c72",
   "metadata": {},
   "source": [
    "## 스파크 내부 스키마 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c01b68-1339-4783-88ff-d20c6bd7107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9737923-18dd-4e65-b42e-67ea74727375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cfa9aca-604e-44f8-b65d-7b82ac10c329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d2346-bdf9-4f40-be9b-71d878e30fb6",
   "metadata": {},
   "source": [
    "## 데이터프레임 생성 및 템프테이블 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d3b1c-87f8-4751-ac43-c9ae17810750",
   "metadata": {},
   "source": [
    "## 데이터프레임을 임시테이블로 등록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5159c528-3187-4a4f-a793-1d3cbbd88867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914a120c-fad3-4ae2-875c-af5cde7f1950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |   people|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f9a68-f532-44d9-afa1-8e7cce0c86a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5958dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|       30.0|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT AVG(age) AS average_age FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe04fd5-9da8-4ed4-ac9b-3ad69d439e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a302c104-c9cb-443e-b20c-a5899e5df5cf",
   "metadata": {},
   "source": [
    "# 2. 쿼리와 메서드 처리 비교하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3818154-39fa-40ba-ab52-1ce80a4e945f",
   "metadata": {},
   "source": [
    "## 파이스파크 메서드(메서드 처리):\n",
    "\n",
    "### 메서드 체인: \n",
    "- 파이스파크 메서드 처리는 메서드를 연속적으로 체인하여 데이터를 처리합니다. \n",
    "- 이 방식은 함수형 프로그래밍 개념을 활용하므로 일련의 데이터 조작 단계를 명확하게 정의할 수 있습니다.\n",
    "\n",
    "### 명령형: \n",
    "- 메서드 처리는 명령형 프로그래밍 방식으로 데이터 처리를 수행합니다. \n",
    "- 개발자는 데이터 처리 단계를 직접 명시하고 제어합니다.\n",
    "\n",
    "### 유연성: \n",
    "- 메서드 처리는 SQL보다 유연하며, 복잡한 데이터 처리 작업을 수행하기에 더 적합할 수 있습니다. \n",
    "- 사용자 정의 함수를 활용하여 데이터 조작을 더욱 특화시킬 수 있습니다.\n",
    "\n",
    "### 타입 안정성: \n",
    "- 메서드 처리는 타입 안정성을 보장하므로 런타임 오류를 방지할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92124a-54d4-4e5b-b843-0c9fc7da28a2",
   "metadata": {},
   "source": [
    "### 함수 내의 내장함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7606bf04-9163-424e-b4cd-fcb1e37d694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c90352-d09d-4770-93d8-bad37056fd46",
   "metadata": {},
   "source": [
    "### 하나의 칼럼을 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8571a09-39ff-433c-b9d1-a08dbf97a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 25|\n",
      "| 30|\n",
      "| 35|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383b773-c720-4459-a02a-a09bcde6df1c",
   "metadata": {},
   "source": [
    "### 하나의 칼럼을 내장함수로 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e3099b-78c9-433d-b5f1-0d60a9b06241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(Age)|\n",
      "+--------+\n",
      "|    30.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08dcdd-363c-40b2-bc29-6de73c41e295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ca1a64-b0d7-4cc8-9b96-0df657c28310",
   "metadata": {},
   "source": [
    "# 3. 서브쿼리 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3d173-13ca-4b70-b612-d36ad43b25ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b65f158",
   "metadata": {},
   "source": [
    "## CTE(공통 테이블 표현식) \n",
    "\n",
    "- with 구문으로 서브쿼리를 공통으로 표시\n",
    "- 본 쿼리에서 이를 임시테이블 처럼 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a48af9-e864-4ba4-8cdf-985d16405a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baeb3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use CTE to process a subquery\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH avg_age AS (\n",
    "        SELECT AVG(age) AS average_age FROM people\n",
    "    )\n",
    "    SELECT name FROM people WHERE age > (SELECT average_age FROM avg_age)\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6357517",
   "metadata": {},
   "source": [
    "## select 절에 서브쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fbeb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   name|age|avg_age|\n",
      "+-------+---+-------+\n",
      "|  Alice| 25|   30.0|\n",
      "|    Bob| 30|   30.0|\n",
      "|Charlie| 35|   30.0|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a subquery in the SELECT clause\n",
    "result3 = spark.sql(\"\"\"\n",
    "    SELECT name, age, (SELECT AVG(age) FROM people) AS avg_age\n",
    "    FROM people\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48469c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d462b96",
   "metadata": {},
   "source": [
    "## from 절에 서브쿼리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd0a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a subquery in the FROM clause\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT subquery.name, subquery.age\n",
    "    FROM (SELECT name, age FROM people WHERE age > 30) AS subquery\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959cf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c70cc7",
   "metadata": {},
   "source": [
    "## where 절 서브쿼리 \n",
    "- 서브쿼리가 하나의 값인지은 비교 등으로 처리\n",
    "- 서브쿼리의 결과가 여러 개의 값일 경우는 in 연산자로 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4396a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    30.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT AVG(age) FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02abd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e62e104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a subquery to filter the data\n",
    "result1 = spark.sql(\"SELECT name FROM people WHERE age > (SELECT AVG(age) FROM people)\")\n",
    "\n",
    "# Show the result\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e057a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd95555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d274e6f-727d-4fda-8214-298c9a4621c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
